DSCI 552, Machine Learning for Data 
Science
University of Southern California
M. R. Rajati, PhD
1
Lesson 2
Linear Regression
2
Linear Regression
•Linear regression is a simple 
approach to supervised 
learning. It assumes that the 
dependence of Y on X1, X2, . . .
Xp is linear.
3
Linear regression
2 4 6 8
3
4
5
6
7
X
f(X)
•Although it may seem overly 
simplistic, linear regression is 
extremely useful both 
conceptually and practically.
4
Linear regression for the 
advertising data
Consider the advertising data shown 
on the next slide. Questions we 
might ask:
•Is there a relationship between 
advertising budget and sales?
•How strong is the relationship between 
advertising budget and sales?
Linear regression for the 
advertising data
Consider the advertising data shown 
on the next slide. Questions we 
might ask:
•Which media contribute to sales?
•How accurately can we predict future 
sales?
•Is the relationship linear?
•Is there synergy among the advertising 
media?
Advertising data
0 50 100 200 300
5 10 15 20 25
TV
Sales
0 10 20 30 40 50
5 10 15 20 25
Radio
Sales
0 20 40 60 80 100
5 10 15 20 25
Newspaper
Sales
Case 1: Advertisement Data
Advertising=read.csv("http://wwwbcf.usc.edu/~gareth/ISL/Advertising.csv", header=TRUE); 
newdata=Advertising[,-1]
fix(newdata)
View(newdata)
names(newdata)
pairs(newdata)
8
Simple linear regression using a 
single predictor X. •We assume a model
Y = β0 + β1X + ε,
where β0 and β1 are two unknown 
constants that represent the 
intercept and slope, also known 
as coefficients or parameters, 
and ε is the error term.
Simple linear regression using a 
single predictor X.
•Given some estimates βˆ
0 and βˆ
1 for 
the model coefficients, we predict 
future sales using
y
ˆ
= βˆ
0 + βˆ
1x,
where y
ˆindicates a prediction of Y 
on the basis of X=x. The hat 
symbol denotes an estimated value.
Estimation of the parameters 
by least squares
•Let y
ˆ
i =βˆ
0+βˆ
1xi be the prediction 
for Y based on the ith value of X.
Then ei = yi − y
ˆ
i represents the i
th
residual
11
Estimation of the parameters 
by least squares
•Let y
ˆ
i = βˆ
0 + βˆ
1xi be the prediction 
for Y based on the i
th value of X. 
Then ei = yi − yˆi represents the ith 
residual
•We define the residual sum of squares 
(RSS) as
or equivalently as
12
Normality of e
•Note that in the following, the statistical 
results including confidence intervals, 
hypothesis testing assume that e is 
normally distributed with mean zero 
and standard deviation s.
13
Estimation of the parameters by 
least squares
The least squares approach chooses 
βˆ
0 and βˆ
1 to minimize the RSS. The 
minimizing values can be shown to be
14
= SXY /SX
2
Example: advertising data
0 50 100 150 200 250 300
5 10 15 20 25
TV
Sales
The least squares fit for the regression of 
sales onto TV. In this case a linear fit 
captures the essence of the relationship, 
although it is somewhat deficient in the left 
of the plot. 15
Assessing the Accuracy of the 
Coefficient Estimates
•The standard error of an 
estimator reflects how it varies 
under repeated sampling. We
have
where σ2 = Var(ε)
16
Assessing the Accuracy of the 
Coefficient Estimates
•These standard errors can be used to 
compute confidence intervals. A 95% 
confidence interval is defined as a 
range of values such that 95% of 
times, the range will contain the true 
unknown value of the parameter. It has 
the form βˆ
1 ± 2 ·SE(βˆ
1).
17
Confidence intervals — continued
That is, there is approximately a 
95% chance that the interval
will contain the true value of β1 
(under a scenario where we got 
repeated samples like the present
sample)
18
Confidence intervals — continued
19
^ ^ ^ ^
More accurate CI
Approximate CI for 
1-α=0.95 (by the 
textbook)
In fact, an interval that will contain the true 
unknown value of the parameter β1 in 1-α
percent of times is
Student’s t distribution
Student's t-distribution (or simply the tdistribution) is any member of a family of 
continuous probability distributions that 
arises when estimating the mean of a 
normally distributed population in situations 
where the sample size is small and the 
population standard deviation is unknown.
20
Student’s t distribution
It was developed by William Sealy Gosset
under the pseudonym Student.
The family is parameterized by a parameter 
n, which is called the degrees of freedom.
The distribution is bell-shaped and has a 
zero mean, but its tails are heavier than the 
standard normal distribution. 
21
Student’s t distribution
t
22
Student’s t distribution
When n®¥, tn®Z, where Z is a standard 
normal distribution, i.e., a normal distribution 
with mean zero and standard deviation 1.
23
Student’s t distribution-cut off points
By tn-2,a/2 , we mean:
Pr(tn-2> tn-2,a/2) = a/2
In other words, the area under the pdf of the 
t distribution with n-1 degrees of freedom is 
a/2 to the right of tn-2,a/2.
24
Advertisement Data for simple 
linear regression
lm.fit=lm(Sales~TV,data=Advertising) ## to get Table 3.1
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit) 
25
Results for the advertising data
Confidence intervals — continued
For the advertising data, the 95% 
confidence interval for β1 is 
approximately [0.042, 0.053]
27
Approximate CI for 
1-α=0.95 (by the 
textbook)
^ ^ ^ ^
More accurate CI
Hypothesis testing •Standard errors can also be used to perform 
hypothesis tests on the coefficients. The most 
common hypothesis test involves testing the 
null hypothesis of
H0 :There is no relationship between X and
Y
versus the alternative hypothesis
HA :There is some relationship between X 
and Y . 28
Hypothesis testing
•Mathematically, this corresponds to
testing
H0 : β1 = 0
versus
HA : β1 ≠ 0,
since if β1 = 0 then the model 
reduces to Y = β0 +ε, and X is not 
associated with Y .
29
Hypothesis testing — continued
• To test the null hypothesis, we compute a tstatistic, given by
• This will have a t-distribution with n − 2 
degrees of freedom, assuming β1 = 0.
Hypothesis testing — continued
• If the null hypothesis is true, the 
probability of observing
of rejecting a true null hypothesis, i.e. a 
Type-I error, and should be set ahead of 
time (metaphorically, by your boss). Why? 
Usually, α is selected to be 5%.
Rejection Region Approach
Rejection Region Approach
Rejection Region Approach
Hypothesis testing — continued
•Using statistical software, it is easy 
to compute the probability of 
observing any value equal to |t| or 
larger. We call this probability the
p-value.
Hypothesis testing — continued
•We call this probability the p-value.
Hypothesis testing — continued
•If the p-value is very small, it 
means that the probability of seeing 
a t statistic extremer than what was 
observed assuming that β1 = 0 is 
very small. So we reject the null.
Advertisement Data for simple 
linear regression
lm.fit=lm(Sales~TV,data=Advertising) ## to get Table 3.1
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
confint(lm.fit) 
38
Results for the advertising data
Inferences about the Slope: 
t Test Example
H0: β1 = 0
H1: β1 ≠ 0
Test Statistic: t = 17.76
From Software output: 
Coefficients Standard Error t Stat P-value
Intercept 7.0325 0.4578 15.36 <0.0001
TV .0475 0.0027 17.67 <0.0001
β t ˆ
1 SE(β ˆ
1)
40
Inferences about the Slope: 
t Test Example H0: β1 = 0
H1: β1 ≠ 0
Test Statistic: t = 17.76
There is 
sufficient 
evidence that TV 
affects sales 
Reject H0
Decision:
Conclusion:
Reject H0 Reject H0
a/2=.025
-tn-2,α/2
Do not reject H0
0
a/2=.025
-1.97 1.97 17.76
d.f. = n-2 = 198
t198,.025 = 1.97
tn-2,α/2
41
42
Assessing the Overall 
Accuracy of the Model
•We compute the Residual 
Standard Error
.
where the residual sum-of-squares 
is
43
Assessing the Overall 
Accuracy of the Model
•The Residual Standard Error
is used to estimate the variance 
of the noise ε, i.e. to measure 
how much on average the 
response deviated from the 
regression line.
.
44
Explanatory Power of a 
Linear Regression Equation
Total variation is made up of two 
parts:
Total Sum of 
Squares
Regression Sum of 
Squares
Error (residual) 
Sum of Squares
=å - 2
i SST (y y) =å - 2
i i
=å - SSE (y y
ˆ ) 2
i SSR (yˆ y)
where:
= Average value of the dependent variable
yi = Observed values of the dependent variable
yˆ
i = Predicted value of y for the given xi value
y
TSS = Regression SS + RSS
45
Explanatory Power of a 
Linear Regression Equation
TSS = total sum of squares 
Measures the variation of the yi values around their mean, 
Regression SS = regression sum of squares 
Explained variation attributable to the linear relationship 
between X and Y
RSS = Residual (error) sum of squares 
Variation attributable to factors other than the linear 
relationship between X and Y
46
y
Explanatory Power of a 
Linear Regression Equation
xi
y
X
yi
TSS = Σ (yi- y)2
RSS = Σ (yi – yi
^ )2
Reg SS = Σ(yi
^ - y)2
_
_
_
y^
Y
y
_
yi
^
Explained 
variation
Unexplained 
variation
47
Total
variation
Assessing the Overall 
Accuracy of the Model
•We are interested in the ratio of 
variation explained to total 
variation, i.e.
•
48
RegSS
TSS
=
Assessing the Overall 
Accuracy of the Model
where is the 
total sum of squares.
•R-squared or fraction of total 
variation explained is
49
Assessing the Overall 
Accuracy of the Model
•It can be shown that in this 
simple linear regression setting 
that R2 = r2
, where r is the 
correlation between X and Y:
50
=
SXY
SX SY
Advertising data results
Multiple Linear Regression
•Here our model is
Y = β0 + β1X1 + β2X2 + · · · + βpXp + ε,
•We interpret βj as the average effect
on Y of a one unit increase in Xj ,
holding all other predictors fixed. In the
advertising example, the model
becomes
sales = β0 + β1 × TV + β2 × radio + β3 ×
newspaper + ε.
Interpreting regression coefficients
•The ideal scenario is when the 
predictors are uncorrelated
— a balanced design:
-Each coefficient can be estimated 
and tested separately.
-Interpretations such as “a unit 
change in Xj is associated with a 
βj change in Y on average, while 
all the other variables stay fixed”, 
are possible.
Interpreting regression coefficients
•Correlations amongst predictors 
cause problems:
-The variance of all coefficients 
tends to increase, sometimes 
dramatically
-Interpretations become 
hazardous — when Xj 
changes, everything else 
changes.
Interpreting regression coefficients
•Claims of causality should 
be avoided for observational 
data.
The woes of (interpreting) 
regression coefficients
“Data Analysis and Regression” 
Mosteller and Tukey 1977
•a regression coefficient βj 
estimates the expected change 
in Y per unit change in Xj, with 
all other predictors held fixed. But 
predictors usually change
together!
56
The woes of (interpreting) 
regression coefficients
•Example: Y total amount of 
change in your pocket;
X1 = # of coins; X2 = # of pennies, 
nickels and dimes. 
By itself, regression coefficient of 
Y on X2 will be > 0. But how about 
with X1 in model?
57
The woes of (interpreting) 
regression coefficients
•Y = number of tackles by a
football player in a season; W
and H are his weight and height.
•Fitted regression model is Y
ˆ
= b0
+0.50W − 0.10H. How do we
interpret βˆ
2 < 0?
58
Two quotes by famous
Statisticians
“Essentially, all models are wrong, but 
some are useful”
George Box
59
Two quotes by famous Statisticians
“The only way to find out what will 
happen when a complex system is 
disturbed is to disturb the system, 
not merely to observe it passively”
Fred Mosteller and John Tukey, 
paraphrasing George Box
60
Estimation and Prediction for 
Multiple Regression
•Given estimates βˆ
0, βˆ
1, . . . βˆ
p, 
we can make predictions 
using the formula
•y
ˆ
= βˆ
0+ βˆ
1x1 + βˆ
2x2 + ···+ βˆ
pxp.
•We estimate β0, β1, . . . , βp as 
the values that minimize the
sum of squared residuals RSS
61
Estimation and Prediction for 
Multiple Regression

This is done using standard 
statistical software. The values βˆ
0, 
βˆ
1, . . . , βˆ
p that minimize RSS are the 
multiple least squares regression 
coefficient estimates.
62
X 1
X 2
Y
Confidence intervals for Multiple 
Regression
64
^ ^ ^ ^
An interval that will contain the true unknown 
value of the parameter βi in 1-α percent of 
times is
Hypothesis testing •Standard errors can also be used to perform 
hypothesis tests on the coefficients. The most 
common hypothesis test involves testing the 
null hypothesis of
H0 :There is no relationship between Xi
and Y
versus the alternative hypothesis
HA :There is some relationship between Xi
and Y . 65
Hypothesis testing
•Mathematically, this corresponds to
testing
H0 : βi = 0
versus
HA : βi ≠ 0,
since if βi = 0 then Xi is not 
associated with Y .
66
Hypothesis testing
•In general, to test the following 
hypothesis
H0 : βi = β
versus
HA : βi ≠ β,
we use a t-statistic:
67
Hypothesis testing — continued
• To test the null hypothesis, we compute a tstatistic, given by
• This will have a t-distribution with n − p − 1
degrees of freedom, assuming βi = β.
Usually zero
Hypothesis testing — continued
•Using statistical software, it is easy 
to compute the probability of 
observing any value equal to |t| or 
larger. We call this probability the
p-value.
Usually zero
Hypothesis testing — continued
•If the p-value is very small, it 
means that the probability of seeing 
a t statistic extremer than what was 
observed (assuming that βi = β) is 
very small. So we reject the null.
Rejection Region Approach
•Similar to simple regression
Results for advertising data
Some important
questions
1.Is at least one of the predictors 
X1, X2, . . . , Xp useful in 
predicting the response?
2.Do all the predictors help to 
explain Y , or is only a subset of 
the predictors useful?
73
Some important
questions
3. How well does the model fit the 
data?
4.Given a set of predictor values, 
what response value should we 
predict, and how accurate is our 
prediction?
74
Is at least one predictor useful?
For the first question, we 
can use the F-statistic
0
Tests on Regression Coefficients
Tests on All Coefficients
F-Test for Overall Significance of the Model
Shows if there is a linear relationship between all of 
the X variables considered together and Y
Use F test statistic
Hypotheses:
H0: β1 = β2 = … = βp = 0 (no linear 
relationship)
H1: at least one βi ≠ 0 (at least one 
independent variable affects Y) 76
F-Test for Overall Significance
Test statistic:
where F has p (numerator) and (n – p – 1) 
(denominator) degrees of freedom 
The decision rule is
Reject H0 if F> Fp,n-p-1,α
77
0 Do not Reject H0
reject H0 Fp,n-p-1,α
F-Test for Overall Significance
Reject H0 if F> Fp,n-p-1,α
78
F-Test for Overall Significance
Reject H0 if F> Fp,n-p-1,α
79
F-Test for Overall Significance
H0: β1 = β2 = β3 = 0
H1: Not all three of β1, β2, 
β3 are zero
df1= 3 df2 = 200-3-1 
Test Statistic: F=570 
Decision:
Conclusion:
Since F test statistic is in 
the rejection region (pvalue < .05), reject H0
There is evidence that at 
least one independent 
variable affects Y 0
F3,196,0.05 = 2.65
Do not Reject H0
reject H0
Critical Value: 
F3,196,0.05 = 2.65
F
80
Deciding on the important
variables
•The most direct approach is called all 
subsets or best subsets regression: 
we compute the least squares fit for all 
possible subsets and then choose 
between them based on some 
criterion that balances training error 
with model size.
81
Deciding on the important
variables
•However we often can’t examine all 
possible models, since they are 2p 
of them; for example when p = 40 
there are over a trillion models!
•Instead we need an automated 
approach that searches through a 
subset of them. We discuss two 
commonly use approaches next.
82
Forward selection
•Begin with the null model — a 
model that contains an 
intercept but no predictors.
•Fit p simple linear regressions 
and add to the null model the 
variable that results in the lowest 
RSS.
Forward selection
•Add to that model the variable 
that results in the lowest RSS 
amongst all two-variable models.
•Continue until some stopping rule 
is satisfied, for example when all 
remaining variables have a pvalue above some threshold.
Backward selection
•Start with all variables in the 
model.
•Remove the variable with the 
largest p-value — that is, the 
variable that is the least statistically 
significant.
•The new (p − 1)-variable model is 
fit, and the variable with the 
largest p-value is removed.
Backward selection
•Continue until a stopping rule is 
reached. For instance, we may 
stop when all remaining variables 
have a significant
p-value defined by some 
significance threshold.
Model selection — continued
•Later we discuss more systematic 
criteria for choosing an “optimal” 
member in the path of models 
produced by forward or 
backward stepwise selection.
Model selection — continued
•These include Mallow’s Cp, 
Akaike information criterion 
(AIC), Bayesian information 
criterion (BIC), adjusted R2 and 
Cross-validation (CV).
Other Considerations in the
Regression Model
Qualitative Predictors
•Some predictors are not 
quantitative but are qualitative, 
taking a discrete set of values.
•These are also called 
categorical predictors or 
factor variables.
Other Considerations in the
Regression Model
See for example the scatterplot 
matrix of the credit card data in the 
next slide.
In addition to the 7 quantitative 
variables shown, there are four 
qualitative variables: gender, student 
(student status), status (marital 
status), and ethnicity (Caucasian, 
African American (AA) or Asian).
Credit Card Data
Qualitative Predictors — cont'd
Example: investigate differences in credit 
card balance between males and 
females, ignoring the other variables. We 
create a new variable .
Resulting model:
Intrepretation?
Credit card data —
continued
Results for gender model:
Qualitative predictors with more 
than two levels • With more than two levels, we create 
additional dummy variables. For example, 
for the ethnicity variable we create two 
dummy variables. The first could be .
and the second could be
•Then both of these variables can be 
used in the regression equation, in 
order to obtain the model
Qualitative predictors with more 
than two levels
DSCI 552, Machine Learning for Data 
Science
University of Southern California
M. R. Rajati, PhD
1
Linear and Logistic Regression:
LDA, QDA
k-NN (k Nearest Neighbors)
optimal separating hyperplane – will 
be discussed later (SVM)
2
Lesson 3
Classification
Classification
• Qualitative variables take values in an 
unordered set C, such as:
eye color∈ {brown, blue, green}
digit∈ {0, 1, …,9}
email∈ {spam, ham}.
• Given a feature vector X and a 
qualitative response Y taking values in 
the set C, the classification task is to build 
a function C(X) that takes as input the 
feature vector Xand predicts its value for 
Y; i.e. C(X)∈C. 3
Classification
•Often we are more interested in 
estimating the probabilities that 
X belongs to each category in
C.
4
Case: Credit Card Default Data
• To predict customers that are likely 
to default
• Possible X variables are:
• Annual Income
• Monthly credit card balance
• The Y variable (Default) is 
categorical: Yes or No
• How do we check the relationship 
between Y and X?
5
Example: Credit Card Defualt
0
0 20000
500 1000 1500 2000 2500
Balance
Income
40000 60000
0 500 1000 1500 2000 2500
No Yes
Default
Balance
0 2000
0
60000
No Yes
Default
Income
40000
6
Can we use Linear Regression?
Suppose for the Default 
classification task that we code
Y =
0 if No
1 if Yes.
Can we simply perform a 
linear regression of Y on X
and classify as Yes if Y
ˆ
> 
0.5? 7
Can we use Linear Regression?
Can we simply perform a linear 
regression of Y on X and classify as 
Yes if Yˆ > 0.5?
• In this case of a binary outcome, 
linear regression does a good job as 
a classifier, and is equivalent to linear 
discriminant analysis which we 
discuss later.
•Since in the population E(Y |X = x) = 
Pr(Y = 1|X = x), we might think that 
regression is perfect for this task. 8
Can we use Linear Regression?
•However, linear regression 
might produce probabilities less 
than zero or bigger than one. 
Logistic regression is more 
appropriate.
9
Linear versus Logistic Regression
0 500 1000 1500 2000 2500
0.0 0.2 0.4 0.6 0.8 1.0
Probability of Default
| | | ||| ||| | |||||||| |||| ||||||||||||||||||||||||||||||||||||||| ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||||| | || | || | |
| 
|||||||||| | 
| |||
0 500 1000 1500 2000 2500
0.0 0.2 0.4 0.6 0.8 1.0
Probability of Default
| | | ||| ||| | |||||||| |||| ||||||||||||||||||||||||||||||||||||||| ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||||| | || | || | |
| 
|||||||||| | 
| |||
Balance Balance
The orange marks indicate the response Y , 
either 0 or 1. 
Linear regression does not estimate Pr(Y = 
1|X) well. Logistic regression seems well 
suited to the task.
10
Linear Regression continued
Now suppose we have a response 
variable with three possible values. A 
patient presents at the emergency room, 
and we must classify them according to 
their symptoms.
Y =
. 1 if stroke;
2 if drug overdose;
3 if epileptic seizure
This coding suggests an ordering, and in fact
implies that the difference between stroke and
drug overdose is the same as between drug
overdose and epileptic seizure. 11
Linear Regression continued
• This coding suggests an ordering, and
in fact implies that the difference
between stroke and drug overdose is
the same as between drug overdose
and epileptic seizure.
• Linear regression is not appropriate 
here.
• Multiclass Logistic Regression or 
Discriminant Analysis are more 
appropriate.
12
Multi-Class and Multi-Label Problems
Multiclass classification means a classification 
task with more than two classes; e.g., classify a 
set of images of animals which may be horses, 
birds, or fish. 
Multiclass classification makes the assumption 
that each sample is assigned to one and only one 
label: an animal can be either a horse or a bird 
but not both at the same time.
13
Multi-Class and Multi-Label Problems
Multilabel classification assigns to each sample 
a set of target labels. This can be thought as 
predicting properties of a data-point that are not 
mutually exclusive, such as topics that are 
relevant for a document.
A text might be about any of religion, politics, 
finance or education at the same time or none of 
these.
14
Binary Classification
A binary classification task assigns 
only one of the two possible classes to 
each observation.
Because multi-class and multi-label 
classification tasks can be performed 
using binary classification techniques, 
many times we focus on binary 
classification.
15
Logistic Regression
Let’s write p(X) = Pr(Y = 1|X) for short and 
consider using balance to predict default. 
Logistic regression uses the form
(e ≈ 2.71828 is a mathematical constant 
[Euler’s number.])
It is easy to see that no matter what values 
β0, β1 or X take, p(X) will have values 
between 0 and 1.
16
Sigmoid Function
Parameters control shape and location of sigmoid curve
β0 controls location of midpoint
β1 controls slope of rise
β0 β1
p(X) 
When x=- β0/ β1 , β0+ β1x=0; thus p(X) = 0.5
X
17
Logistic Regression
A bit of rearrangement gives
This monotone transformation is called 
the log odds or logit transformation of
p(X).
18
Definition of Odds
• The probability of an event 
divided by the probability of its 
complement is called its odds.
• Example: The probability of 
winning in a casino is 1%. 
What is the odds of winning in 
that casino?
O(W)=Pr(W)/(1-Pr(W)) 
=0.01/0.99=1/99
19
Logit Model
Therefore, the logit model is trying 
to predict the log of odds of a 
model as a linear combination of 
the predictor(s):
log(O(Y=1|X=x))=β0+β1X
20
Linear versus Logistic Regression
Balance Balance
Logistic regression ensures that our 
estimate for p(X) lies between 0 
and 1.
0 500 1000 1500 2000 2500
0.0 0.2 0.4 0.6 0.8 1.0
Probability of Default
| | | ||| ||| | |||||||| |||| ||||||||||||||||||||||||||||||||||||||| ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||||| | || | || | |
| 
|||||||||| | 
| |||
0 500 1000 1500 2000 2500
Probability of Default
| | | ||| ||| | |||||||| |||| ||||||||||||||||||||||||||||||||||||||| ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| ||||| | || | || | |
| 
|||||||||| | 
| |||
21
Class as A Bernoulli Random Variable
One can see class in a binary 
classification problem as a 
Bernoulli random variable that 
can take two values 0 and 1:
Pr(Y=1|X=x) = p(x)
Pr(Y=0|X=x) = 1-p(x)
This can be rewritten as:
22
pY|X ( y | x) = Pr(Y = y | X = x) =[ p(x)]
y
[1− p(x)]
1−y
, y = 0,1
Independent Sample of Bernoulli 
Variables
Assume that we have an independent
sample whose classes are Y(1) = y1, Y(2)
= y2 ,…,Y (N) = yN
yi is 0 or 1.
The joint probability mass function of 
this independent sample is:
23
Independent Sample of Bernoulli 
Variables
The assumption is that the 
probability that Y(i)
= yi is a function 
of the features X(i)
=xi
So
24
Independent Sample of Bernoulli 
Variables
The joint probability mass function 
is a function of β0, β1and is called 
the likelihood function, given the 
data samples.
25
Maximum Likelihood: Simple 
Example
Your friend gives you a biased coin and 
tells you that he is sure that the 
probability of Heads is either 0.1 or 0.5. 
You flip the coin 100 times and see 90 
Heads. What would be your best 
estimate of the probability of heads, 
given that you are sure that your friend 
is right?
26
Maximum Likelihood: Simple 
Example
What would be your best estimate of the probability of 
heads, given that you are sure that your friend is right?
27
Maximum Likelihood: Simple 
Example
What would be your best estimate of the probability of 
heads, given that you are sure that your friend is right?
28
Maximum Likelihood: Simple 
Example
What would be your best estimate of the 
probability of heads, given that you are sure that 
your friend is right?
This simple example motivates us to 
estimate parameters from data samples 
by maximizing their likelihood. 
29
Maximum Likelihood
We use maximum likelihood to
estimate the parameters β0, β1.
This likelihood gives the probability 
of the observed zeros and ones in 
the data. We pick β0 and β1 to 
maximize the likelihood of the 
observed data.
30
Maximum Likelihood
Most statistical packages can fit 
linear logistic regression models by 
maximum likelihood. In R we use the 
glm function. In Python, 
LogisticRegression is used.
31
Maximum Likelihood
Methods of Maximizing Likelihood:
1- Gradient Methods (to be seen in 
the Lesson on Neural Networks)
2- Expectation Maximization
32
Are the coefficients significant?
• We perform a hypothesis test to see 
whether β0 and β1 are significantly different 
from zero. 
• A Z test is used instead of a T test, but the 
p-value is interpreted similarly.
33
H0: βi = 0
HA: βi ≠ 0
Are the coefficients significant?
• Because the estimates of βi ‘s are Maximum 
Likelihood Estimates, one can show that 
assuming that the null hypothesis
H0: βi = β
is true, the quantity
follows a standard normal distribution 
N(0,1).
34
Usually 
zero
Are the coefficients significant?
The quantity
follows a standard normal distribution 
N(0,1).
35
Are the coefficients significant?
Note: This statement is true for any 
maximum likelihood estimate, provided that 
the number of samples is large:
The quantity
follows a standard normal distribution 
N(0,1) when the null hypothesis is true.
36
Are the coefficients significant?
The quantity
follows a standard normal distribution 
N(0,1).
The rejection region is:
z > zα/2
z < -zα/2
37
Are the coefficients significant?
The rejection region is:
z > zα/2
z < -zα/2
38
Are the coefficients significant?
The p-value is the probability of observing 
something whose magnitude is bigger than 
|z|:
39
Are the coefficients significant?
• The p-value for balance is very small, and 
estimate of β1 is positive, so we are sure 
that if the balance increase, then the 
probability of default will increase as well.
40
Predictions
What is our estimated probability 
of default for someone with a 
balance of $1000?
41
Predictions
With a balance of $2000?
42
Lets do it again, using student 
as the predictor.
43
Logistic regression with several variables
Logistic regression is a linear classifier?!
Logistic regression with several variables
Why is coefficient for student negative, 
while it was positive before?
Confounding
500
0.
0
0.2 0.40.6 0.8
Default Rate
Yes
0 500 1000 1500 2000 2500
1000 1500 2000 No
Credit Card Balance Student Status Credit Card Balance
• Students tend to have higher balances than nonstudents, so their marginal default rate is higher than
for non-students.
• But for each level of balance, students default less 
than non-students.
• Multiple logistic regression can tease this out.
To whom should credit be offered?
• A student is riskier than non students if 
no information about the credit card 
balance is available
• However, that student is less risky than 
a non student with the same credit card 
balance! 
• Example of data-driven decisionmaking
48
Example: South African Heart Disease
• 160 cases of MI (myocardial
infarction) and 302 controls (all
male in age range 15-64), from
Western Cape, South Africa in
early 80s.
• Overall prevalence very high in this 
region: 5.1%.
Example: South African Heart Disease
•Measurements on seven predictors 
(risk factors), shown in scatterplot
matrix.
•Goal is to identify relative strengths 
and directions of risk factors.
•This was part of an intervention 
study aimed at educating the 
public on healthier diets.
0
Scatterplot matrix
of the South
African
Heart
Disease data
. The
response is color
coded
— The
cases (MI) are
red, the controls
turquoise
.
famhist is
a
binary variable,
with
1 indicating
family history of
MI
.
52
Class Imbalance
• Intuitively, a dataset is imbalanced when 
members of certain class(es) are rare. 
• The lack of observations of certain 
classes does not always imply their 
irrelevance. 
• For example, in medical studies of rare 
diseases, the small number of infected 
patients (cases) conveys the most 
valuable information for diagnosis and 
treatments.
Types of Imbalance
•Formally, an imbalanced dataset 
exhibits one or more of the 
following properties:
•Marginal Imbalance. A dataset 
is marginally imbalanced if one 
class is rare compared to the 
other class. In other 
words, Pr(Y=1)≈0.
Types of Imbalance
Marginal Imbalance. 
Such imbalance typically occurs in 
data sets for predicting click-through 
rates in online advertising, detecting 
fraud or diagnosing rare diseases. 
Types of Imbalance
•Formally, an imbalanced dataset 
exhibits one or more of the following 
properties:
• Conditional Imbalance. A dataset 
is conditionally imbalanced when it 
is easy to predict the correct labels 
in most cases. For example, 
if X∈{0,1}, the dataset is 
conditionally imbalanced 
if Pr(Y=1∣X=0)≈0 and Pr(Y=1∣X=1
)≈1.
Subsampling (Down-sampling)
• Typically in such problems, the 
statistical noise is primarily driven by 
the number of representatives of the 
rare class
• We might hope to remedy the 
problem by subsampling the training 
set in a way that enriches for the 
rare class. 
57
However, if the training set is 
randomly sampled to be balanced, the 
test set should be sampled to be more 
consistent with the state of nature and 
should reflect the imbalance so that 
honest estimates of future 
performance can be computed.
58
Subsampling (Down-sampling)
• Ling and Li (1998 ) provide an approach 
to up-sampling in which cases from the 
minority classes are sampled with 
replacement until each class has 
approximately the same size.
• Some minority class samples may show 
up in the training set with a fairly high 
frequency
59
Upsampling
Ling C, Li C (1998). “Data Mining for Direct Marketing: Problems and solutions.”
In “Proceedings of the Fourth International Conference on Knowledge
Discovery and Data Mining,” pp. 73–79.
• The synthetic minority over-sampling 
technique (SMOTE) is a data sampling 
procedure that uses both up-sampling and 
down-sampling, depending on the class
• To up-sample for the minority class, SMOTE 
synthesizes new cases. To do this, a data 
point is randomly selected from the minority 
class and its K -nearest neighbors (KNNs) 
are determined.
60
SMOTE
Chawla N, Bowyer K, Hall L, Kegelmeyer W (2002). “SMOTE: Synthetic Minority 
Over–Sampling Technique.” Journal of Artificial Intelligence Research,16 (1), 321–
357.
• The new synthetic data point is a random 
combination of the predictors of the randomly 
selected data point and its neighbors.
• SMOTE can down-sample cases from the 
majority class via random sampling.
• Three operational parameters:
• the amount of up-sampling, 
• the amount of down-sampling, 
• and the number of neighbors
61
SMOTE
Chawla N, Bowyer K, Hall L, Kegelmeyer W (2002). “SMOTE: Synthetic Minority 
Over–Sampling Technique.” Journal of Artificial Intelligence Research,16 (1), 321–
357.
From left to right: The original simulated data 
set and realizations of a down-sampled version, 
an up-sampled version, and sampling using 
SMOTE
62
SMOTE
Case-control sampling and logistic
regression
•In South African data, there are 160
cases, 302 controls — π˜ = 0.35 are
cases. Yet the prevalence of MI in this
region is π = 0.05.
•With case-control samples, we can 
estimate the regression parameters 
βj accurately (if our model is correct); 
the constant term β0 is incorrect.
Case-control sampling and logistic
regression
•Can correct the estimated 
intercept by a simple
transformation
•Why?
Case-control sampling and logistic
regression
•Can correct the estimated 
intercept by a simple
transformation
•Often cases are rare and we take 
them all; up to five times that 
number of controls is sufficient. 
Diminishing returns in 
unbalanced binary data
2 4 6 8 10 12 14
0.04 0.05 0.06 0.07 0.08 0.09
Control/Case Ratio
Coefficient Variance
Simulation 
Theoretical Sampling more
controls 
cases
than 
reduces
the variance of
the parameter
estimates. But
after a ratio of
about 5 to 1 the
variance reduction flattens
out.
Logistic regression with more than two
classes
So far we have discussed logistic 
regression with two classes. It is 
easily generalized to more than two 
classes. One version (used in the R 
package glmnet) has the symmetric
form
67
Logistic regression with more than 
two classes
Here there is a linear function for each 
class. (Examine the logit model)
Multiclass logistic regression is also 
referred to as multinomial regression.
68
Logistic regression with more than 
two classes
The students will recognize that some cancellation is 
possible, and only K −1 linear functions are needed as
in 2-class logistic regression:
69
(Bayesian) Discriminant Analysis
• Here the approach is to model the 
distribution of X in each of the 
classes separately, and then use 
Bayes theorem to flip things around 
and obtain Pr(Y |X).
• When we use normal (Gaussian) 
distributions for each class, this 
leads to linear or quadratic 
discriminant analysis.
70
Discriminant Analysis
• However, this approach is quite 
general, and other distributions 
can be used as well. We will 
focus on normal distributions.
71
Bayes theorem for classification
Thomas Bayes was a famous 
mathematician whose name 
represents a big subfield of statistical 
and probablistic modeling. Here we 
focus on a simple result, known as 
Bayes theorem:
Pr(Y = k|X = x) = Pr(X = x|Y = k) · Pr(Y = k) 
Pr(X = x)
72
Bayes theorem for classification
Pr(Y = k|X = x) = Pr(X = x|Y = k) · Pr(Y = k) 
Pr(X = x)
One writes this slightly differently for 
discriminant analysis:
where
• fk(x) = Pr(X = x|Y = k) is the density for X
in class k. Here we will use normal densities
for these, separately in each class.
• πk = Pr(Y = k) is the marginal or prior
probability for class k.
73
Classify to the highest density
−4 −2 0 2 4
π1 =0.5, π2 =0.5
−4 −2 0 2 4
• We classify a new point according to which 
density is highest.
• When the priors are different, we take them 
into account as well, and compare πkfk(x). 
On the right, we favor the pink class — the 
decision boundary has shifted to the left.
π1 =0.3, π2 =0.7
74
Why discriminant analysis?
•When the classes are well-separated, 
the parameter estimates for the logistic 
regression model are surprisingly 
unstable. Linear discriminant analysis 
does not suffer from this problem.
Why discriminant analysis?
•If n is small and the distribution of 
the predictors X is approximately 
normal in each of the classes, the 
linear discriminant model is again 
more stable than the logistic 
regression model.
Why discriminant analysis?
•Linear discriminant analysis is 
popular when we have more than 
two response classes, because it 
also provides low-dimensional views 
of the data.
Linear Discriminant Analysis when 
p = 1
The Gaussian density . has the form
Here µk is the mean, and σk 2 is the 
variance (in class k). We will assume 
that all the σk = σ are the same.
78
Linear Discriminant Analysis when p = 1
Plugging this into Bayes formula, 
we get a rather complex 
expression for pk(x) = Pr(Y = k|X 
= x):
Happily, there are simplifications and
cancellations.
79
Linear Discriminant Analysis when p = 1
Happily, there are simplifications and cancellations.
80
Linear Discriminant Analysis when p = 1
Happily, there are simplifications and cancellations.
81
Discriminant functions
To classify at the value X = x, we 
need to see which of the pk(x) is 
largest. Taking logs, and discarding 
terms that do not depend on k, we see 
that this is equivalent to assigning x to 
the class with the largest discriminant
score:
Note that δk(x) is a linear function of x. 82
Discriminant functions
If there are K = 2 classes and π1 
= π2 = 0.5, then one can see that 
the decision boundary is at
(show this)
83
Discriminant functions
(show this)
84
−4 −2 0 2 4 −3 −2 −1 0 1 2 3 4
0 1 2
3
4
5
Example with µ1 = −1.5, µ2 = 1.5, π1 = 
π2 = 0.5, and σ2 = 1.
Typically we don’t know these 
parameters; we just have the training 
data. In that case we simply estimate the 
parameters and plug them into the rule.
85
Estimating the parameters
Where is the usual
formula for the estimated variance in the kth class.
Linear Discriminant Analysis when p > 1
x1
x1
x2
x2
Discriminant function:
Density:
87
Linear Discriminant Analysis when p > 1
Discriminant function:
Density:
88
Linear Discriminant Analysis when p > 1
x1
x1
x2
x2
Despite its complex form,
δk(x) = ck0 + ck1x1 + ck2x2 + . . . + ckpxp
is a linear function.
89
Illustration: p = 2 and K = 3 classes −4 −2 0 2 4
−4 −2 0 2 4
X1
X2
−4 −2 0 2 4
X2
−4 −2 0 2 4
X1
• Here π1 = π2 = π3 =1/3.
• The dashed lines are known as the Bayes 
decision boundaries. Were they known, they 
would yield the fewest misclassification 
errors, among all possible classifiers.
Fisher’s Iris Data
4 variables
3 species
50
samples/class
•Setosa
•Versicolor
•Virginica
LDA classifies
all but 3 of the 
150 training 
samples
correctly.
92
Fisher’s Discriminant Plot
l l
l
l l
l l
l ll
l l l
l ll
l
l l
ll
l ll
l l
ll
l
l l
ll ll
l l l l
l
l
l
l
l
l
l
l l
l
l
l ll l l
l
l
l l l l
l ll
l l
l l l ll
l
l
l
ll
l
l
l
l
l
l
l
l l l
l
l l l
l
l
l
l
l
l
ll
l
l l
lll
l l l l l
l l l
l
l l l
l ll
l l
l
−10 −5 0 5 10
−2 −1 0 1 2
Discriminant Variable 1
Discriminant Variable
2
lll
ll
l l
When there are K classes, linear 
discriminant analysis can be 
viewed exactly in a K − 1 
dimensional plot.
93
Fisher’s Discriminant Plot l l
l
l l
l l
l ll
l l l
l ll
l
l l
ll
l ll
l l
ll
l
l l
ll ll
l l l l
l
l
l
l
l
l
l
l l
l
l
l ll l l
l
l
l l l l
l ll
l l
l l l ll
l
l l
ll
l
l
l
l
l
l
l
l l l
l
l l l
l
l
l
l
l
l
ll
l
l l
lll
l l l l l
l l l
l
l l l
l ll
l l
l
−10 −5 0 5 10
−2 −1 0 1 2
Discriminant Variable 1
Discriminant Variable
2
lll
ll
l l
Why? Because it essentially classifies to the 
closest centroid, and they span a K − 1 
dimensional plane.
Even when K > 3, we can find the “best” 2-
dimensional plane for vizualizing the 
discriminant rule.
94
From δk(x) to probabilities
Once we have estimates δ
ˆ
k(x), we can 
turn these into estimates for class
probabilities:
So classifying to the largest δˆ
k(x) 
amounts to classifying to the class for 
which Pr^ (Y = k|X = x) is largest.
95
From δk(x) to probabilities
• So classifying to the largest 
δˆ
k(x) amounts to classifying to 
the class for which Pr^ (Y = k|X 
= x) is largest.
• When K = 2, we classify to 
class 2 if Pr^(Y = 2|X = x) ≥
0.5, else to class 1.
96
LDA on Credit Data: Confusion Matrix
(23 + 252)/10000 errors — a 2.75% 
misclassification rate! 
97
LDA on Credit Data
Some caveats:
•This is training error, and we 
may be overfitting. Not a big 
concern here since n = 10000 
and p = 4!
98
LDA on Credit Data
Some caveats:
•If we classified to the prior —
always to class No in this case —
we would make 333/10000 errors, 
or only 3.33%.
•Of the true No’s, we make 
23/9667 = 0.2% errors; of the 
true Yes’s, we make 252/333 = 
75.7% errors!
• This is because of class imbalance! 99
Types of errors
• False positive (type I error)
rate:The fraction of negative 
examples that are classified as 
positive — 0.2% in example.
• False negative (type II error)
rate:The fraction of positive 
examples that are classified as 
negative — 75.7% in example.
Types of errors
https://chemicalstatistician.files.wordpress.com/2014/05/pregnant.jpg?w=500
Measures for Different Types of Error
The sensitivity or recall of a binary classifier 
is the rate that the event of interest is 
predicted correctly for all samples having 
the event, or
TP/P=TP/(TP+FN)
It is also called the True Positive Rate 
(TPR) or Hit Rate (HR).
• What proportion of credit card defaults did 
we detect? 102
Measures for Different Types of Error
The specificity of a binary classifier is the 
rate that non-events are predicted correctly 
for all non-event samples or
TN/N=TN/(TN+FP)
It is also called the True Negative Rate 
(TNR).
• What proportion of credit card non-defaults 
did we detect?
103
Types of errors
We produced the confusion matrix for credit 
data by classifying to class Yes if
Pr^ (Default = Yes|Balance, 
Student) ≥ 0.5
We can change the two error rates by 
changing the threshold from 0.5 to some 
other value in [0, 1]:
Pr^(Default = Yes|Balance, Student) ≥
threshold, and vary threshold.
Varying the threshold
0.0 0.2 0.4 0.6
Error Rate
Overall Error 
False Positive 
False Negative
0.0 0.1 0.2 0.3 0.4 0.5
In order to reduce the false negative rate, 
we may want to reduce the threshold to 
0.1 or less.
Threshold
105
ROC Curve
True positive rate
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
The ROC plot displays both simultaneously. Sometimes 
we use the AUC or area under the curve to summarize 
the overall performance. Higher AUC is good.
False positive rate
106
Measures for Different Types of Error
The precision or the positive predictive 
value of a binary classifier is the ratio of 
true positives with respected to all 
detected positives. 
Precision= PPV=TP/(TP+FP)
Of those defaults that we detected, what 
proportion actually defaulted?
107
Precision-Recall Curve
From: https://stackoverflow.com/questions/33294574/good--roc--curve--but--poor--precision--recall--curve
Measures for Different Types of Error
The negative predictive value of a 
binary classifier is the ratio of true 
negatives with respect to all detected 
negatives. 
NPV=TN/(TN+FN)
Of those non-defaults that we detected, what 
proportion actually did not default?
109
Measures for Different Types of Error
The F1 score or F measure of a binary 
classifier is the harmonic mean of 
precision and recall:
110
Measures for Different Types of Error
F1 Score: seeks a balance between 
Precision (ratio of true positives to all 
detected positives) and Recall (True 
Positive Rate). 
F1 Score might be better than accuracy if 
we seek a balance between Precision and 
Recall AND there is a class imbalance 
(large number of Actual Negatives).
111
Measures for Different Types of Error
Fb Score: seeks a skewed balance between 
Precision (ratio of true positives to all 
detected positives) and Recall (True 
Positive Rate).
112
Multiple Classes?
Precision/recall/Fb are specific to 
one class
• How to summarize for multiple 
classes?
Multiple Classes?
Two different ways of averaging:
• A macro average just averages the 
individually calculated scores of each
class
• Weights each class equally
Multiple Classes?
Two different ways of averaging:
• A micro average calculates the metric by first pooling all instances of each class
• Weights each instance equally
Measures: Training or Testing?
Note that all of the measures used to 
evaluate types of error can be computed 
over both training and test sets.
116
Other forms of Discriminant Analysis
When fk(x) are Gaussian densities, with 
the same covariance matrix Σin each class, 
this leads to linear discriminant analysis. By 
altering the forms for fk(x), we get different
classifiers.
•With Gaussians but different Σk in each 
class, we get quadratic discriminant
analysis.
117
Other forms of Discriminant Analysis
• With Gaussians but different Σk in each class,
we get quadratic discriminant analysis. Let’s 
show it for p=1.
118
Other forms of Discriminant Analysis
• With Gaussians but different Σk in each class,
we get quadratic discriminant analysis. Let’s 
show it for p=1.
119
Other forms of Discriminant Analysis
• With (conditional
independence model) in each class we get 
naïve Bayes. For Gaussian this means the 
Σk are diagonal.
• Many other forms, by proposing specific 
density models for fk(x), including 
nonparametric approaches.
120
Quadratic Discriminant Analysis −4 −3 −2 1 2
−4 −2 0 2 4 −4 −2 0 2 4
X1 X1
X2
−4 −3 −2 −1 0 1 2
X2
−1 0
Because the Σk are different, the quadratic 
terms matter.
Logistic Regression versus LDA
For a two-class problem, one can show 
that for LDA .
So it has the same form as logistic 
regression.
The difference is in how the parameters are
estimated.
122
Logistic Regression versus LDA
.
The difference is in how the parameters are estimated.
• Logistic regression uses the conditional likelihood 
based on Pr(Y |X) (known as discriminative learning).
• LDA uses the full likelihood based on Pr(X,Y ) (known
as generative learning).
• Despite these differences, in practice the results are
often very similar.
Logistic regression can also fit quadratic boundaries like 
QDA, by explicitly including quadratic terms in the model.
123
Summary
• Logistic regression is very popular for 
classification, especially when K = 2.
• LDA is useful when n is small, or the classes 
are well separated, and Gaussian 
assumptions are reasonable. Also when K>2.
• Naïve Bayes is useful when p is very large.
• See Section 4.5 for some comparisons of 
logistic regression, LDA and KNN.
124
Naïve Bayes
Assumes features are independent in each class.
Useful when p is large, and so multivariate methods like 
QDA and even LDA break down.
• Gaussian naïve Bayes assumes each Σk is diagonal:
• can use for mixed feature vectors (qualitative and 
quantitative). If Xj is qualitative, replace f kj (xj ) 
with probability mass function (histogram) over 
discrete categories.
Despite strong assumptions, naïve Bayes often 
produces good classification results.
Bayesian: Classify to the highest density
• fk(x) = Pr(X = x|Y = k) is the density
for X in class k. Here we will use
normal densities for these, separately
in each class.
•πk = Pr(Y = k) is the marginal or 
prior probability for class k.
Pr(Y = k|X = x) = Pr(X = x|Y = k) · Pr(Y = k) 
Pr(X = x)
126
Bayesian: Classify to the highest density
• We classify a new point according to which 
density is highest.
• When the priors are different, we take them 
into account as well, and compare 
πkfk(x) = Pr(Y = k) Pr(X = x|Y = k) . 
Pr(Y = k|X = x) = Pr(X = x|Y = k) · Pr(Y = k) 
Pr(X = x)
127
Easy to estimate priors πk from data. (How?)
The real challenge: how to estimate 
fk(x) = Pr(X = x|Y = k) 
= Pr(X = (x1,x2,…,xp)|Y = k)
Bayesian classifiers
128
How to estimate 
fk(x1,x2,…,xp) = Pr(X = (x1,x2,…,xp)|Y = k)
• In the general case, where the attributes xj
have dependencies, this requires estimating 
the full joint distribution fk(x1,x2,…,xp) for 
each class k in C.
• There is almost never enough data to 
confidently make such estimates.
Bayesian classifiers
129
Assume independence among attributes xj when 
class is given: 
fk(x1,x2,…,xp) = fk(x1) fk(x2) … fk(xn)
Usually straightforward and practical to estimate 
fk(xi
) = Pr(Xi =xi
|Y = k) for all xj and k.
New sample is classified to Y=k if
πkΠi fk(xi
) is maximal.
Naïve Bayes classifier
130
Class priors:
π^k= Nk / N
π ( No ) = 7/10
π ( Yes ) = 3/10
For discrete attributes:
Pr^ ( Xi
=xi | Y=k ) = | xik | / 
Nk
where | xik | is number 
of instances in class k
having attribute value xi
Examples:
Pr^( Status = Married | 
No ) = 4/7
Pr^ ( Refund = Yes | Yes ) 
= 0
How to estimate fk(xi
) = Pr(Xi =xi
|Y = k) from data?
131
For continuous attributes: 
Discretize the range into bins 
replace with an ordinal attribute
Two-way split: ( xi < v ) or ( xi > v )
replace with a binary attribute
Probability density estimation:
• assume attribute follows some standard 
parametric probability distribution (usually a 
Gaussian)
• use data to estimate parameters of 
distribution (e.g. mean and variance)
• once distribution is known, can use it to 
estimate the conditional probability Pr(Xi
=xi
|Y = k)
How to estimate fk(xi
) from data?
132
Gaussian distribution:
f k (xi
) = Pr 
#
!$%!"
e
&($!%&'()*
*+!"
one for each ( xi
, k ) pair
For ( Income | Class = No ):
sample mean = 110
sample variance = 2975
How to estimate fk(xi
) from data?
Tid Refund Marital
Status
Taxable
Income Evade
1 Yes Single 125K No
2 No Married 100K No
3 No Single 70K No
4 Yes Married 120K No
5 No Divorced 95K Yes
6 No Married 60K No
7 Yes Divorced 220K No
8 No Single 85K Yes
9 No Married 75K No
10 No Single 90K Yes 10
categorical
categorical
continuous
class
0.0072
2 (54.54)
1 ( Income 120| No ) 2(2975)
(120 110)
2
= = =
- -
p e
p
Pr(
133
Problem: if one of the conditional probabilities 
is zero, then the entire expression becomes 
zero.
This is a significant practical problem, 
especially when training samples are limited.
Ways to improve probability estimation:
Naïve Bayes classifier
N m
N mp
p x C
N c
N
p x C
N
N
p x C
i
ji
j i
i
ji
j i
i
ji
j i
+
+
=
+
+
=
=
m - estimate : ( | )
1
Laplace : ( | )
Original: ( | ) c: number of levels 
variable xj can 
take.
p: prior probability
m: parameter
134
Robust to isolated noise samples.
Handles missing values by ignoring the sample 
during probability estimate calculations.
Robust to irrelevant attributes.
NOT robust to redundant attributes.
Independence assumption does not hold in this 
case.
Use other techniques such as Bayesian Belief 
Networks (BBN).
Summary of Naïve Bayes
135
Appendix: 
Text Classification
136
Is this spam?
Who wrote which Federalist papers?
1787-8: anonymous essays try to convince New 
York to ratify U.S Constitution: Jay, Madison, 
Hamilton. 
Authorship of 12 of the letters in dispute
1963: solved by Mosteller and Wallace using 
Bayesian methods
James Madison Alexander Hamilton
Male or female author?
1. By 1925 present-day Vietnam was divided into three parts 
under French colonial rule. The southern region embracing 
Saigon and the Mekong delta was the colony of CochinChina; the central area with its imperial capital at Hue was 
the protectorate of Annam…
2. Clara never failed to be astonished by the extraordinary 
felicity of her own name. She found it hard to trust herself to 
the mercy of fate, which had managed over the years to 
convert her greatest shame into one of her greatest 
assets…
S. Argamon, M. Koppel, J. Fine, A. R. Shimoni, 2003. “Gender, Genre, and Writing Style 
in Formal Written Texts,” Text, volume 23, number 3, pp. 321–346
Positive or negative movie 
review?
unbelievably disappointing 
Full of zany characters and richly applied satire, and some great plot twists
this is the greatest screwball comedy ever filmed
It was pathetic. The worst part about it was the boxing scenes.
What is the subject of this article?
Antogonists and Inhibitors
Blood Supply
Chemistry
Drug Therapy
Embryology
Epidemiology
…
141
MeSH Subject Category Hierarchy
?
MEDLINE Article
Text Classification
• Assigning subject categories, topics, or 
genres
• Spam detection
• Authorship identification
• Age/gender identification
• Language Identification
• Sentiment analysis
• …
Classification Methods: 
Hand-coded rules
Rules based on combinations of words or 
other features
spam: black-list-address OR (“dollars” 
AND“have been selected”)
Accuracy can be high
If rules carefully refined by expert
But building and maintaining these rules is 
expensive
Classification Methods:
Supervised Machine Learning
Any kind of classifier
Naïve Bayes
Logistic regression
Support-vector machines
k-Nearest Neighbors
…
The bag of words representation
I love this movie! It's sweet, 
but with satirical humor. The 
dialogue is great and the 
adventure scenes are fun… It 
manages to be whimsical and 
romantic while laughing at the 
conventions of the fairy tale 
genre. I would recommend it to 
just about anyone. I've seen 
it several times, and I'm 
always happy to see it again 
whenever I have a friend who 
hasn't seen it yet.
γ( )=c
The bag of words representation
I love this movie! It's sweet, 
but with satirical humor. The 
dialogue is great and the 
adventure scenes are fun… It 
manages to be whimsical and 
romantic while laughing at the 
conventions of the fairy tale 
genre. I would recommend it to 
just about anyone. I've seen 
it several times, and I'm 
always happy to see it again
whenever I have a friend who 
hasn't seen it yet.
γ( )=c
The bag of words representation: 
using a subset of words
x love xxxxxxxxxxxxxxxx sweet
xxxxxxx satirical xxxxxxxxxx
xxxxxxxxxxx great xxxxxxx
xxxxxxxxxxxxxxxxxxx fun xxxx
xxxxxxxxxxxxx whimsical xxxx
romantic xxxx laughing
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxx recommend xxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xx several xxxxxxxxxxxxxxxxx
xxxxx happy xxxxxxxxx again
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxx
γ( )=c
The bag of words representation
γ( )=c
great 2
love 2
recommend 1
laugh 1
happy 1
... ...
Bag of Words
The bag-of-words model is a 
simplifying representation used in 
natural language processing and 
information retrieval (IR). 
In this model, a text (such as a 
sentence or a document) is 
represented as the bag (multiset) 
of its words, disregarding 
grammar and even word order 
but keeping multiplicity.
Example
(1) John likes to watch movies. Mary 
likes movies too. 
(2) John also likes to watch football 
games.
Parsed Model:
"John","likes","to","watch","movies","M
ary","likes","movies","too“
"John","also","likes","to","watch","foo
tball","games"
Example
(1) John likes to watch movies. Mary likes 
movies too. 
(2) John also likes to watch football games.
Bag of Words Model:
BoW1 =
{"John":1,"likes":2,"to":1,"watch":1,"mov
ies":2,"Mary":1,"too":1}; 
BoW2 =
{"John":1,"also":1,"likes":1,"to":1,"watc
h":1,"football":1,"games":1};
Example
(1) John likes to watch movies. Mary likes 
movies too. 
John also likes to watch football games.
Combined Documents:
BoW3 =
{"John":2,"likes":3,"to":2,"watch":2,"mov
ies":2,"Mary":1,"too":1,"also":1,"footbal
l":1,"games":1};
Application
The Bag-of-words model is mainly used as a 
tool of feature generation. 
After transforming the text into a "bag of 
words", we calculate various measures to 
characterize the text. 
The most common type of characteristics, or 
features calculated from the Bag-of-words 
model is term frequency, namely, the 
number of times a term appears in the text. 
Application
For the example above, we can 
construct the following two lists to 
record the term frequencies of all 
the distinct words (BoW1 and 
BoW2 ordered as in BoW3):
(1)[1, 2, 1, 1, 2, 1, 1, 0, 0, 0]
(2)[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]
Term Frequency
The simplest choice to calculate Term 
Frequency to measure the importance of a 
word in a document is to use the raw count of 
a term in a document.
tf(t,d)=number of occurrences of term t in 
document d
Term Frequency
Other choices:
Boolean Frequencies tf(t,d)=1 if t occurs in d 
otherwise 0
Adjustment for document length: 
tf(t,d)=[number of occurrences of term t in 
document d]/number of words in document d 
Inverse Document Frequency
A measure of information each 
word provides, i.e. if a word is 
common or rare among documents
IDF(t,D) = log(total number of 
documents in the corpus D/number 
of documents where the term t 
appears)
TFIDF
A combined measure for each word is 
the TF-IDF which is a product of TF 
and IDF:
TFIDF(t,d,D)=TF(t,d).IDF(t,D)
Therefore, each document can be 
represented as a vector representing 
the bag of words, but instead of simple 
frequencies, the TFIDF of each word 
can be given in the vector.
Naïve Bayes and Language 
Modeling
Naïve bayes classifiers can use 
any sort of feature
URL, email address, dictionaries, 
network features
But if, as in the previous slides
We use only word features 
we use all of the words in the text 
(not a subset)
Then 
Naïve bayes has an important 
similarity to language modeling.
Each class = a unigram 
language model
Assigning each word: P(word | c)
Assigning each sentence: P(s|c)=Π P(word|c)
0.1 I
0.1 love
0.01 this
0.05 fun
0.1 film
…
I love this fun film
0.1 0.1 .05 0.01 0.1
Class pos
P(s | pos) = 0.0000005 
Sec.13.2.1
Naïve Bayes as a Language 
Model
Which class assigns the higher probability 
to s?
0.1 I
0.1 love
0.01 this
0.05 fun
0.1 film
Model pos Model neg
I love this fun film
0.1 0.1 0.01 0.05 0.1
0.2 0.001 0.01 0.005 0.1
P(s|pos) > P(s|neg)
0.2 I
0.001 love
0.01 this
0.005 fun
0.1 film
Sec.13.2.1
How to Deal with Missing Values?
From:
https://machinelearningmastery.com/handl
e-missing-data-python/
Appendix
162
• The simplest strategy for handling missing 
data is to remove samples that contain a 
missing value (feature).
• Advantage: 
• Simplicity
• Disadvantage: 
• Missing a lot of information
• Works poorly if the percentage of 
missing values is high (say 30%), 
compared to the whole dataset
A Simple Strategy
163
• Imputing refers to using a model to 
replace missing values.
• A constant value that has meaning within 
the domain, such as 0, distinct from all 
other values. This works for categorical 
features.
Data Imputation
164
• Imputing refers to using a model to 
replace missing values.
• A feature value from another randomly 
selected observation.
Data Imputation
165
• Imputing refers to using a model to 
replace missing values.
• A statistics such as mean, median or 
mode value of the feature/column.
Data Imputation
166
• Imputing refers to using a model to 
replace missing values.
• A value estimated by another predictive 
model:
• Treat the missing value as the output 
of your predictive model
• Predict it based on other data points 
that do not have missing values
• Examples: KNN regression and 
classification and Linear 
Regression
Data Imputation
167
• Imputing refers to using a model to 
replace missing values.
• A value estimated by another predictive 
model:
• What if a lot of data has missing 
values?
• There will not be enough data without 
missing values for prediction of other 
missing values
• An iterative method based on 
Expectation Maximization can be used
Data Imputation
168
• Imputing refers to using a model to 
replace missing values.
• A value estimated by another predictive 
model:
• An iterative method based on 
Expectation Maximization can be 
used:
• First, impute all missing values randomly or 
using mean or median
• Predict missing values using other data 
points
• Update all missing values and iterate 
Data Imputation
169
• k-Nearest Neighbors can ignore a feature 
from a distance measure when a value is 
missing.
• Calculate distance measure without the 
missing feature
Algorithms That Support 
Missing Values
170
• There are also algorithms that can use the 
missing value as a unique and different 
value (say NaN) when building the 
predictive model, such as classification and 
regression trees.
Algorithms That Support 
Missing Values
171
• The scikit-learn implementations of 
decision trees and k-NN are not robust to 
missing values. Although it is being 
considered.
• This remains as an option if you consider 
using another algorithm implementation 
(such as xgboost) or developing your own 
implementation.
Algorithms That Support 
Missing Values
172
DSCI 552, Machine Learning for Data 
Science
University of Southern California
M. R. Rajati, PhD
1
Lesson 4 
Resampling
2
3
• Repeatedly drawing samples from a training 
set and refitting a model on each sample to 
obtain more information about the fitted 
model.
• Example: Estimate the variability of a linear 
regression fit by repeatedly drawing different 
samples from the training data, fitting a 
regression model to each new sample, and 
then examining the extent to which the 
resulting fits differ.
Resampling Methods
4
• Model Assessment: having chosen a 
final model, estimating its prediction 
error on new data.
• Model Selection: estimating the 
performance of different models in order 
to choose the best one.
Resampling Methods
5
Cross-Validation
Used to estimate test set prediction error 
rates associated with a given machine 
learning method to evaluate its 
performance, or to select the appropriate 
level of model flexibility.
Bootstrap
Used most commonly to provide a
measure of accuracy of a parameter 
estimate or of a given machine learning 
method.
Resampling Methods (cont.)
6
The generalization performance of a machine 
learning method relates to its prediction capability 
on independent test sets.
Assessment of this performance is extremely 
important in practice, since it guides the choice of 
the machine learning method or model.
Further, this gives us a measure of the qualityof 
the ultimately chosen model.
Model Assessment
Test Error
The average prediction error of a machine 
learning method on new observations.
The prediction error over an independent test 
sample.
Training Error
The average loss over the training sample:
Note: The training error rate can dramatically
underestimate the test error rate
Model Assessment (cont.)
7
Model Assessment (cont.)
§ As the model becomes more and more complex, it
uses the training data more and is able to adapt to
more complicated underlyingstructures.
§ Hence, there is a decrease in bias but an increase in 
variance.
§ However, training error is not a good estimate of the
test error.
• Training error consistently decreases with 
model complexity.
• A model with zero training error is overfit to 
the training data and will typically generalize
poorly.
Model Assessment (cont.)
§ If we are in a data-rich situation, the best 
approach for both model selection and 
model assessment is to randomly divide 
the dataset into three parts: training set, 
validation set, and test set.
Model Assessment (cont.)
§ The training set is used to fit the models. The 
validation set is used to estimate prediction 
error for model selection. The test set is used 
for assessment of the prediction error of the 
final chosen model.
§ A typical split might by 50% for training, and 
25% each for validation and testing.
Best solution: use a large designated test set, 
which is often not available. For the methods 
presented here, there is insufficient data to split 
it into three parts.
There are some methods to make mathematical 
adjustments to the training error rate in order to 
estimate the test error rate:
Cp statistic, AIC, BIC (we will discuss thesenext)
Model Assessment (cont.)
Here, we consider cross-validation (CV) 
methods that estimate the test error by 
holding out a subset of the training 
observations from the fitting process, and 
then applying the machine learning method 
to those held out observations.
Model Assessment (cont.)
• The most important use of validation is 
for model selection
• In almost every learning situation, there 
are some choices to be made and we 
need a principled way of making these 
choices.
• The leap is to realize that validation can 
be used to estimate the out-of-sample 
error for more than one model.
Overview: Model Selection
Suppose we have M models; 
validation can be used to 
select one of these models.
We use the training data to fit 
the model, and we evaluate 
each model on the validation 
set to obtain the validation 
errors.
It is now a simple matter to 
select the model with the 
lowest validation error.
Overview: Model Selection (cont.)
We wish to find a set of variables that give thelowest
validation error rate (an estimate of the test errorrate).
If we have a large data set, we can randomly split the 
data into separate training and validationdata sets.
Then, we use the training data set to build eachpossible 
model and select the model that gives the lowest error 
rate when applied to the validation dataset.
Validation Set Approach
Training Data Validation Data
Example: we want to predict mpg from
horsepower
Two models:
mpg ~ horsepower
mpg ~ horsepower + horspower2
Which model gives a better fit?
We randomly split (50/50) 392 observations into
training and validation data sets, and we fit both
models using the training data.
Next, we evaluate both models usingthe validation 
data set.
Winner = model with the lowest validation (testing)
MSE
Validation Set Approach: Example
INF 552 - © M R Rajati- 2018 17
Validation Set Approach: ExampleResults
Left Panel: Validation error estimates for a
single split into training and validation data
sets.
Right Panel: Validation error estimates for
multiple splits; shows the test error rate is
highly variable.INF 552 - © M R Rajati- 2018 18
Advantages:
• Conceptually simple and easyimplementation.
Drawbacks:
• The validation set error rate (MSE) can be 
highly variable.
• Only a subset of the observations (those inthe 
training set) are used to fit the model.
• Machine learning methods tend to perform 
worse when trained on fewer observations.
• Thus, the validation set error rate may tend to 
overestimate the test error rate for the model fit 
on the entire data set.
Validation Set Approach: Review
Instead of two subsets of comparable size, a 
single observation is used for the validation set 
and the remaining observations (n – 1) make up 
the training set.
Leave-One-Out Cross-Validation
Leave-One-Out Cross-Validation
§ LOOCV Algorithm:
– Split the entire data set of size n into:
• Blue = training data set
• Beige = validation data set
– Fit the model using the training dataset
– Evaluate the model using validationset and 
compute the corresponding MSE.
– Repeat this process n times, producingn squared 
errors. The average of these n squared errors 
estimates the test MSE.
LOOCV has far less bias and, therefore, tends 
not to overestimate the test error rate.
Performing LOOCV multiple times always 
yields the same results because there is no 
randomness in the training/validation set splits.
LOOCV is computationally intensive because 
the model has to be fit n times.
Validation Set Approach vs.LOOCV
• The simplest and most widely used 
method for estimating prediction error.
• Directly estimates the average prediction 
error when the model is applied to an 
independent test sample.
K-Fold Cross-Validation
• Had we enough data, we would use a 
validation set to assess the performance 
of the model.
• To finesse the problem, K-fold crossvalidation uses part of the available data 
to fit the model, and a different part to 
test it.
K-Fold Cross-Validation
We use this method because LOOCV is 
computationally intensive.
We randomly divide the data set of into K folds 
(typically K = 5 or 10).
K-Fold Cross-Validation (cont.)
K-Fold Cross-Validation (cont.)
§ The first fold is treated as a validation set, and 
the method is fit on the remaining K – 1 folds. 
The MSE is computed on the observations in 
the held-out fold. The process is repeated K 
times, taking out a different part each time.
§ By averaging the K estimates of the test error,
we get an estimated validation (test) error rate
for new observations.
K-Fold Cross-Validation (cont.)
Let the K folds be F1, … , FK, where Fk denotes the 
indices of the observations in fold k. There are nk 
observations in fold k: if N is a multiple of K, then 
nk = n / K.
Compute:
where and y^i is the
fitted value for observation i, obtained from the 
data withfold k removed.
Right Panel: 10-fold CV run nine separate 
times, each with a different random split of the 
data into ten parts.
Note: LOOCV is a special case of K-fold, where
K = n
16 18 20 22 24 26 28
Mean Squared Error
2 4 6 8 10 2
Degree of Polynomial
Left Panel: LOOCV Error Curve
10
16 18 20 22 24 26 28
K-Fold Cross-Validation vs. LOOCV
LOOCV 10−fold CV
4 6 8
Degree of Polynomial
Mean Squared Error
INF 552 - © M R Rajati - 2018 28
Which is better, LOOCV or K-fold CV?
• LOOCV is more computationally 
intensive 
• LOOCV has less bias than K-fold CV 
(when K < n) :
• Each training set is (K−1)/K as big as
the original training set, so the
estimates of prediction error will typically 
be biased upward. Why?
Bias-Variance Trade-off for KFold Cross-Validation
Which is better, LOOCV or K-fold CV?
• However, LOOCV has higher variance
than K-fold CV (when K < n):
• It doesn’t shake up the data enough.
The estimates from each fold are highly
correlated and hence their average can
have high variance.
• Thus, we see the bias-variance trade-off 
between the two resampling methods
Bias-Variance Trade-off for KFold Cross-Validation
K-fold CV with K = 5 or K = 10 are 
commonly used, as these values have been 
shown empirically to yield test error rate 
estimates that suffer neither from 
excessively high bias nor from very high 
variance
Bias-Variance Trade-off for KFold Cross-Validation
We will cover classification problems in 
more detail later in the course, but we 
briefly show how CV can be used when 
Y is qualitative (categorical) as opposed 
to quantitative. Here, rather than use 
MSE to quantify test error, we instead 
use the number of misclassified 
observation.
Cross-Validation on Classification 
Problems
LOOCV Error Rate:
We use CV as follows:
• Divide data into K folds; hold-out one part and 
fit using the rest (compute error rate on holdout data); repeat K times.
• CV Error Rate: average over the K errorswe 
have computed
Cross-Validation on Classification 
Problems
Note1: prediction accuracy is not always a 
good measure of how well a classifier 
performs. Hence, for highly imbalanced data 
sets, one can estimate other measures such 
as precision, recall, or F scores, using cross 
validation
Cross-Validation on Classification 
Problems: Class Imbalance
Note 2: when data is highly imbalanced, 
some of the folds may contain few points 
or even no points from a rare class. 
Therefore, sometimes stratified cross 
validation is used, which seeks to ensure 
that each fold has the same ratio of each 
class as the original data set. 
Cross-Validation on Classification 
Problems: Class Imbalance
Cross-validation: right andwrong
• Consider a simple classifier applied to some 
two-class data:
1.Starting with 5000 predictors and 50 
samples, find the 100 predictors having 
the largest correlation with the class 
labels.
2. We then apply a classifier such as logistic
regression, using only these 100 
predictors.
How do we estimate the test set 
performance of this classifier?
Can we apply cross-validation in step 2, 
forgetting about step 1?
NO!
• This would ignore the fact that in 
Step 1, the procedure has 
already seen the labels of the 
training data, and made use of 
them. This is a form of training 
and must be included in the 
validation process.
The Wrong and Right Way
• Wrong: Apply cross-validation in 
step 2.
• Right: Apply cross-validation to 
steps 1 and 2.
Cross-Validation: Wrong Way
Cross-Validation: Right Way
• The bootstrap is used to quantify 
uncertainty associated with a given 
estimator or machine learning 
method; it is a general tool for 
assessing statistical accuracy.
The Bootstrap
• As an example, it can be used to 
estimate the standard errors of the 
coefficients from a linear 
regression fit, or a confidence 
interval for that coefficient.
• The use of the term bootstrap 
derives from the phrase “to pull 
oneself up by one’s bootstraps.”
The Bootstrap
The Bootstrap: Overview
The Bootstrap: Overview (cont.)
Suppose we have a model fit to a set of training data. 
We denote the training set by Z = (z1, z2, . . . , zN) 
where zi = (xi
, yi
).
The basic idea is to randomly draw datasets with 
replacement from the training data, each samplethe 
same size as the original trainingset.
This is done B times, producing B bootstrap datasets. 
Then we refit the model to each of the bootstrap 
datasets, and examine the behavior of the fits over the 
B replications.
The Bootstrap: Overview (cont.)
S(Z) is any quantity 
computed from the 
data Z, for example, 
the prediction at some 
input point.
From the bootstrap 
sampling we can 
estimate any aspect of 
the distribution of S(Z), 
for example, its 
variance:
The Bootstrap: Overview (cont.)
Note that this estimated variance can be
thought of as a Monte-Carlo estimate of
the variance of S(Z) under sampling from
the empirical distribution function.
Suppose that we wish to invest a fixed sum of money
in two financial assets that yield returns X and Y; note
that X and Y are random quantities.
We will invest a fraction α of our money in X, and will 
invest the remaining 1 – α inY.
Goal: Since there is variability associated with the 
returns on these two assets, we wish to choose α to 
minimize the total risk, or variance, of ourinvestment.
The Bootstrap: An Example
In other words, we wish to minimize 
Var(αX + (1 – α)Y)=
The value that minimizes the risk, in this 
examples, is:
where
The Bootstrap: An Example (cont.)
In reality, the quantities, , and are
unknown, so we can compute estimates for 
these quantities using a data set that 
contains past measurements for X and Y. 
We can then estimate the value of α that 
minimizes riskusing:
The Bootstrap: An Example (cont.)
σX
2 σY
2 σXY
To do so, the process of 
simulating 100 paired 
observations of X and Y is 
repeated 1,000 times; now we 
have 1,000 estimates for α.
The mean over all 1,000 estimates 
for α à
The standard deviation of the 
estimates à
The Bootstrap: An Example (cont.)
In order to quantify the 
accuracy of our estimate of α, 
we also estimate the standard 
deviation of
The Bootstrap: An Example (cont.)
We can use this approach for 
estimating α on a simulated 
data set, where 100 pairs of 
returns for the investments X 
and Y are simulated.
α
ˆ
In the real world, this procedure cannot be 
applied because for real data we cannot 
generate new samples from the original 
population.
The Bootstrap: An Example (cont.)
In rough terms, for a random sample from
the population, we would expect to differ
from α by approximately 0.08, on average.
α
ˆ
The bootstrap approach, however, allows 
us to use a computer to mimic the process 
of obtaining new data sets; this enables 
use to estimate the variability of our 
estimate without generating additional 
samples.
The Bootstrap: An Example (cont.)
Rather than repeatedly 
obtaining independent data 
sets from the population, we 
instead obtain distinct data sets 
by repeatedly sampling 
observations from the original 
data set with replacement.
The Bootstrap: An Example (cont.)
Each of these bootstrap data 
sets is created by sampling 
with replacement, and is the 
same size as the original data 
set.
As a result, some observations
may appear more than once in
a given bootstrap data set and
some not at all.
The Bootstrap: An Example (cont.)
A graphical illustration of the 
bootstrap approach on a small 
sample containing n = 3 
observations.
Each bootstrap data set 
contains n observations, 
sampled with replacement 
from the original data set.
Each bootstrap data set is 
used to obtain an estimate of 
α.
The Bootstrap: An Example (cont.)
Denoting the first bootstrap data set by 
, we 
can use to produce a new bootstrap 
estimate for α, which we call
This procedure is repeated B times for some 
large value of B (say 100 or 1000), in order to 
produce B different bootstrap data sets, 
,

,…, 
, and B corresponding α estimates,
.
The Bootstrap: An Example (cont.)
α
ˆ *1
αˆ *1
,αˆ *2
,...,αˆ *B
We compute the standard error of these 
bootstrap estimates using:
This serves as an estimate of the
standard error of estimated from the
original data set.
The Bootstrap: An Example (cont.)
α
ˆ
In more complex data situations, 
figuring out the appropriate way to 
generate bootstrap samples can 
require some thoughts.
For example, if the data is a time 
series, we cannot simply sample the 
observations with replacement.
The Bootstrap: More Details
However, we can instead createblocks 
of consecutive observations, and 
sample those with replacement. Then, 
we paste together sampled blocks to 
obtain a bootstrap dataset.
The Bootstrap: More Details
Although the bootstrap is used 
primarily to obtain standard errors of 
an estimate, it can also provide 
approximate confidence intervalsfor a 
population parameters.
One approach is known as Bootstrap 
Percentile confidence interval.
The Bootstrap: More Details (cont.)
Easy Algorithm for Building (1-α) Confidence 
Interval Using Bootstrap
1. Create B bootstrap datasets from original 
dataset with the same size N.
2. Calculate the mean of each bootstrap sample 
m1, m2,…,mB. Consider them as a sample of the 
sample means.
3. Order the sample means and call them m(1), 
m(2),…,m(B)
4. The middle (1-α)B of the sample yield the (1-α) 
Confidence interval for the mean
Bootstrap Confidence Intervals
Easy Algorithm for Building (1-α) Confidence 
Interval for The Mean Using Bootstrap
Note: The algorithm on the previous slide can be 
applied to any statistic like median, variance, 
standard deviation, quartiles, etc. 
To calculate a Bootstrap C.I. for any statistic, it is 
adequate to calculate that statistic from each 
bootstrap sample, instead of the sample mean.
Bootstrap Confidence Intervals
Assume the sample data is 
30, 37, 36, 43, 42, 43, 43, 46, 41, 42 
Problem: Estimate the mean μ of the 
underlying distribution and give an 80% 
bootstrap confidence interval. 
Bootstrap Percentile Confidence 
Interval: Example
The sample mean is = 40.3, which is an 
estimate of the true mean μ of the underlying 
distribution. 
To construct the confidence interval we need 
to know how much the distribution of varies 
around μ. 
Bootstrap Percentile Confidence 
Interval: Example
x
x
20 bootstrap samples were generated, each of size 
10. Each of the 20 columns in the following array is 
one bootstrap sample. 
Bootstrap Percentile Confidence 
Interval: Example
Next we compute for each bootstrap sample (i.e. 
each column) and sort them from smallest to 
biggest: 
Bootstrap Percentile Confidence 
Interval: Example
Sorted Sample Means
38.7 38.9 38.9 39.4 39.8 40.1 40.2 40.4 40.5 40.5 40.7 40.7 41 41.2 41.4 41.5 41.5 41.9 41.9 42.3
The middle 80% of the data gives us the 
C.I., i.e. [m3,m18] = [38.9, 41.9]
Bootstrap Percentile Confidence 
Interval: Example
In cross-validation, each of the K validation 
folds is distinct from the other K – 1 folds 
used for training (i.e. there is no overlap).
To estimate the prediction error using the 
bootstrap, one approach would be to fit the 
model in question on a set of bootstrap 
samples, and then keep track of how well it 
predicts the original training set.
The Bootstrap: More Details (cont.)
If is the predicted value at xi
, from the 
model fitted to the bth bootstrap dataset, 
our estimate is:
The Bootstrap: More Details (cont.)
ˆ
f *b
(xi
)
This estimate does not provide a good estimate 
in general because the bootstrap datasets are 
acting as the training samples, while the original 
training set is acting as the test sample, and 
these two samples have observations in 
common.
Each bootstrap sample has significant overlap with 
the original data.
The Bootstrap: More Details (cont.)
§ Note that about 2/3 of the original N 
data points appear in each bootstrap 
sample:
§ One the other hand
The Bootstrap: More Details (cont.)
Pr{Observation i∈ bootstrap sample b}
=1−Pr{Observation i∉ bootstrap sample b}
Pr{Observation i∉ bootstrap sample b}
= Pr{Observation i is not any of the N members of the bootstrap sample b}
§ One the other hand
§ Therefore
when N is large, this is equal to e-1
• So
The Bootstrap: More Details (cont.)
Pr{Observation i∉ bootstrap sample b}
= Pr{Observation i is not any of the N members of the bootstrap sample b}
= Pr{Each of the N members of the bootsrap sample b is one of the N-1 observations other than i }
Pr{Observation i∉ bootstrap sample b}= (1−1/ N)
N
Pr{Observation i∈ bootstrap sample b}≈1−e
−1 ≈ 0.632
This overlap can make overfit predictions like 
unrealistically good, and is the reason that 
cross-validation explicitly uses non-overlapping 
data for the training and test samples.
In other words, this will cause the bootstrap to 
seriously underestimate the true prediction 
error.
By mimicking cross-validation, a better 
bootstrap estimate can be obtained.
The Bootstrap: More Details (cont.)
For each observation, we only keep track of 
prediction from bootstrap samples not containing 
that observation.
The leave-one-out bootstrap estimate of 
prediction error is defined by:
Here C-i is the set of indices of the bootstrap 
samples b that do not contain observation I, and
|C-i
| is the number of such samples.
The Bootstrap: More Details (cont.)
Note that the leave-one-out bootstrap 
solves the problem of overfitting, but has a 
training-set-size bias.
The “.632 estimator” is designed to alleviate 
this bias.
The Bootstrap: More Details (cont.)
Resampling methods for machine 
learning model selection and 
assessment.
The validation set approach, leaveone-out cross-validation, and K-fold
cross-validation.
The bootstrap method for assessing 
statistical accuracy.
Summary
DSCI 552, Machine Learning for Data 
Science
University of Southern California
M. R. Rajati, PhD
1
2
Lesson 5
Linear Model Selection and 
Regularization
Linear Model Selection and 
Regularization
• Recall the linear model
Y = β0 + β1X1 + · · · + βpXp + ε.
• We will consider some approaches for extending 
the linear model framework. 
• In Chapter 7 of the text, the linear model is 
generalized in order to accommodate non-linear, 
but still additive, relationships.
• In the lectures covering Chapter 8 we consider 
even more general non-linear models.
All hail to linear models!
•Despite its simplicity, the linear model 
has distinct advantages in terms of its 
interpretability and often shows good
(acceptable) predictive performance.
All hail to linear models!
•To improve the simple linear model, the 
ordinary least squares fitting can be 
replaced with some alternative fitting 
procedures.
Why consider alternatives to least
squares?
•Prediction Accuracy: especially when p > 
n, to control the variance.
• Model Interpretability: By removing 
irrelevant features — that is, by setting 
the corresponding coefficient estimates 
to zero — we can obtain a model that is 
more easily interpreted. 
•Approaches for automatically performing 
feature selection will be presented.
Three classes of methods
•Subset Selection. Identifying a 
subset of the p predictors that are 
related to the response and fitting a 
model using least squares on the 
reduced set of variables.
Three classes of methods
•Shrinkage. Fitting a model involving 
all p predictors, but the estimated 
coefficients are shrunken towards 
zero relative to the least squares 
estimates. 
•shrinkage (= regularization) 
reduces the variance and can
perform variable selection.
Three classes of methods
• Dimension Reduction. The p 
predictors are projected into a M -
dimensional subspace, where M < 
p. 
•Achieved by computing M 
different linear combinations, or 
projections, of the variables.
• These M projections are used as 
predictors to fit a linear
regression model by least
squares.
Subset Selection
Best subset and stepwise model selection
procedures
Best Subset Selection
1.Let M0 denote the null model, which contains no 
predictors. This model simply predicts the sample mean 
for each observation.
2.For k = 1, 2, . . . p:
• Fit all models that contain exactly k predictions
• Pick the best among these models and call it Mk . 
• Here best is defined as having the smallest RSS, or 
equivalently largest R2 .
3.Select a single best model from among M0 , . . . , Mp
using cross-validated prediction error, Cp (AIC), BIC, or 
adjusted R2.
Example- Credit data set
For each possible model containing a subset of the ten 
predictors in the Credit data set, the RSS and R2 are displayed. 
The red frontier tracks the best model for a given number of 
predictors, according to RSS and R2. Though the data set 
contains only ten predictors, the x-axis ranges from 1 to 11, 
since one of the variables is categorical and takes on three 
values, leading to the creation of two dummy variables
Extensions to other models
•Although we have presented best 
subset selection here for least
squares regression, the same ideas 
apply to other types of models, such 
as logistic regression.
•The deviance— negative two times 
the maximized log-likelihood— plays 
the role of RSS for a broader class of 
models.
Stepwise Selection
•Best subset selection cannot be 
applied with very large p. Why not?
•Best subset selection may also suffer 
from statistical problems when p is 
large: the larger the search space, the 
higher the chance of finding models 
that look good on the training data, 
even though they might not have any
predictive power on future data.
Stepwise Selection
•Thus an enormous search space can 
lead to overfitting and high variance 
of the coefficient estimates.
•For both of these reasons, stepwise 
methods, which explore a far more 
restricted set of models, are 
attractive alternatives to best subset
selection.
Forward Stepwise Selection
•Forward stepwise selection begins 
with a model containing no 
predictors, and then adds predictors 
to the model, one-at-a-time, until all 
of the predictors are in the model.
Forward Stepwise Selection
•In particular, at each step the 
variable that gives the greatest 
additional improvement to the fit is 
added to the model.
Forward Stepwise Selection
In Detail
1.Let M0 denote the null model, which contains no 
predictors.
2.For k = 0, . . . , p − 1:
1. Consider all p − k models that augment the predictors in
Mk with one additional predictor.
2. Choose the best among these p − k models, and call it
Mk+1. Here best is defined as having smallest RSS or
highest R2.
3.Select a single best model from among M0, . . . , Mp using 
cross-validated prediction error, Cp (AIC), BIC, or adjusted 
R2.
More on Forward Stepwise
Selection
•Computational advantage over best 
subset selection is clear.
•It is not guaranteed to find the best
possible model out of all 2p models
containing subsets of the p predictors.
Why not? Give an example.
Credit data example
The first four selected models for best
subset selection and forward stepwise
selection on the Credit data set. The first
three models are identical but the fourth
models differ.
Backward Stepwise Selection
•Like forward stepwise selection, 
backward stepwise selection provides 
an efficient alternative to best subset 
selection.
•However, unlike forward stepwise 
selection, it begins with the full least 
squares model containing all p 
predictors, and then iteratively 
removes the least useful predictor,
one-at-a-time.
Backward Stepwise Selection: details
Backward Stepwise Selection
1.Let Mp denote the full model, which contains all p
predictors.
2.For k = p, p − 1, . . . , 1:
1. Consider all k models that contain all but one 
of the predictors in Mk, for a total of k − 1 
predictors.
2. Choose the best among these k models, and call 
it Mk−1. Here best is defined as having smallest 
RSS or highest R2.
3.Select a single best model from among M0, . . . , Mp 
using cross-validated prediction error, Cp (AIC), 
BIC, or adjusted R2.
More on Backward Stepwise
Selection
• Like forward stepwise selection, the backward 
selection approach searches through only 1 + 
p(p + 1)/2 models, and so can be applied in 
settings where p is too large to apply best 
subset selection
• Like forward stepwise selection, backward
stepwise selection is not guaranteed to yield
the best model containing a subset of the p
predictors.
More on Backward Stepwise
Selection
•Backward selection requires that the 
number of samples n is larger than 
the number of variables p (so that the 
full model can be fit). 
•In contrast, forward stepwise can be 
used even when n < p, and so is the 
only viable subset method when p is 
very large.
Choosing the Optimal
Model
•The model containing all of the
predictors will always have the
smallest RSS and the largest R2
,
since these quantities are related to
the training error.
Choosing the Optimal
Model
•We wish to choose a model with low
test error, not a model with low training
error. Recall that training error is
usually a poor estimate of test error.
Choosing the Optimal
Model
•Therefore, RSS and R2 are not 
suitable for selecting the best 
model among a collection of models 
with different numbers of
predictors.
Estimating test error: two approaches
•We can indirectly estimate test error 
by making an adjustment to the 
training error to account for the bias 
due to overfitting.
Estimating test error: two approaches
•We can directly estimate the test 
error, using either a validation set 
approach or a cross-validation 
approach, as discussed in previous 
lectures.
•We illustrate both approaches next.
Cp, AIC, BIC, and Adjusted R2
•These techniques adjust the training 
error for the model size, and can be 
used to select among a set of models 
with different numbers of variables.
29
Cp, AIC, BIC, and Adjusted R2
•The next figure displays Cp, BIC, and 
adjusted R2 for the best model of each 
size produced by best subset selection 
on the Credit data set.
30
Credit data example
Now for some details
Mallow’s Cp:
where d is the total # of parameters used and σˆ2 
is an estimate of the variance of the error ε
associated with each response measurement.
We learned that Residual Sum of Squares, RSE, 
is used to estimate the variance of error:
Now for some details
• The AIC criterion is defined for a large 
class of models fit by maximum
likelihood:
AIC = −2 log L + 2 · d
where L is the maximized value of the 
likelihood function for the estimated
model.
Now for some details
• In the case of the linear model with 
Gaussian errors, maximum likelihood and 
least squares are the same thing, and Cp 
and AIC are equivalent. 
Details on BIC
• Like Cp, the BIC will tend to take on a small value for a 
model with a low test error, and so generally we select 
the model that has the lowest BIC value.
• Notice that BIC replaces the 2dσˆ2 used by Cp with a 
log(n)dσˆ2 term, where n is the number of 
observations.
• Since log n > 2 for any n > 7, the BIC statistic
generally places a heavier penalty on models with
many variables, and hence results in the selection of
smaller models than Cp.
Adjusted R2
For a least squares model with d variables, the
adjusted R2 statistic is calculated as
where TSS is the total sum of squares.
Unlike Cp, AIC, and BIC, for which a small value 
indicates a model with a low test error, a large value 
of adjusted R2 indicates a model with a small test 
error. Remember:
Adjusted R2
Maximizing the adjusted R2 is equivalent to 
minimizing RSS/(n-d-1).
While RSS always decreases as the number 
of variables in the model increases, RSS/
(n-d-1) may increase or decrease, due to the 
presence of d in the denominator.
Unlike the R2 statistic, the adjusted R2 statistic 
pays a price for the inclusion of unnecessary 
variables in the model.
Validation and Cross-Validation
•Each of the procedures returns a 
sequence of models Mk indexed by 
model size k = 0, 1, 2, . . .. Our job here 
is to select kˆ. Once selected, we will 
return model Mkˆ
38
Validation and Cross-Validation
•We compute the validation set error or the 
cross-validation error for each model Mk 
under consideration, and then select the k 
for which the resulting estimated test error 
is smallest.
39
Validation and Cross-Validation
•This procedure has an advantage relative 
to AIC, BIC, Cp, and adjusted R2, in that it 
provides a direct estimate of the test error, 
and doesn’t require an estimate of the 
error variance σ2.
40
Validation and Cross-Validation
2 3 / 5 7
•It can also be used in a wider range of 
model selection tasks, even in cases 
where it is hard to pinpoint the model 
degrees of freedom (e.g. the number of 
predictors in the model) or hard to 
estimate the error variance σ2.
41
Credit data example
42
Details of Previous Figure
•The validation errors were calculated 
by randomly selecting three-quarters 
of the observations as the training set, 
and the remainder as the validation 
set.
43
Details of Previous Figure
•The cross-validation errors were 
computed using k = 10 folds. In this 
case, the validation and crossvalidation methods both result in a 
six-variable model.
44
Details of Previous Figure
•However, all three approaches
suggest that the four-, five-, and
six-variable models are roughly
equivalent in terms of their test
errors.
Shrinkage Methods
Ridge regression and Lasso
•The subset selection methods use 
least squares to fit a linear model 
that contains a subset of the 
predictors.
Shrinkage Methods
Ridge regression and Lasso
•As an alternative, we can fit a 
model containing all p predictors 
using a technique that constrains 
or regularizes the coefficient 
estimates, or equivalently, that 
shrinks the coefficient estimates 
towards zero.
Shrinkage Methods
Ridge regression and Lasso
•It may not be immediately obvious why
such a constraint should improve the fit,
but it turns out that shrinking the
coefficient estimates can significantly
reduce their variance.
Ridge regression
Recall that the least squares fitting procedure 
estimates β0, β1,…, βp using the values that 
minimize
In contrast, the ridge regression coefficient 
estimates β^R are the values that minimize
where λ≥ 0 is a tuning parameter, to be 
determined separately. Note that β0 is not 
regularized.
Ridge regression: continued
• As with least squares, ridge regression 
seeks coefficient estimates that fit the data 
well, by making the RSS small.
• However, the second term, called 
a shrinkage penalty, is small when β1,…, βp
are close to zero, and so it has the effect of 
shrinking the estimates of βj towards zero.
Ridge regression: continued
• The tuning parameter serves to control 
the relative impact of these two terms 
on the regression coefficient estimates.
• Selecting a good value for λ is critical; 
cross-validation is used for this.
Credit data example
Details of Previous Figure
• In the left-hand panel, each curve corresponds 
to the ridge regression coefficient estimate for 
one of the ten variables, plotted as a function 
of λ.
• The right-hand panel displays the same ridge 
coefficient estimates as the left-hand panel, 
but instead of displaying on the x-axis, we now 
display , where β^ denotes the 
vector of least squares coefficient estimates.
• The notation denotes the norm of a 
vector, and is defined as
Ridge regression: scaling of predictors
•The standard least squares 
coefficient estimates are scale 
equivariant: multiplying Xj by a 
constant c simply leads to a 
scaling of the least squares 
coefficient estimates by a factor of 
1/c.
Ridge regression: scaling of predictors
•In other words, regardless of how 
the jth predictor is scaled, Xj βˆ
j will 
remain the same.
Ridge regression: scaling of predictors
•In contrast, the ridge regression 
coefficient estimates can change 
substantially when multiplying a 
given predictor by a constant, due 
to the sum of squared coefficients 
term in the penalty part of the 
ridge regression objective
function.
Ridge regression: scaling of predictors
• Therefore, it is best to apply ridge 
regression after standardizing the 
predictors, using the formula
•Practical note: try raw, standardized, 
and normalized data.
Ridge Regression Improves Over Least
Squares?
The Bias-Variance tradeoff
58
Ridge Regression Improves Over Least
Squares?
The Bias-Variance tradeoff
Simulated data with n = 50 observations, p = 
45 predictors, all having nonzero coefficients. 
Squared bias (black), variance (green), and 
test mean squared error (purple) for the ridge 
regression predictions on the simulated data 
set, as a function of λ and . The 
horizontal dashed lines indicate the minimum 
possible MSE. The purple crosses indicate the 
ridge regression models for which the MSE is 
smallest. 59
The Lasso
•Ridge regression does have one 
obvious disadvantage: unlike subset 
selection, which will generally select 
models that involve just a subset of the 
variables, ridge regression will include 
all p predictors in the final model
The Lasso
•The Lasso is a relatively recent 
alternative to ridge regression that 
overcomes this disadvantage. The
lasso
The Lasso: continued
•As with ridge regression, the lasso 
shrinks the coefficient estimates 
towards zero.
The Lasso: continued
•However, in the case of the lasso, the 
L1 penalty has the effect of forcing 
some of the coefficient estimates to be 
exactly equal to zero when the tuning 
parameter λ is sufficiently large.
The Lasso: continued
•Hence, much like best subset selection, 
the lasso performs variable selection.
•We say that the lasso yields sparse 
models — that is, models that 
involve only a subset of the variables.
The Lasso: continued
•As in ridge regression, selecting a good 
value of λ for the lasso is critical; 
cross-validation is again the method of 
choice.
Aside: Sparsity
Sparsity is an essential issue in Machine Learning
66
Example: Credit dataset
The Variable Selection Property of 
the Lasso
Why is it that the lasso, unlike ridge regression, 
results in coefficient estimates that are exactly 
equal to zero?
One can show that the lasso and ridge regression 
coefficient estimates respectively solve the problems
and 
68
The Lasso Picture
Different Constraints 
in 2D
The Lasso Constraint 
in 3D
The Lasso in 3D
Other Constraints in 
3D
Comparing the Lasso and Ridge Regression
Same simulated data with n = 50 
observations, p = 45 predictors, all 
having nonzero coefficients.
Lasso: solid
Ridge: dashed
Comparing the Lasso and Ridge Regression
Left: Plots of squared bias (black), variance (green), 
and test MSE (purple) for the lasso on the simulated 
data set. Right: Comparison of squared bias, variance 
and test MSE between lasso (solid) and ridge 
(dashed). Both are plotted against their R2 on the 
training data, as a common form of indexing. The 
crosses in both plots indicate the lasso model for 
which the MSE is smallest.
Comparing the Lasso and Ridge 
Regression: continued
Simulated data with n = 50 observations, p = 45 
predictors, only two having nonzero coefficients.
Lasso: solid
Ridge: Dashed 
Comparing the Lasso and Ridge 
Regression: continued
Left: Plots of squared bias (black), variance (green), and 
test MSE (purple) for the lasso. The simulated data is 
similar to previous slides, except that now only two 
predictors are related to the response.
Right:Comparison of squared bias, variance and test 
MSE between lasso (solid) and ridge (dashed). Both are 
plotted against their R2 on the training data, as a 
common form of indexing. The crosses in both plots 
indicate the lasso model for which the MSE is smallest.
Conclusions
•These two examples illustrate that neither 
ridge regression nor the lasso will 
universally dominate the other.
•In general, one might expect the lasso to 
perform better when the response is a 
function of only a relatively small number 
of predictors.
Conclusions
•However, the number of predictors that 
is related to the response is never 
known a priori for real data sets.
•A technique such as cross-validation can 
be used in order to determine which 
approach is better on a particular data 
set.
•As for subset selection, for ridge 
regression and lasso we require a 
method to determine which of the 
models under consideration is best.
Selecting the Tuning Parameter for 
Ridge Regression and Lasso
•That is, we require a method selecting 
a value for the tuning parameter λ or 
equivalently, the value of the 
constraint s.
Selecting the Tuning Parameter for 
Ridge Regression and Lasso
• Cross-validation provides a simple way to 
tackle this problem. We choose a grid of 
λ values, and compute the crossvalidation error rate for each value of λ.
• We then select the tuning parameter 
value for which the cross-validation error 
is smallest.
• Finally, the model is re-fit using all of 
the available observations and the 
selected value of the tuning
parameter.
Selecting the Tuning Parameter for 
Ridge Regression and Lasso
Credit data example
Left: Cross-validation errors that result from 
applying ridge regression to the Credit data 
set with various values of λ.
Right: The coefficient estimates as a function 
of λ. The vertical dashed lines indicates the 
value of selected by cross-validation.
Simulated data example
Left: Ten-fold cross-validation MSE for the lasso, 
applied to the sparse simulated data set with two 
predictors.
Right: The corresponding lasso coefficient 
estimates are displayed. The vertical dashed 
lines indicate the lasso fit for which the crossvalidation error is smallest.
Elastic Net
•Although the lasso does a good job in 
eliminating irrelevant variables, it does not 
handle redundant variables (highly 
correlated variables) very well
• The coefficient paths tend to be erratic 
and can sometimes show wild behavior 
(e.g. starting from zero and increasing, 
when λ increases)
85
Elastic Net
• Consider a simple but extreme example, 
where the coefficient for a variable Xj with 
a particular value for λ is βˆj > 0. If we 
augment our data with an identical copy Xj’
= Xj
, then they can share this coefficient in 
infinitely many ways— any β˜j + β˜j’ = βˆj 
with both pieces positive—and the loss 
and L1 penalty are indifferent. 
•So the coefficients for this pair are not 
defined.
86
Elastic Net
•A quadratic penalty, on the other hand, will 
divide βˆj exactly equally between these two 
twins. 
• In practice, we are unlikely to have an 
identical pair of variables, but often we do 
have groups of very correlated variables.
87
Elastic Net
• In microarray studies, groups of genes in the 
same biological pathway tend to be 
expressed (or not) together, and hence 
measures of their expression tend to be 
strongly correlated.
88
Elastic Net
• The elastic net is a regularization method that 
combines L1 and L2 regularizations, and 
enforces the coefficients of correlated variables 
to vary together as λ changes. The Elastic Net 
penalty is:
where α ∈ [0,1] is a parameter that can be 
varied. When α = 1, it reduces to the L1-norm or 
lasso penalty, and with α = 0, it reduces to the 
squared L2-norm, corresponding to the ridge 
penalty. 89
Elastic Net
• Example: Six variables, highly correlated in groups of 
three. The lasso estimates (α = 1), as shown in the 
left panel, exhibit somewhat erratic behavior as the 
regularization parameter λ is varied. In the right panel, 
the elastic net with (α = 0.3) includes all the variables, 
and the correlated groups are pulled together.
90
Elastic Net
• Of course, this example is idealized, and in practice 
the group structure will not be so cleanly evident. But 
by adding some component of the ridge penalty to the 
L1-penalty, the elastic net automatically controls for 
strong within-group correlations.
91
Elastic Net vs. Lasso Constraints
• Figure 4.2 compares the constraint region for the 
elastic net (left image) to that of the lasso (right 
image) when there are three variables. We see that 
the elastic-net ball shares attributes of the L2 ball and 
the L1 ball: the sharp corners and edges encourage 
selection, and the curved contours encourage sharing 
of coefficients. 
92
Dimension Reduction Methods
•The methods that we have discussed 
so far in this chapter have involved 
fitting linear regression models, via 
least squares or a shrunken 
approach, using the original 
predictors, X1, X2, . . . , Xp.
93
Dimension Reduction Methods
•We now explore a class of approaches 
that transform the predictors and then 
fit a least squares model using the 
transformed variables.
• We will refer to these techniques as 
dimension reduction methods.
94
Dimension Reduction Methods
•Note that they are only feature 
transformation methods, not feature 
selection methods.
95
Dimension Reduction Methods: details
• Let Z1, Z2, …, ZM represent M < p linear 
combinations of our original p predictors. 
That is,
(1)
for some constants φm1,φm2,…. φmp.
• We can then fit the linear regression model,
(2)
using ordinary least squares.
Dimension Reduction Methods: details
• Note that in the model
the regression coefficients are given by 
θ1, θ2,…, θM.
• If the constants φm1,φm2,…. φmp are 
chosen wisely, then such dimension 
reduction approaches can often 
outperform OLS regression/reduce 
variance.
Notice that from definition (1),
where
(3)
Hence model (2) can be thought of 
as a special case of the original 
linear regression model.
• Dimension reduction serves to 
constrain the estimated βj
coefficients, since now they 
must take the form (3).
• Can win in the bias-variance 
tradeoff.
Principal Components Regression
•Here we apply principal components 
analysis (PCA) (discussed in 
Chapter 10 of the text) to define the 
linear combinations of the 
predictors, for use in our regression.
Principal Components Regression
•The first principal component is that 
(normalized) linear combination of 
the variables with the largest 
variance.
Principal Components Regression
•The second principal component 
has largest variance, subject to 
being uncorrelated with the first.
•And so on.
Principal Components Regression
•Hence with many correlated original 
variables, we replace them with a 
small set of principal components that 
capture their joint variation.
Pictures of PCA
The population size (pop) and ad spending (ad) for 
100 different cities are shown as purple circles. The 
green solid line indicates the first principal 
component, and the blue dashed line indicates the 
second principal component.
Pictures of PCA: continued
A subset of the advertising data. Left: The first 
principal component, chosen to minimize the sum 
of the squared perpendicular distances to each 
point, is shown in green. These distances are 
represented using the black dashed line segments. 
Right: The left-hand panel has been rotated so that 
the first principal component lies on the x-axis.
Pictures of PCA: continued
Plots of the first principal component 
scores zi1 versus pop and ad. The 
relationships are strong.
Pictures of PCA: continued
Plots of the second principal component 
scores zi2 versus pop and ad. The
relationships are weak.
Application to Principal Components
Regression
PCR was applied to two simulated data sets. The 
black, green, and purple lines correspond to squared 
bias, variance, and test mean squared error, 
respectively. Left: Simulated data 2 (Sparse) Right: 
Simulated data 1 (Non-sparse)
Choosing the number of directions M
Left: PCR standardized coefficient estimates on
the Credit data set for different values of M .
Right: The 10-fold cross validation MSE obtained
using PCR, as a function of M .
Partial Least Squares
•PCR identifies linear combinations, or 
directions, that best represent the
predictors X1, . . . , Xp.
•These directions are identified in an 
unsupervised way, since the response 
Y is not used to help determine the 
principal component directions.
Partial Least Squares
•That is, the response does not 
supervise the identification of the 
principal components.
Partial Least Squares
•Consequently, PCR suffers from a 
potentially serious drawback: there is 
no guarantee that the directions that 
best explain the predictors will also be 
the best directions to use for predicting 
the response.
Partial Least Squares: continued
•Like PCR, PLS is a dimension 
reduction method, which first identifies 
a new set of features Z1, . . . , ZM that 
are linear combinations of the original 
features, and then fits a linear model 
via OLS using these M new features.
Partial Least Squares: continued
•But unlike PCR, PLS identifies these 
new features in a supervised way –
that is, it makes use of the response Y 
in order to identify new features that 
not only approximate the old features 
well, but also that are related to the 
response.
Partial Least Squares: continued
•Roughly speaking, the PLS approach
attempts to find directions that help
explain both the response and the
predictors.
Details of Partial Least Squares
•After standardizing the p predictors, PLS 
computes the first direction Z1 by setting 
each φ1j equal to the coefficient from the 
simple linear regression of Y onto Xj .
Details of Partial Least Squares
• One can show that this coefficient is 
proportional to the correlation between Y and 
Xj . Hence, in computing , 
PLS places the highest weight on the 
variables that are most strongly related to 
the response.
Details of Partial Least Squares
•Subsequent directions are found by 
taking residuals and then repeating the 
above prescription.
Summary
•Model selection methods are an 
essential tool for data analysis,
especially for big datasets involving 
many predictors.
Summary
•Research into methods that give 
sparsity, such as the lasso is an 
especially hot area.
•Later, we will return to sparsity in 
SVMs. 
DSCI 552, Machine Learning for Data 
Science
University of Southern California
M. R. Rajati, PhD
1
INF 552 - © M R Rajati - 2018 2
Tree-Based Methods
Tree-based Methods
•Here we describe tree-based methods 
for regression and classification.
•These involve stratifying or segmenting 
the predictor space into a number of 
simple regions.
•Since the set of splitting rules used to 
segment the predictor space can be 
summarized in a tree, these types of 
approaches are known as decision-tree
methods.
Pros and Cons
•Tree-based methods are simple 
and useful for interpretation.
•However they typically are not 
competitive with the best supervised 
learning approaches in terms of 
prediction accuracy.
Pros and Cons
•Hence we also discuss bagging, 
random forests, and boosting. These 
methods grow multiple trees which are 
then combined to yield a single 
consensus prediction.
•Combining a large number of trees can
often result in dramatic improvements
in prediction accuracy, at the expense
of some loss of interpretation.
The Basics of Decision Trees
•Decision trees can be applied 
to both regression and 
classification problems.
•We first consider regression 
problems, and then move on to 
classification.
Baseball salary data: how would 
you stratify it?
Salary is color-coded from low (blue, green) to 
high (yellow,red)
Decision tree for these data
| Years < 4.5
Hits <117.5
5.11
6.00 6.74
Details of previous figure
•For the Hitters data, a regression 
tree for predicting the log salary of 
a baseball player, based on the 
number of years that he has played 
in the major leagues and the 
number of hits that he made in the 
previous year.
Details of previous figure
•At a given internal node, the label (of 
the form Xj < tk) indicates the left-hand 
branch emanating from that split, and 
the right-hand branch corresponds to Xj 
≥ tk. For instance, the split at the top of 
the tree results in two large branches. 
The left-hand branch corresponds to 
Years<4.5, and the right-hand branch 
corresponds to Years ≥ 4.5.
Details of previous figure
•The tree has two internal nodes and 
three terminal nodes, or leaves. The 
number in each leaf is the mean of 
the response for the observations 
that fall there.
Results
• Overall, the tree stratifies or segments the players 
into three regions of predictor space: R1 ={X | 
Years< 4.5}, R2 ={X | Years ≥ 4.5, Hits<117.5}, and 
R3 ={X | Years ≥ 4.5, Hits ≥ 117.5}.
Terminology for Trees
•In keeping with the tree analogy, 
the regions R1, R2, and R3 are 
known as terminal nodes
•Decision trees are typically 
drawn upside down, in the 
sense that the leaves are at 
the bottom of the tree.
Terminology for Trees
•The points along the tree where 
the predictor space is split are 
referred to as internal nodes
•In the hitters tree, the two 
internal nodes are indicated by 
the text Years<4.5 and
Hits<117.5.
Interpretation of Results
•Years is the most important factor 
in determining Salary, and 
players with less experience earn 
lower salaries than more
experienced players.
•Given that a player is less 
experienced, the number of Hits 
that he made in the previous year 
seems to play little role in his
Salary.
Interpretation of Results
•For players with an experience 
of five or more years, the 
number of Hits made in the 
previous year does affect 
Salary, and players who made 
more Hits last year tend to 
have higher salaries.
Interpretation of Results
•Surely an over-simplification, 
but compared to a regression 
model, it is easy to display, 
interpret and explain
Details of the tree-building
process
1.We divide the predictor space — that is, 
the set of possible values for X1, X2, . . . , 
Xp — into J distinct and non-overlapping 
regions, R1, R2, . . . , RJ .
2.For every observation that falls into the 
region Rj , we make the same prediction, 
which is simply the mean of the response 
values for the training observations in Rj.
More details of the tree-building
process
•In theory, the regions could have 
any shape. However, we choose to 
divide the predictor space into high
dimensional rectangles, or boxes, 
for simplicity and for ease of 
interpretation of the resulting 
predictive model.
More details of the tree-building
process
•The goal is to find boxes R1, . . . , 
RJ that minimize the RSS, given
by
•where yˆRj is the mean response for 
the training observations within 
the jth box.
More details of the tree-building process
•Unfortunately, it is computationally 
infeasible to consider every 
possible partition of the feature 
space into J boxes.
•For this reason, we take a topdown, greedy approach that is 
known as recursive binary splitting.
More details of the tree-building process
•The approach is top-down 
because it begins at the top of the 
tree and then successively splits 
the predictor space; each split is 
indicated via two new branches 
further down on the tree.
More details of the tree-building process
•It is greedy because at each step of 
the tree-building process, the best 
split is made at that particular step, 
rather than looking ahead and 
picking a split that will lead to a 
better tree in some future step.
Details— Continued
•We first select the predictor Xj and 
the cutpoint s such that splitting the 
predictor space into the regions
{X|Xj < s} and {X|Xj ≥ s} leads to the 
greatest possible reduction in RSS.
•Next, we repeat the process, looking for 
the best predictor and best cutpoint in 
order to split the data further so as to 
minimize the RSS within each of the 
resulting regions.
Details— Continued
•However, this time, instead of splitting the
entire predictor space, we split one of the
two previously identified regions. We now
have three regions.
•Again, we look to split one of these three 
regions further, so as to minimize the RSS. 
The process continues until a stopping 
criterion is reached; for instance, we may 
continue until no region contains more 
than five observations.
Predictions
•We predict the response for a 
given test observation using the 
mean of the training observations 
in the region to which that test 
observation belongs.
•A five-region example of this 
approach is shown in the next 
slide.

Details of previous figure
Top Left: A partition of two-dimensional 
feature space that could not result from 
recursive binary splitting.
Top Right: The output of recursive binary 
splitting on a two-dimensional example.
Bottom Left: A tree corresponding to the 
partition in the top right panel.
Bottom Right: A perspective plot of the 
prediction surface corresponding to that
tree.
Pruning a tree
•The process described above may produce
good predictions on the training set, but is
likely to overfit the data, leading to poor
test set performance.Why?
•A smaller tree with fewer splits (that is,
fewer regions R1, . . . , RJ) might lead to
lower variance and better interpretation at
the cost of a little bias.
Pruning a tree
•One possible alternative to the 
process described above is to 
grow the tree only so long as the 
decrease in the RSS due to each 
split exceeds some (high)
threshold.
Pruning a tree
•This strategy will result in smaller 
trees, but is too short-sighted: a 
seemingly worthless split early on 
in the tree might be followed by a 
very good split — that is, a split
that leads to a large reduction in
RSS later on.
Pruning a tree— continued
•A better strategy is to grow a 
very large tree T0, and then
prune it back in order to obtain a
subtree
•Cost complexity pruning — also 
known as weakest link pruning 
— is used to do this
Pruning a tree— continued
• We consider a sequence of trees indexed by a
nonnegative tuning parameter α. For each value 
of α there corresponds a subtree T ⊂T0 such
that
is as small as possible. Here |T| indicates the 
number of terminal nodes of the tree T , R m is 
the rectangle (i.e. the subset of predictor space) 
corresponding to the mth terminal node, and 
yˆRm is the mean of the training observations in
R m .
Pruning a tree— continued
Choosing the best subtree
•The tuning parameter α controls a 
trade-off between the subtree’s 
complexity and its fit to the training
data.
•We select an optimal value α
ˆusing
cross-validation.
•We then return to the full data set 
and obtain the subtree 
corresponding to α
ˆ.
Summary: tree algorithm
1.Use recursive binary splitting to grow 
a large tree on the training data, 
stopping only when each terminal 
node has fewer than some minimum 
number of observations.
2.Apply cost complexity pruning to the 
large tree in order to obtain a 
sequence of best subtrees, as a 
function of α.
Summary: tree algorithm
3. Use K-fold cross-validation to choose α. For 
each
k = 1, . . . , K:
K 3.1Repeat Steps 1 and 2 on the K−1 th fraction of 
the training data, excluding the kth fold.
3.2Evaluate the mean squared prediction error on 
the data in the left-out kth fold, as a function of 
α.
Average the results, and pick α to minimize the 
average error.
4.Return the subtree from Step 2 that corresponds 
to the chosen value of α.
Baseball example continued
•First, we randomly divided the 
data set in half, yielding 132 
observations in the training set 
and 131 observations in the test
set.
Baseball example continued
•We then built a large regression 
tree on the training data and 
varied α in in order to create 
subtrees with different numbers 
of terminal nodes.
•Finally, we performed six-fold 
cross-validation in order to 
estimate the cross-validated MSE 
of the trees as a function of α.
Baseball example continued
Baseball example continued
2 4 6 8 10
0.0 0.2 0.4 0.6 0.8 1.0
Tree Size
Mean Squared Error
Training 
Cross−Validation 
Test
Classification Trees
•Very similar to a regression tree, 
except that it is used to predict a 
qualitative response rather than a 
quantitative one.
•For a classification tree, we predict
that each observation belongs to
the most commonly occurring class
of training observations in the
region to which it belongs.
Details of classification trees
•Just as in the regression setting, 
we use recursive binary splitting 
to grow a classification tree.
•In the classification setting, RSS 
cannot be used as a criterion 
for making the binary splits
Details of classification trees
•A natural alternative to RSS is the 
classification error rate, which is 
simply the fraction of the training 
observations in that region that do 
not belong to the most common
class:
•Here p
ˆ
mk represents the proportion 
of training observations in the mth
region that are from the kth class.
E = 1 − max(pˆ mk).
Example
A simple example:
Details of classification trees
•However classification error is not 
sufficiently sensitive for treegrowing, and in practice two other 
measures are preferable.
Gini index and Deviance
•The Gini index is defined by
a measure of total variance across
the K classes. The Gini index
takes on a small value if all of the
p
ˆ
mk’s are close to zero or one.
Gini index and Deviance
•For this reason the Gini index is referred
to as a measure of node purity — a small
value indicates that a node contains
predominantly observations from a single
class.
•The Tree Algorithm in this case is called
CART (Classification and Regression
Trees)
Gini index and Deviance
•An alternative to the Gini index is 
cross-entropy, given by
•The tree algorithms that use cross 
entropy are ID3, C4.5, and C5.0
• It turns out that the Gini index and the 
cross-entropy are very similar 
numerically.
Comparison of Impurity Measures
•For a two-class problem
Weighted Impurity
• Most of the time, to be more fair, we 
add up the weighted impurity of the 
regions resulting from a split.
•We choose weights to be the fraction
of data points in each region.
100
10
300 300 90
Example: heart data
•These data contain a binary outcome 
HD for 303 patients who presented 
with chest pain.
•An outcome value of Yes indicates the 
presence of heart disease based on 
an angiographic test, while No means 
no heart disease.
Example: heart data
•There are 13 predictors including 
Age, Sex, Chol (a cholesterol 
measurement), and other heart 
and lung function
measurements.
•Cross-validation yields a tree with six 
terminal nodes. See next figure.

Trees Versus Linear Models
Top Row: True linear boundary; Bottom row: true 
non-linear boundary.
Left column: linear model; Right column: tree-based
model
Advantages and Disadvantages of Trees
• Trees are very easy to explain to 
people. In fact, they are sometimes
even easier to explain than linear
regression!
Advantages and Disadvantages of Trees
• Some people believe that decision 
trees more closely mirror human 
decision-making than do the 
regression and classification 
approaches seen in previous
chapters.
Advantages and Disadvantages of Trees
• Trees can be displayed graphically, 
and are easily interpreted even by 
a non-expert (especially if they are 
small).
• Trees can easily handle qualitative 
predictors without the need to 
create dummy variables.
Advantages and Disadvantages of Trees
• Unfortunately, trees generally do not 
have the same level of predictive 
accuracy as some of the other regression 
and classification approaches seen in 
this book.
However, by aggregating many decision 
trees, the predictive performance of 
trees can be substantially improved. We 
introduce these concepts next.
Bagging
•Bootstrap aggregation, or bagging, is a 
general-purpose (wrapper) procedure 
for reducing the variance of a 
statistical learning method; we 
introduce it here because it is 
particularly useful and frequently 
used in the context of decision trees.
• It is an ensemble method.
Bagging
•Recall that given a set of n independent
observations Z1, . . . , Zn, each with 
variance σ2, the variance of the mean Z¯ 
of the observations is given by σ2/n.
• In other words, averaging a set of 
observations reduces variance. Of 
course, this is not practical because 
we generally do not have access to 
multiple training sets.
Bagging— continued
• Instead, we can bootstrap, by taking repeated 
samples from the (single) training data set.
• In this approach we generate B different 
bootstrapped training data sets. We then train 
our method on the bth bootstrapped training set 
in order to get fˆ∗b(x), the prediction at a point x. 
We then average all the predictions to obtain
This is called bagging.
Bagging classification trees
•The above prescription can be applied to 
regression trees
•For classification trees: for each test 
observation, we record the class predicted 
by each of the B trees, and take a majority 
vote: the overall prediction is the most 
commonly occurring class among the B
predictions.
Out-of-Bag Error Estimation
•It turns out that there is a very
straightforward way to estimate 
the test error of a bagged model.
Out-of-Bag Error Estimation
•Recall that the key to bagging is that 
trees are repeatedly fit to 
bootstrapped subsets of the 
observations.
•One can show that on average, 
each bagged tree makes use of 
around two-thirds of the
observations.
Out-of-Bag Error Estimation
•The remaining one-third of the observations
not used to fit a given bagged tree are called
the out-of-bag (OOB) observations.
Out-of-Bag Error Estimation
•We can predict the response for the i
th
observation using each of the trees in 
which that observation was OOB. This 
will yield around B/3 predictions for the 
i
th observation, which we average.
•This estimate is essentially the LOO 
cross-validation error for bagging, if B 
is large.
Random Forests
•Random forests provide an 
improvement over bagged trees 
by way of a small tweak that
decorrelates the trees. This 
reduces the variance when we 
average the trees.
•As in bagging, we build a 
number of decision trees on 
bootstrapped training samples.
Random Forests
•But when building these decision 
trees, each time a split in a tree is 
considered, a random selection of m 
predictors is chosen as split 
candidates from the full set of p 
predictors. The split is allowed to 
use only one of those m predictors.
Random Forests
•A fresh selection of m predictors is 
taken at each split, and typically we 
choose m ≈ p1/2 — that is, the 
number of predictors considered at 
each split is approximately equal to 
the square root of the total number 
of predictors (4 out of the 13 for the 
Heart data).
Random Forests: An Alternative Version
•Alternatively, one can build B
decision trees, each of which on a 
random subset of m features.
•Instead of a bootstrap sample from 
the training set, one can even use a 
subsample with replacement from 
the training set, which is equivalent 
to a bootstrap sample with a size 
different from the training set.
Bagging the heart data
Details of previous figure
Bagging and random forest results for the 
Heart data.
• The test error (black and orange) is shown 
as a function of B, the number of
bootstrapped training sets used.
• Random forests were applied with m = p 1/2 .
• The dashed line indicates the test error 
resulting from a single classification tree.
• The green and blue traces show the OOB 
error, which in this case is considerably 
lower
Example: gene expression data
•We applied random forests to a 
high-dimensional biological data 
set consisting of expression 
measurements of 4,718 genes 
measured on tissue samples from
349 patients.
Example: gene expression data
•There are around 20,000 genes 
in humans, and individual genes 
have different levels of activity, or 
expression, in particular cells, 
tissues, and biological
conditions.
Example: gene expression data
•Each of the patient samples has a
qualitative label with 15 different levels:
either normal or one of 14 different types
of cancer.
•We use random forests to predict 
cancer type based on the 500 genes 
that have the largest variance in the 
training set.
Example: gene expression data
•We randomly divided the observations 
into a training and a test set, and 
applied random forests to the training 
set for three different values of the 
number of splitting variables m.
Results: gene expression data
0 100 400 500
0.2 0.3 0.4 0.5
200 300
Number ofTrees
Test Classification Error
m=p
m=p/2
m= p
Details of previous figure
•Results from random forests for 
the fifteen-class gene expression 
data set with p = 500 predictors.
•The test error is displayed as a 
function of the number of trees. 
Each colored line corresponds to a 
different value of m, the number of 
predictors available for splitting at 
each interior tree node.
Details of previous figure
•Random forests (m < p) lead to a 
slight improvement over bagging (m 
= p). A single classification tree has 
an error rate of 45.7%.
Boosting
•Like bagging, boosting is a 
general approach that can be 
applied to many statistical 
learning methods for regression 
or classification. We only discuss 
boosting for decision trees.
Boosting
•Recall that bagging involves 
creating multiple copies of the 
original training data set using 
the bootstrap, fitting a separate 
decision tree to each copy, and 
then combining all of the trees in 
order to create a single predictive 
model.
Boosting
•Notably, each tree is built on a 
bootstrap data set, 
independent of the other
trees.
•Boosting works in a similar way, 
except that the trees are grown 
sequentially: each tree is grown 
using information from previously 
grown trees.
Boosting algorithm for regression trees
1.Set fˆ(x) = 0 and ri = yi for all i in the 
training set. 
2.For b = 1, 2, . . . , B, repeat:
1. Fit a tree fˆb with d splits (d +1 
terminal nodes) to the training data 
(X, r).
2. Update fˆ by adding in a shrunken 
version of the new tree:
fˆ(x) ← fˆ(x) + λfˆb(x).
3. Update the residuals,
ri ← ri − λfˆb(xi).
Boosting algorithm for regression
trees
Output the boosted model,
•Unlike fitting a single large decision
tree to the data, which amounts to
fitting the data hard and potentially
overfitting, the boosting approach
instead learns slowly.
What is the idea behind this procedure?
•Given the current model, we fit a 
decision tree to the residuals 
from the model. We then add this 
new decision tree into the fitted 
function in order to update the 
residuals.
What is the idea behind this procedure?
What is the idea behind this procedure?
•Each of these trees can be rather 
small, with just a few terminal nodes, 
determined by the parameter d in the 
algorithm.
What is the idea behind this procedure?
•By fitting small trees to the residuals, we 
slowly improve fˆ in areas where it does 
not perform well. The shrinkage 
parameter λ slows the process down 
even further, allowing more and different 
shaped trees to attack the residuals.
Boosting for classification
•Boosting for classification is similar in 
spirit to boosting for regression, but is a 
bit more complex. We will not go into 
detail here (see appendix), nor does the 
text book.
• More details in Elements of 
Statistical Learning, chapter 10 or 
Appendix.
•The R package gbm (gradient boosted 
models) handles a variety of
regression and classification problems.
Gene expression data
continued
0 1000 4000 5000
0.05 0.10 0.15 0.20 0.25
2000 3000
Number of Trees
Test Classification Error
Boosting: depth=1 
Boosting: depth=2 
RandomForest: m= p
Details of previous figure
•Results from performing boosting and 
random forests on the fifteen-class 
gene expression data set in order to 
predict cancer versus normal.
Details of previous figure
•The test error is displayed as a function 
of the number of trees. For the two 
boosted models, λ = 0.01. Depth-1 trees 
slightly outperform depth-2 trees, and 
both outperform the random forest, 
although the standard errors are around 
0.02, making none of these differences
significant.
•The test error rate for a single tree is
24%.
Tuning parameters for boosting
1.The number of trees B. Unlike bagging and 
random forests, boosting can overfit if B is too 
large, although this overfitting tends to occur 
slowly if at all. We use cross-validation to select
B.
Tuning parameters for boosting
2.The shrinkage parameter λ, a small positive 
number. This controls the rate at which 
boosting learns. Typical values are 0.01 or 
0.001, and the right choice can depend on the 
problem. Very small λ can require using a very 
large value of B in order to achieve good
performance.
Tuning parameters for boosting
3. The number of splits d in each tree, 
which controls the complexity of 
the boosted ensemble. Often d = 1 
works well, in which case each tree 
is a stump, consisting of a single 
split and resulting in an additive 
model. More generally d is the 
interaction depth, and controls the 
interaction order of the boosted 
model, since d splits can involve at 
most d variables.
Another regression example
Another classification example
Variable importance measure
•For bagged/RF regression trees, 
we record the total amount that 
the RSS is decreased due to 
splits over a given predictor, 
averaged over all B trees. A large 
value indicates an important
predictor.
Variable importance measure
•Similarly, for bagged/RF
classification trees, we add up the
total amount that the Gini index
is decreased by splits over a
given predictor, averaged over all
B trees.
Variable importance measure
Fbs 
RestECG 
ExAng 
Sex 
Slope 
Chol
Variable 
importance 
plot for the 
Heart data
Summary
•Decision trees are simple and 
interpretable models for regression and
classification
•However they are often not 
competitive with other methods in 
terms of prediction accuracy
Summary
•Bagging, random forests and boosting are 
good methods for improving the prediction 
accuracy of trees. They work by growing 
many trees on the training data and then 
combining the predictions of the resulting 
ensemble of trees.
•The latter two methods— random forests 
and boosting— are among the state-of-theart methods for supervised learning. 
However their results can be difficult to 
interpret.
Extra Trees
Appendix
• Extremely Randomized Trees 
were proposed by Geurts et al. in 
2006.
• The idea is to make the trees even 
weaker, but to compensate that 
with the large number of 
estimators in the ensemble. 
Boosting (Appendix)
More specifically, not only the 
features and samples shown to the 
tree are randomized, but also the 
growing of individual trees is random.
Boosting (Appendix)
• In particular the split point i.e., the 
thresholds of comparisons is 
randomized. This makes training 
each tree faster.
• Implemented as 
sklearn.ensemble.ExtraTreesClas
sifier.
Boosting (Appendix)
Boosting for Classification
Boosting iteratively learns weak 
classifiers; a weak classifier is one 
whose error rate is only slightly better 
than random guessing.
Boosting (Appendix)
Boosting for Classification
The purpose of boosting is to 
sequentially apply the weak classification 
algorithm to repeatedly modified versions 
of the data, thereby producing a 
sequence of weak classifiers.
The predictions from all of them are then 
combined through a weighted majority 
vote to produce the final prediction.
Boosting (Appendix)
Boosting for Classification 
(cont’d)
Thus, the final result is the 
weighted sum of the results of 
weak classifiers.
Boosting (cont.)
Boosting for Classification 
(cont’d)
The alphas in the final classifier 
are computed by the boosting 
algorithm, which weight the 
contribution to give higher 
influence to more accurate 
classifiers in the sequence.
Weights are modified at each 
boosting step!
Boosting (cont.)
Boosting (cont.)
Up-weight data that are difficult; incorrectly classified 
in the previous round. Down-weight data that are 
easy; correctly classified in the previous round.
There are many different boosting 
algorithms for classification: 
AdaBoost, LPBoost, BrownBoost, 
LogitBoost, Gradient Boosting, etc.
Boosting (cont.)
Boosting (cont.)
Example of Boosting Algorithm (AdaBoost):
Boosting Intuition
• We adaptively weigh each data case.
• Data cases which are wrongly classified get 
high weight (the algorithm will focus on them)
• Each boosting round learns a new (simple) 
classifier on the weighed dataset.
Boosting Intuition
• These classifiers are weighed to combine 
them into a single powerful classifier.
• Classifiers that that obtain low training error 
rate have high weight.
• We stop by using monitoring a hold out set 
(cross-validation). 
An Illustrative Example
Original training set: equal weights to all training 
samples
Taken from “A Tutorial on Boosting” by Yoav Freund and Rob Schapire
AdaBoost example
ROUND 1
ε = error rate of 
classifier
α = weight of classifier
AdaBoost example
ROUND 2
AdaBoost example
ROUND 3
AdaBoost example
Boosting (cont.)
Boosting is remarkably resistant to 
overfitting, and it is fast and simple.
In fact, it can often continue to improve 
even when the training error has gone 
to zero.
Boosting (cont.)
It improves the performance of 
many kinds of machine learning 
algorithms.
Boosting does not work when:
Not enough data, base learner is too 
weak or too strong, and/or 
susceptible to noisy data.
More
INF 552 - © M R Rajati - 2018 124
More
INF 552 - © M R Rajati - 2018 125
Appendix
More on Ensemble Methods 
Stacking
Stacking
x
NB
knn
svm
meta ŷ
DT
LR
knn
meta ŷ
• Basic idea: use the output of multiple 
classifiers as input to a meta-model
(meta-learner).
• We ‘stack’ the meta-model on top of 
the base models
classification regression
x
Stacking – a naïve approach
• Let’s consider the regression case
• Base model predictions are f1(x),. .., fL(x)
• Meta learner could be a linear regression 
model
• If we could choose wi to minimize true error, the 
stacked model would always be at least as good as 
any base model!
– Worst case we set all wi to 0 except that of the 
best base model
• But what if we minimize the train error instead?
Stacking – a naïve approach
• Consider the following base models
• What weights would minimize train set error?
• Does that yield good generalization error for the 
meta model?
Stacking – a naïve approach
• Naïve implementation of stacking 
prefers over-fitted models
• Underlying problem: the outputs of the 
base models have been adapted to the
labels.
Stacking – a naïve approach
• Thus, inputs of the meta model are 
not representative of the inputs it will 
get at test-time.
• To avoid preference for overfitted 
models, inputs to the meta- model 
should not have seen the labels for 
the data points themselves
Stacking – second attempt
• Now, we can train the meta-model on the 
data in the base model outputs paired with 
the target label
• Any base-model output is now a good 
indication of test-time behavior
• If the meta-model has free parameters itself, 
we can cross-validate using the same folds
Stacking – second attempt
• Usually, the meta-model is relatively simple 
(e.g. linear regression or logistic
regression)
• Empirical Recommendation: Do not have 
your base-learners as the meta-learner. 
For example, if you use logistic regression 
as the base-learner, use some other 
classifier (e.g. SVM) as the meta-learner.
Testing the stacked model
• To test the stacked model, again we set 
aside a test set from the very beginning
• Have several versions of the base 
models from cross-validation!
Testing the stacked model
• Two approaches:
– Retrain the base models on the whole dataset
• Possible disadvantage: slightly different input to metamodel
– Use an average of the trained base models
• Possible disadvantage: time cost
• Then feed the base model predictions into 
the trained meta- model
Comparison to model selection
• If we force meta-learner to use just one base 
model (with weight 1) and set all other 
weights to 0, this is equivalent to selecting 
the best model with cross-validation
• More expressive meta-models (e.g. linear / 
logistic regression) can leverage the 
relative strength of multiple models
Comparison to model selection
• A very complex meta-model (e.g. decision 
tree) could again easily overfit
• Could use cross-validation on the metalevel to ensure good generalization
properties
Effectiveness of stacking
• Stacking generally improves performance, 
but not by much
• Additional cost of training and evaluating 
multiple models
Effectiveness of stacking
• If interpretability or speed are important 
consideration, stacking might not help you much.
• In competitions where a small gain is important and 
time cost is not so much of an issue, it is usually
effective!
• Quite useful in collaborative approaches where 
everyone can integrate their own model in overall 
system
Adaptive Meta-Learners
• The meta-learner is a function that depends on 
the input feature vector 
• Thus, the ensemble implements a function that 
is local to each region in feature space 
• This divide-and-conquer approach leads to 
modular ensembles where relatively simple 
classifiers specialize in different parts of I/O 
space 
Adaptive Meta-Learners
• In contrast with static-combiner ensembles, the 
individual experts here do not need to perform 
well for all inputs, only in their region of 
expertise 
• Representative examples of this approach are 
Mixture of Experts (ME) and Hierarchical ME 
[Jacobs et al., 1991; Jordan and Jacobs, 1994] 
Adaptive Meta-Learners
Mixture of Experts
• ME is the classical adaptive ensemble 
method 
• A gating network is used to partition feature 
space into different regions, with one expert in 
the ensemble being responsible for generating 
the correct output within that region [Jacobs et 
al., 1991] 
Mixture of Experts
• The experts in the ensemble and the gating 
network are trained simultaneously.
• ME can be extended to a multi-level hierarchical 
structure, where each component is itself a ME. 
DSCI 552, Machine Learning for Data 
Science
University of Southern California
M. R. Rajati, PhD
1
2
Lesson 7
Support Vector Machines
Support Vector Machines
Here we approach the two-class 
classification problem in a direct way:
We try and find a plane that 
separates the classes in feature
space.
If we cannot, we get creative in two ways:
• We soften what we mean by “separates”,
and
• We enrich and enlarge the feature space so 
that separation is possible.
What is a Hyperplane?
• A hyperplane in p dimensions is a flat 
affine subspace of dimension p − 1.
• In general the equation for a hyperplane 
has the form
β0 + β1X1 + β2X2 + . . . + βpXp
= 0
• In p = 2 dimensions a hyperplane is a
line.
What is a Hyperplane?
• If β0 = 0, the hyperplane goes 
through the origin, otherwise
not.
• The vector β = (β1, β2, · · · , βp) is 
called the normal vector
— it points in a direction 
orthogonal to the surface of a 
hyperplane.
Hyperplane in 2 Dimensions
X1
X2
Separating Hyperplanes −1 0 2 3
−1 0 1 2 3 −1 0 1 2 3
X1 X1
X2
−1 0 1 2 3
X2
1
• If f (X) = β0 + β1X1 + · · · + βpXp, then f (X) > 0 for
points on one side of the hyperplane, and f (X) < 0 for 
points on the other.
• If we code the colored points as Yi = +1 for blue, 
say, and Yi = −1 for mauve, then if Yi · f (Xi) > 0 
for all i, f (X) = 0 defines a separating hyperplane.
Maximal Margin Classifier
−1 0 1 2 3
X1
X2
−1 0 1 2 3
Among all separating hyperplanes, find the one that makes the 
biggest gap or margin between the two classes.
This can be rephrased as a convex quadratic 
program, and solved efficiently. 
Constrained optimization problem
Maximal Margin Classifier
The constraint makes
sure that a unique solution for 
βi
's exists.
Combined with 
it guarantees that the minimum
margin is M if M is positive.
Maximal Margin Classifier
For each observation to be on the correct 
side of the hyperplane we would simply need
so the constraint
in fact requires that each observation be on 
the correct side of the hyperplane, with some 
cushion, provided that M is positive.
Maximal Margin Classifier
Since if 
Defines a hyperplane, then 
also defines a hyperplane for any nonzero k
guarantees a unique set of βi
's exists, 
Maximal Margin Classifier
Moreover, the constraint
adds meaning to 
one can show that with this constraint the 
perpendicular distance from the i th
observation to the hyperplane is given by
Non-separable Data
0 1 2 3
X1
X2
−1.0 −0.5 0.0 0.5 1.0 1.5 2.0
The data on the left are
not separable by a linear
boundary.
This is often the case,
unless N < p.
Noisy Data
−1
0
2
3
−1
0
2
3
−1 0 1 2 3 −1 0 1 2 3
X1 X1
X2
X
1
2
1
Sometimes the data are separable, but noisy. This 
can lead to a poor solution for the maximal-margin 
classifier. The support vector classifier maximizes a 
soft margin.
Support Vector Classifier
−0.5 0.0 0.5 1.0 1.5 2.0 2.5
−1 0 1 2 3 4
1
2
3
4 5
6
7
8
9
10
−0.5 0.0 0.5 1.0 1.5 2.0 2.5
−1 0 1 2 3 4
1
2
3
4 5
6
7
8
9
10
11
12
X1 X1
X2
X2
Support Vector Classifier
Details of Optimization Problem 
The slack variable εi tells us where the i
th
observation is located, relative to the 
hyperplane and relative to the margin.
• If εi = 0 then the i
th observation is on 
the correct side of the margin
Details of Optimization Problem 
The slack variable εi tells us where the i
th
observation is located, relative to the 
hyperplane and relative to the margin.
• If εi
> 0 then the i
th observation is on 
the wrong side of the margin, and we 
say that the i
th observation has 
violated the margin. 
Details of Optimization Problem 
The slack variable εi tells us where the i
th
observation is located, relative to the 
hyperplane and relative to the margin.
• If εi >1 then it is on the wrong side of 
the hyperplane.
Details of Optimization Problem 
• C bounds the sum of the εi
’s, and so it 
determines the number and severity of 
the violations to the margin (and to the 
hyperplane) that we will tolerate.
Details of Optimization Problem 
• We can think of C as a budget for the 
amount that the margin can be 
violated by the N observations.
• If C = 0 then there is no budget for 
violations to the margin, so all εi
’s = 0, 
which leads to the maximal margin 
hyperplane optimization problem
Details of Optimization Problem 
For C > 0 no more than C observations
can be on the wrong side of the hyperplane, 
because if an observation is on the wrong 
side of the hyperplane then εi >1, and the 
optimization problem requires that the sum of 
εi
’s be less than C.
Details of Optimization Problem 
• As the budget C increases, we become 
more tolerant of violations to the margin, 
and so the margin will widen.
• Conversely, as C decreases, we become 
less tolerant of violations to the margin and 
so the margin narrows. 
• An example is shown in the next 
slide.
C is a regularization parameter
C is a regularization parameter
A support vector classifier was fit using four 
different values of the tuning parameter C. 
• The largest value of C was used in the 
top left panel, and smaller values were 
used in the top right, bottom left, and 
bottom right panels. 
C is a regularization parameter
A support vector classifier was fit using four 
different values of the tuning parameter C. 
• When C is large, there is a high 
tolerance for observations being on the 
wrong side of the margin, and so the 
margin will be large.
• As C decreases, the tolerance for 
observations being on the wrong side of 
the margin decreases, and the margin 
narrows.
C is a regularization parameter
• C is treated as a tuning parameter 
generally chosen via cross-validation. 
• C controls the bias-variance trade-off of 
the statistical learning technique. 
C is a regularization parameter
• When C is small, we seek narrow 
margins that are rarely violated; this 
amounts to a classifier that is highly fit to 
the data, which may have low bias but 
high variance. 
C is a regularization parameter
• On the other hand, when C is larger, the 
margin is wider and we allow more 
violations to it; this amounts to fitting the 
data less hard and obtaining a classifier 
that is potentially more biased but may 
have lower variance.
Support Vectors
• Only observations that either lie on 
the margin or that violate the margin 
will affect the hyperplane, and 
hence the classifier obtained.
• In other words, an observation that 
lies strictly on the correct side of the 
margin does not affect the support 
vector classifier!
Support Vectors
• Changing the position of that 
observation would not change the 
classifier at all, provided that its 
position remains on the correct side 
of the margin. 
Support Vectors
• Observations that lie directly on the 
margin, or on the wrong side of the 
margin for their class, are known as 
support vectors . These 
observations do affect the support 
vector classifier.
Support Vectors
The fact that the support vector 
classifier’s decision rule is based only
on a potentially small subset of the 
training observations (the support 
vectors) means that it is quite robust 
to the behavior of observations that
are far away from the hyperplane. 
Comparison with LDA
• This property is distinct from some 
of the other classification methods 
that we have seen, such as linear 
discriminant analysis. 
• The LDA classification rule 
depends on the mean of all of the 
observations within each class, as 
well as the within-class covariance 
matrix computed using all of the 
observations.
Comparison with LR
In contrast, logistic regression, unlike 
LDA, has very low sensitivity to 
observations far from the decision 
boundary. In fact we will see that the 
support vector classifier and logistic 
regression are closely related.
Linear boundary can fail
−4 −2 0 2 4
−4 −2
0
2
4
X1
X2
Sometimes a linear
boundary simply won’t
work, no matter what
value of C.
The example on the
left is such a case.
What to do?
Feature Expansion
Enlarge the space of features by 
including transformations; e.g. X1
2
, X1
3
, 
X1X2, X1X2
2
,. . .. Hence go from a pdimensional space to a M > p 
dimensional space.
• Fit a support-vector classifier in the 
enlarged space.
• This results in non-linear decision 
boundaries in the original space.
Feature Expansion
Example: Suppose we use (X1, X2, X12
, 
X22
, X1X2) instead of just (X1, X2). Then 
the decision boundary would be of the form
β0 + β1X1 + β2X2 + β3X1
2 + β4X2
2 +
β5X1X2 =0
This leads to nonlinear decision boundaries in 
the original space (quadratic conic sections).
Cubic Polynomials
Here we use a basis
expansion of cubic polynomials
From 2 variables to 9
The support-vector classifier
in the enlarged space solves
the problem in the lowerdimensional space
−4 −2 0 2 4
−4 −2
0
2
4
X1
X2
β0 +β1X1 +β2X2 +β3X12 +β4X22 +β5X1X2 +β6X13
+β7X23 +β8X1X22 +β9X12X2 = 0
Nonlinearities and Kernels
• Polynomials (especially highdimensional ones) get wild rather fast.
• There is a more elegant and controlled 
way to introduce nonlinearities in 
support-vector classifiers — through the 
use of kernels.
• Before we discuss these, we must 
understand the role of inner products in 
support-vector classifiers.
Inner products and support
vectors
— inner product between vectors
• The linear support vector classifier can be 
represented as
with n parameters
Subject to 
αi
i=1
n
∑ = 0
Inner products and support
vectors
• The linear support vector classifier can be 
represented as
The intercept can be calculated as:
where xj is one of the support vectors and 
yj is its label.
β0
=
1
yj
− αi
〈xi
, xj
〉
i=1
N
∑
Inner products and support
vectors
• The linear support vector classifier can be 
represented as
with n parameters
in order to evaluate the function f(x), we 
need to compute the inner product between 
the new point x and each of the training
points xi .
αi
i=1
n
∑ = 0
Inner products and support
vectors
To estimate the parameters α1,…, αn and 
β0, all we need are the inner products 
<xi
,xi’
> between all pairs of
training observations.
However, it turns out that αi is nonzero only 
for the support vectors in the solution—that 
is, if a training observation is not a support
vector, then its αi equals zero.
Inner products and support
vectors
In other words
where S is the support set of indices i for 
support vectors.
α
ˆ
i
i∈S
∑ = 0
β0
=
1
yj
− αˆi
〈xi
, xj
〉
i∈S
∑
Inner products and support
vectors
To expand the feature space, one can use a 
transformed set of features u = φ(x). Note 
that u does not need to be of the same 
dimension as x.
The classifier in the new feature space can 
be represented as
f
1(x) = f (ϕ(x)) = β0 + αˆi
〈ϕ(x),ϕ(xi
)〉
i∈S
∑
Inner products and support
vectors
It can be shown that a class of functions 
called kernels can be represented as inner 
products < >
The interesting point is that we do not need 
to explicitly know the function φ(x) to use 
the kernel that correspond to it!
They are generalizations of the inner 
product.
Kernels and Support Vector
Machines
Now suppose that every time the inner product 
appears in our equations, we replace it with
K(xi
,xi’
)
where K is a kernel . 
A kernel is a function that quantifies the 
similarity of two observations. 
For SVC, the Kernel is the usual inner product, 
which is called a linear kernel, because φ(x) = 
x. 
• A function K(xi
,xj
) is a Kernel function, i.e. 
it can be written in the following form 
if and only if it satisfies Mercer’s condition.
Aside: Mercer’s Condition
K(xi
, xj
) = ϕ(xi
) ⎡
⎣ ⎤
⎦
T
ϕ(xj
) ⎡
⎣
⎢ ⎤
⎦
⎥
• Mercer’s condition: for all finite 
sequences (x1,x2,…,xn) and all choices 
from the sequence of real numbers 
(c1,c2,…,cn) :
Aside: Mercer’s Condition
ci
cj
K(xi
, xj
)
j=1
n
∑
i=1
n
∑ ≥0
Kernels and Support Vector
Machines
For example
computes the inner-products needed for d 
dimensional polynomials-basis functions!
Kernels and Support Vector
Machines
Try it for p = 2 and d = 2.
• The solution has the form
Radial Kernel
−4 −2 0 2 4
−4 −2
0
2
4
X1
X2
Implicit feature space;
very high dimensional.
Controls variance by
squashing down most
dimensions severely.
RBF Kernel
• If a given test observation x∗= (x1
∗ . . .x p
∗ )T
is far from a training observation xi in terms 
of Euclidean distance, then the exponent 
will be very negative, and so K (x∗
, xi ) will 
be very tiny.
RBF Kernel
• Therefore, xi will play virtually no role in 
f(x∗). 
• Recall that the predicted class label for the 
test observation x∗ is based on the sign of 
f(x∗).
• The radial kernel has very local behavior: 
only nearby training observations have an 
effect on the class label of a test 
observation. (Similar to?)
Computational Advantage
• What is the advantage of using a kernel 
rather than simply enlarging the feature 
space using functions of the original 
features?
• One advantage is computing without 
explicitly working in the enlarged feature 
space. 
Computational Advantage
• This is important because in many 
applications of SVMs, the enlarged 
feature space is so large that 
computations are intractable.
• For some kernels, such as the radial, the 
feature space is implicit and infinitedimensional, so we could never do the 
computations there anyway!
Example: Heart Data
ROC curve is obtained by changing the threshold 
0 to threshold t in fˆ(X) > t, and recording false 
positive and true positive rates as t varies. Here 
we see ROC curves on training data.
Example continued: Heart 
Test Data
Multi-Class and Multi-Label 
Classification Revisited
Multiclass classification means a classification 
task with more than two classes; e.g., classify a 
set of images of animals which may be horses, 
birds, or fish. 
Multiclass classification makes the assumption 
that each sample is assigned to one and only one 
label: an animal can be either a horse or a bird 
but not both at the same time.
SVMs: more than 2 classes?
The SVM as defined works for K = 2 
classes. What do we do if we have K > 2
classes?
• OVA One versus All (The rest). Fit K 
different 2-class SVM classifiers f
ˆ
k(x), 
k = 1, . . . , K; each class versus the 
rest. Classify x∗ to the class for which 
fˆ
k(x∗) is largest.
SVMs: more than 2 classes?
The SVM as defined works for K = 2 
classes. What do we do if we have K > 2
classes?
• OVO One versus One. Fit all 
pairwise classifiers f
ˆ
kl(x). Classify x*
to the class that wins the most 
pairwise competitions.
Which to choose? If K is not too large, use
OVO.
Multi-Class and Multi-Label Problems
Multilabel classification assigns to each sample 
a set of target labels. This can be thought as 
predicting properties of a data-point that are not 
mutually exclusive, such as topics that are 
relevant for a document.
A text might be about any of religion, politics, 
finance or education at the same time or none of 
these.
63
Multi-Class and Multi-Label 
Classification Revisited
• Three methods to solve a multilabel classification problem:
• Problem Transformation
• Transform multi-label problem into 
single-label problem(s)
• Adapted Algorithms
• adapting conventional algorithms to 
directly perform multi-label classification
• Ensemble approaches
• Combining multiple classifiers
Multi-Class and Multi-Label 
Classification Revisited
• Problem Transformation
• Binary Relevance
• Classifier Chains
• Label Powerset
Multi-Class and Multi-Label 
Classification Revisited
• Binary Relevance
• The simplest technique, which treats each 
label as a separate binary or multi-class 
classification problem.
• Example: consider a case as shown below. 
X is the independent feature and Y’s are 
the target variable.
Multi-Class and Multi-Label 
Classification Revisited
• Binary Relevance
• Solution: The problem is broken into 4 
different single class classification 
problems.
Multi-Class and Multi-Label 
Classification Revisited
• Binary Relevance
• Solution: For a new data point x*, each of 
the labels Y1,…, Y4 is predicted separately.
• Note1: Any classifier (e.g. Naïve Baye’s, 
Logistic Regression, and SVM) can be 
used for predicting each label
• Note 2: Each label may give rise to a 
binary or multi-class classification problem.
Multi-Class and Multi-Label 
Classification Revisited
• Classifier Chains
• In this approach, the first classifier is 
trained just on the input data and then 
each next classifier is trained on the 
input space and all the previous 
classifiers in the chain.
Multi-Class and Multi-Label 
Classification Revisited
• Example: X as the input space and 
Y’s as the labels.
Multi-Class and Multi-Label 
Classification Revisited
• Solution: In classifier chains, this 
problem would be transformed into 4 
different single label problems, just 
like shown below. Here yellow 
colored is the input space and the 
white part represent the target 
variable.
Multi-Class and Multi-Label 
Classification Revisited
• Note: This is quite similar to 
binary relevance, the only 
difference being it forms chains in 
order to preserve label 
correlation.
Multi-Class and Multi-Label 
Classification Revisited
• Label Powerset
• This method transforms the problem 
into a multi-class problem with one 
multi-class classifier is trained on all 
unique label combinations found in 
the training data.
Multi-Class and Multi-Label 
Classification Revisited
• Example: X are features, Y1,…Y4 are 
labels
Multi-Class and Multi-Label 
Classification Revisited
• Solution: x1 and x4 have the same 
labels, similarly, x3 and x6 have the 
same set of labels. So, label powerset
transforms this problem into a single 
multi-class problem as shown below.
Multi-Class and Multi-Label 
Classification Revisited
• Solution: The label powerset method 
has given a unique class to every 
possible label combination that is 
present in the training set.
Multi-Class and Multi-Label 
Classification Revisited
• Adapted Algorithms
• The most famous adapted 
algorithm is Multilabel kNN
(MLkNN)
Multi-Class and Multi-Label 
Classification Revisited
• ML-kNN uses the kNN algorithm 
independently for each label l. 
• It finds the k nearest examples to 
the test instance and considers 
those that are labeled at least with l
as positive and the rest as 
negative. 
Multi-Class and Multi-Label 
Classification Revisited
• What mainly differentiates this 
method from other binary 
relevance (BR) methods is the use 
of prior probabilities. ML-kNN can 
also rank labels.
Multi-Class and Multi-Label 
Classification Revisited
• Adapted Algorithms
• Decision Trees can be modified to 
perform multi label classification.
• The main modification is in 
calculating purities for each region.
Multi-Class and Multi-Label 
Classification Revisited
• Ensemble Algorithms
• AdaBoost.MH and AdaBoost.MR
Evaluation Metrics
• One cannot simply use our normal 
metrics to calculate the accuracy of the 
predictions. 
• Accuracy score/ Exact match metric: 
This function calculates subset accuracy 
meaning the predicted set of labels 
should exactly match with the true set of 
labels.
Evaluation Metrics
• Hamming Loss: The fraction of the 
wrong labels to the total number of 
labels, which is:
•
L
N
1
NL
I( y
ˆ
ij ≠ yij
)
j=1
L
∑
i=1
N
∑
Multi-Class and Multi-Label 
Classification Revisited
• For an interesting overview, see:
• https://arxiv.org/pdf/1703.08991.pdf
SVC versus Logistic
Regression?
With f (X) = β0 + β1X1 + . . . + βpXp one can 
rephrase support-vector classifier optimizationas
Loss
This has the form
loss plus penalty.
The loss is known as the
hinge loss.
Very similar to “loss” in
logistic regression
(negative log-likelihood).
SVC versus Logistic
Regression?
Similarly, one can rewrite logistic regression when 
Y is -1 or 1 (instead of 0,1) as Loss
The negative log
likelihood loss function is:
� � = �! � = �! =
1
1 + exp(−�!�(�!))
−log[� � = �! � = �! ]
= log[(1 + exp(−�!�(�!))]
Loss Plus Penalty: L1 
Regularization
The above optimization problem has the form of
loss plus penalty.
Note that the original penalty in SVC is a ridge
penalty.
One can change the penalty with L1 penalty
and perform variable selection along with
Support Vector Classification.
Loss Plus Penalty: L1 
Regularization
Because the hinge loss function is not
differentiable, the solution paths for different
values of λ may have jumps.
Therefore, some authors replace the hinge loss
with a differentiable squared hinge loss.
Different combinations of hinge loss and squared
hinge loss with L1 and L2 penalty result in various
versions of SVM.
Comparison of L1 Regularized 
SVM and Logistic Regression
Details of Previous Figure
A comparison of the coefficient paths for the L1- SVM 
versus L1 logistic regression on two examples.
In the left we have the South African heart disease 
data (N = 462 and p = 9), and on the right the 
Leukemia data (N = 38 and p = 6087). The dashed 
lines are the SVM coefficients, the solid lines logistic 
regression. The similarity is striking in the left example, 
and strong in the right.
Which to use: SVM or 
Logistic Regression
• When classes are (nearly) 
separable, SVM does better than 
LR. So does LDA.
Which to use: SVM or 
Logistic Regression
• SVMs are popular in high-dimensional 
classification problems with p>>n, since 
the computations are O(pn2) for both 
linear and nonlinear kernels. (Some 
references say between O(n2) and 
O(n3)).
• Additional efficiencies can be realized 
for linear SVMs (Shalev-Shwartz, 
Singer and Srebro 2007).
Which to use: SVM or 
Logistic Regression
• When classes are (nearly) 
separable, SVM does better than 
LR. So does LDA.
• When not, LR (with ridge penalty) 
and SVM very similar.
Which one to use: SVM or 
Logistic Regression
• If you wish to estimate probabilities, 
LR is the choice.
• However, one can estimate 
probabilities from distances of data 
points from the SVC hyperplane:
Which one to use: SVM or 
Logistic Regression
• For nonlinear boundaries, kernel SVMs
are popular, as we will see. Can use
kernels with LR and Bayesian LDA as
well, but computations are more
expensive.
• Also, Kernels try to find a feature space
in which decision boundaries are linear.
That’s not commensurate with
characteristics of LR.
• The VC dimension is a measure of 
the capacity (complexity, expressive 
power, richness, or flexibility) of a space 
of functions that can be learned by 
a classification algorithm.
The Vapnik-Chervonenkis Dimension
• A classification model f with some 
parameter vector β is said to shatter a 
set of data points (x1,x2,…,xn) if for all 
assignments of labels to those points, 
there exists a β such that the model f
makes no errors when evaluating that 
set of data points. 
The Vapnik-Chervonenkis Dimension
• The VC dimension of a model f is the 
maximum number of points that can be 
arranged so that f shatters them. 
The Vapnik-Chervonenkis Dimension
• Example: f is a straight line as a 
classification model on points in a twodimensional plane (this is the model 
used by a perceptron). 
• The line should separate positive data 
points from negative data points.
The Vapnik-Chervonenkis Dimension
• There exist sets of 3 points that can 
indeed be shattered using this model 
(any 3 points that are not collinear can 
be shattered). However, no set of 4 
points can be shattered. Thus, the VC 
dimension of this particular classifier is 
3. 
The Vapnik-Chervonenkis Dimension
• Note, only 3 of the 23 = 8 possible label 
assignments are shown for the three 
points.
The Vapnik-Chervonenkis Dimension
3 points shattered 4 points impossible
• Note, only 3 of the 23 = 8 possible label 
assignments are shown for the three 
points.
The Vapnik-Chervonenkis Dimension
3 points shattered 4 points impossible
• The VC Dimension of affine 
classifiers of the form f(x)= βTx+ β0, 
β ∈ℝp is p+1.
• This includes SVC, a linear SVM.
• The VC Dimension of an SVM 
equipped with an RBF kernel is 
infinite.
The Vapnik-Chervonenkis Dimension
Support Vector Regression
• There is an extension of the SVM 
for regression, called support 
vector regression (SVR) . 
• We saw that least squares 
regression seeks coefficients β0, 
β1, . . . , βp such that the sum of 
squared residuals is as small as 
possible. 
Support Vector Regression
• Support vector regression instead 
seeks coefficients that minimize a 
different type of loss, where only 
residuals larger in absolute value 
than some positive constant 
contribute to the loss function. 
• This is an extension of the margin 
used in support vector classifiers 
to the regression setting.
SVM Learning
DSCI 552, Machine Learning for Data 
Science
University of Southern California
M. R. Rajati, PhD
1
INF 552 - © M R Rajati - 2018 2
Lesson 8
Unsupervised Learning
Unsupervised Learning
Unsupervised vs Supervised Learning:
• Most of this course focuses on 
supervised learning methods such as 
regression and classification.
• In that setting we observe both a set 
of features X1, X2, . . . , Xp for each 
object, as well as a response or 
outcome variable Y . The goal is then 
to predict Y using X1, X2, . . . , Xp.
Unsupervised Learning
Unsupervised vs Supervised Learning:
• Here we instead focus on 
unsupervised learning, we where 
observe only the features X1, X2, . . . 
, Xp. 
•We are not interested in prediction, 
because we do not have an 
associated response variable Y.
The Goals of Unsupervised
Learning
•The goal is to discover interesting things 
about the measurements: is there an 
informative way to visualize the data? 
Can we discover subgroups among the 
variables or among the observations?
•We discuss two methods:
• principal components analysis, a tool used for 
data visualization or data pre-processing 
before supervised techniques are applied, and
• clustering, a broad class of methods for 
discovering unknown subgroups in data.
The Challenge of Unsupervised
Learning
• Unsupervised learning is more subjective than 
supervised learning, as there is no simple goal 
for the analysis, such as prediction of a
response.
• But techniques for unsupervised learning are 
of growing importance in a number of fields:
• subgroups of breast cancer patients grouped by 
their gene expression measurements,
• groups of shoppers characterized by their 
browsing and purchase histories,
• movies grouped by the ratings assigned by movie
viewers.
Another advantage
• It is often easier to obtain unlabeled 
data — from a lab instrument or a 
computer — than labeled data, which 
can require human intervention.
•For example it is difficult to 
automatically assess the overall 
sentiment of a movie review: is it 
favorable or not?
Clustering
•Clustering refers to a very broad 
set of techniques for finding 
subgroups, or clusters, in a data
set.
•We seek a partition of the data into 
distinct groups so that the 
observations within each group are 
quite similar to each other.
Clustering
•We must define what it means for 
two or more observations to be
similar or different.
• Indeed, this is often a domainspecific consideration that must be 
made based on knowledge of the 
data being studied.
Clustering for Market Segmentation
•Suppose we have access to a large 
number of measurements (e.g. 
median household income, 
occupation, distance from nearest 
urban area, and so forth) for a large 
number of people.
Clustering for Market
Segmentation
•Our goal is to perform market 
segmentation by identifying 
subgroups of people who might be 
more receptive to a particular 
form of advertising, or more likely 
to purchase a particular product.
Clustering for Market
Segmentation
•The task of performing market 
segmentation amounts to 
clustering the people in the 
data set.
Two clustering methods
• In K-means clustering, we seek to 
partition the observations into a 
pre-specified number of clusters.
Two clustering methods
• In hierarchical clustering, we do not know 
in advance how many clusters we want; 
in fact, we end up with a tree-like visual 
representation of the observations, 
called a dendrogram, that allows us to 
view at once the clusterings obtained 
for each possible number of clusters, 
from 1 to n.
K-means clustering
K=2 K=3 K=4
K-means clustering
K=2 K=3 K=4
A simulated data set with 150 observations in 2-dimensional
space. Panels show the results of applying K-means 
clustering with different values of K, the number of clusters. 
The color of each observation indicates the cluster to which it 
was assigned using the K-means clustering algorithm. Note 
that there is no ordering of the clusters, so the cluster 
coloring is arbitrary. These cluster labels were not used in 
clustering; instead, they are the outputs of the clustering 
procedure.
Details of K-means clustering
Let C1, . . . , CK denote sets containing the indices of 
the observations in each cluster. These sets satisfy 
two properties:
1. C1∪C2∪. . . ∪CK = {1, . . . , n}. In other words,
each observation belongs to at least one of 
the K clusters.
2. Ck ∩ Ck' =∅ for all distinct k and k'
. In other 
words, the clusters are non-overlapping: no 
observation belongs to more than one cluster.
For instance, if the i
th observation is in the kth cluster,
then i ∈Ck.
Details of K-means clustering:
continued
• The idea behind K-means clustering is 
that a good clustering is one for which the 
within-cluster variation is as small as
possible.
Details of K-means clustering:
continued
• The within-cluster variation for cluster Ck is 
a measure WCV(Ck) of the amount by 
which the observations within a cluster 
differ from each other.
• Hence we want to solve the problem
Details of K-means clustering:
continued
• In words, this formula says that we want to 
partition the observations into K clusters 
such that the total within-cluster variation, 
summed over all K clusters, is as small as
possible.
How to define within-cluster
variation?
• Typically we use Euclidean distance
where |Ck| denotes the number of 
observations in the kth cluster.
How to define within-cluster
variation?
• Combining the above equations gives the 
optimization problem that defines K-means 
clustering,
K-Means Clustering Algorithm
1.Randomly assign a number, from 1 to K, 
to each of the observations. These serve 
as initial cluster assignments for the
observations.
2.Iterate until the cluster assignments stop
changing:
1.For each of the K clusters, compute the 
cluster centroid.
The kth cluster centroid is the vector of 
the p feature means for the 
observations in the kth cluster.
2.Assign each observation to the cluster
whose centroid is closest (where closest 
is defined using Euclidean distance).
Properties of the Algorithm
• This algorithm is guaranteed to decrease the 
value of the objective function at each step. 
Why? Note that
where is the mean for feature j in
cluster Ck.
• however it is not guaranteed to give the global 
minimum. Why not?
Example
Data Step1 Iteration 1, Step 2a
Iteration 1, Step 2b Iteration 2, Step 2a Final Results
Details of Previous Figure
The progress of the K-means algorithm with
K=3.
• Top left: The observations are shown.
• Top center: In Step 1 of the algorithm, 
each observation is randomly assigned to 
a cluster.
• Top right: In Step 2(a), the cluster centroids 
are computed. These are shown as large 
colored disks. Initially the centroids are 
almost completely overlapping because the 
initial cluster assignments were chosen at
random.
Details of Previous Figure
The progress of the K-means algorithm with
K=3.
• Bottom left: In Step 2(b), each 
observation is assigned to the nearest
centroid.
• Bottom center: Step 2(a) is once again 
performed, leading to new cluster
centroids.
• Bottom right: The results obtained after 10
iterations.
Example: different starting values
320.9 235.8 235.8
235.8 235.8 310.9
Details of Previous Figure
K-means clustering performed six times 
on the data from previous figure with K 
= 3, each time with a different random 
assignment of the observations in Step 
1 of the K-means algorithm.
Details of Previous Figure
Above each plot is the value of the 
objective
Three different local optima were 
obtained, one of which resulted in a 
smaller value of the objective and 
provides better separation between the
clusters.
Those labeled in red all achieved the 
same best solution, with an objective 
value of 235.8
K-Medoids Clustering
• Similar to K-means, but in each 
step we do not compute the 
centroid of each cluster.
• We compute the medoid, which is a 
data point that has the smallest 
average dissimilarity with other data 
points in the cluster.
• This way you limit the center of your 
cluster to be one of your data 
points.
Hierarchical Clustering
•K-means clustering requires us to 
pre-specify the number of clusters 
K. This can be a disadvantage (later 
we discuss strategies for choosing
K)
•Hierarchical clustering is an 
alternative approach which does 
not require that we commit to a 
particular choice of K.
Hierarchical Clustering
•In this section, we describe bottomup or agglomerative clustering. This 
is the most common type of 
hierarchical clustering, and refers to 
the fact that a dendrogram is built 
starting from the leaves and 
combining clusters up to the trunk.
Hierarchical Clustering: the idea
A B
C
D
E
Builds a hierarchy in a “bottom-up”
fashion...
Hierarchical Clustering: the idea
A B
C
D
E
3 8 / 5 2
Builds a hierarchy in a “bottom-up” fashion...
Hierarchical Clustering: the idea
A B
C
D
E
Builds a hierarchy in a “bottom-up” fashion...
A B
C
D
E
Builds a hierarchy in a “bottom-up” fashion...
Hierarchical Clustering: the idea
A B
C
D
E
Builds a hierarchy in a “bottom-up” fashion...
Hierarchical Clustering: the idea
Hierarchical Clustering Algorithm
A B
C
D
E
0 1 2
3
4
The approach in words:
• Start with each point in its own cluster.
• Identify the closest two clusters and merge them.
• Repeat.
• Ends when all points are in a single cluster.
D
E
B
A
C
Dendrogram
Hierarchical Clustering Algorithm
A B
C
D
E
0 1 2
3
4
The height of each node in the plot is proportional 
to the value of the intergroup dissimilarity 
between its two daughters
D
E
B
A
C
Dendrogram
An Example
−6 −4 −2 0 2
−2 0 4
X1
X2
2
45 observations generated in 2-dimensional space. In 
reality there are three distinct classes, shown in
separate colors.
However, we will treat these class labels as unknown and 
will seek to cluster the observations in order to discover 
the classes from the data.
Application of hierarchical
clustering
0 2 4 6 8 10
0 2 4 6 8 10
0 2 4 6 8 10
Details of previous figure
• Left: Dendrogram obtained from 
hierarchically clustering the data from 
previous slide, with complete linkage 
and Euclidean distance.
•Center: The dendrogram from the lefthand panel, cut at a height of 9 
(indicated by the dashed line). This cut 
results in two distinct clusters, shown in
different colors.
Details of previous figure
•Right: The dendrogram from the lefthand panel, now cut at a height of 5. 
This cut results in three distinct clusters, 
shown in different colors. 
•Note that the colors were not used in 
clustering, but are simply used for 
display purposes in this figure
Algorithm
Nested Clusters
• The term hierarchical refers to the fact that 
clusters obtained by cutting the dendrogram
at a given height are necessarily nested
within the clusters obtained by cutting the 
dendrogram at any greater height. 
• However, on an arbitrary data set, this 
assumption of hierarchical structure might be 
unrealistic
• Hierarchical clustering can sometimes yield 
worse (i.e. less accurate) results than K -
means clustering for a given number of 
clusters
Nested Clusters: Example
• Observations: a 50–50 split of males 
and females, evenly split among 
Americans, Japanese, and French.
• The best division into two groups might 
split these people by gender.
• The best division into three groups 
might split them by nationality.
Nested Clusters: Example
• The true clusters are not nested
• The best division into three groups 
does not result from taking the 
best division into two groups and 
splitting up one of those groups. 
• This situation could not be wellrepresented by hierarchical 
clustering.
Dissimilarity between Groups
• The concept of dissimilarity between a 
pair of observations needs to be 
extended to a pair of groups of 
observations .
• This extension is achieved by 
developing the notion of linkage, which 
defines the dissimilarity between two 
groups of observations. The four most 
common types of linkage—complete, 
average, single, and centroid
Types of Linkage
Linkage Description
Complete
Maximal inter-cluster dissimilarity. Compute all
pairwise dissimilarities between the observations in
cluster A and the observations in cluster B, and record
the largest of these dissimilarities.
Single
Minimal inter-cluster dissimilarity. Compute all
pairwise dissimilarities between the observations in
cluster A and the observations in cluster B, and record
the smallest of these dissimilarities.
Average
Mean inter-cluster dissimilarity. Compute all pairwise
dissimilarities between the observations in cluster A
and the observations in cluster B, and record the
average of these dissimilarities.
Centroid
Dissimilarity between the centroid for cluster A (a
mean vector of length p) and the centroid for cluster B.
Centroid linkage can result in undesirable inversions.
Types of Linkage
• For centroid dissimilarity, inversion can occur,
whereby two clusters are fused at a height below
either of the individual clusters in the dendrogram.
This can lead to difficulties in visualization as well
as in interpretation of the dendrogram.
Choice of Dissimilarity Measure
• So far have used Euclidean distance.
• An alternative is correlation-based distance which 
considers two observations to be similar if their 
features are highly correlated.
• This is an unusual use of correlation, which is 
normally computed between variables; here it is 
computed between the observation profiles for 
each pair of observations.
5 10 15 20
0 5 10 15 20
Variable Index
Observation 1
Observation 2
Observation 3
2
3
1
Choice of Dissimilarity Measure
• Correlation-based distance focuses on the shapes 
of observation profiles rather than their magnitudes.
• Observations 1, 3 have small Euclidean distance 
but weak correlation (low correlation similarity).
• Observations 1, 2 have large Euclidean distance but 
strong correlation (high correlation similarity).
5 10 15 20
0 5 10 15 20
Variable Index
Observation 1
Observation 2
Observation 3
2
3
1
Practical issues
• Should the observations or features first be 
standardized in some way? For instance, maybe 
the variables should be centered to have mean 
zero and scaled to have standard deviation
one.
• In the case of hierarchical clustering,
• What dissimilarity measure should be used?
• What type of linkage should be used?
• How many clusters to choose? (in both Kmeans or hierarchical clustering). Difficult 
problem. No agreed-upon method. See 
Elements of Statistical Learning, chapter 13 for 
more details.
How many clusters?
•Sometimes, we might have no problem 
specifying the number of clusters K 
ahead of time, e.g.,
•Segmenting a client database into K 
clusters for K salesmen
•Other times, K is implicitly defined by 
cutting a hierarchical clustering tree at 
a given height
• In most exploratory applications, K is
unknown.
•What is the “right” value of K?
This is a hard problem
Determining the number of clusters is a
hard problem!
Why is it hard?
• Determining the number of clusters is 
a hard task for humans to perform
(unless the data are low-dimensional). 
Not only that, it’s just as hard to
explain what it is we’re looking for.
Usually, statistical learning is 
successful when at least one of these 
is possible
This is a hard problem
• Why is it important?
• E.g., it might mean a big difference
scientifically if we were convinced
that there were K = 2 subtypes of
breast cancer vs. K = 3 subtypes
• One of the (larger) goals of data 
mining/statistical learning is 
automatic inference; choosing K is 
certainly part of this.
Reminder: within-cluster variation
We focus on K-means, but most ideas apply to other 
settings
Recall: given the number of clusters K, the K-means 
algorithm approximately minimizes the within-cluster 
variation:
over clustering assignments C, where Xk is the average of 
points in group k
Clearly a lower value of W is better. So why not just run 
K-means for a bunch of different values of K, and 
choose the value of K that gives the smallest W (K)?
That’s not going to work
Problem: within-cluster variation just keeps 
decreasing: scree plot
Example: n = 250, p = 2, K = 1, . . . 10
Between-cluster variation
Within-cluster variation measures how tightly grouped
the clusters are. As we increase the number of clusters
K, this just keeps going down. What are we missing?
Between-cluster variation measures how spread 
apart the groups are from each other:
where as before Xk is the average of points in group k, 
and X is the overall average, i.e.
Example: between-cluster variation
Example: n = 100, p = 2, K = 2
l
l
l
l
l l
l
l
l
l
l
l
l
l
l l
l
l
l
ll
l
l
l
l l l
l
l
l llllll l l
l
l l l
l
l l l l
l
l
l l
l l l
ll
l l l
l l
l
l
l l
l
l l
l
l
l
l l
l ll
l llll
l l ll
l l l ll
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
ll
l l l
l
l
ll
l
l
l
l
l
l
l l
l lll
ll
ll
l l
l
l
l
l
l l l l l l l
l l l
l l l
l
ll
l
l
l
ll
l l l
l
l l
ll l
l l
−2 0 2 4 6 8
−1
0 1 2
3
lll l ll
l l X 1
l X lXl 2lll
Still not going to work
Bigger B is better, can we use it to choose K? 
Problem: between- cluster variation just keeps 
increasing
Running example: n = 250, p = 2, K = 1, . . . 10
CH index
• Ideally we’d like our clustering assignments C to
simultaneously have a small W and a large B
• This is the idea behind the CH index proposed by Calinski
and Harabasz (1974), in “A dendrite method for cluster
analysis”
• For clustering assignments coming from K clusters, we
record CH score:
CH(K) =
B(K)/(K −1)
W (K)/(n −K)
• To choose K, just pick some maximum number of clusters
to be considered Kmax (e.g., K = 20), and choose the
value of K with the largest score CH(K).
Example: CH index
Running example: n = 250, p = 2, K = 2, . . . 10.
We would choose K = 4 clusters, which seems 
reasonable
General problem: the CH index is not defined for K = 1. 
We could never choose just one cluster (the null model)!
Gap statistic
It’s true that W(K) keeps dropping, but
how much it drops at any one K should 
be informative
The gap statistic was introduced by 
Tibshirani et al. (2001), “Estimating the 
number of clusters in a data set via the 
gap statistic”
Gap statistic
It is based on this idea. We compare 
the observed within-cluster variation 
W (K) to Wunif(K), the within-cluster
variation we’d see if we instead had 
points distributed uniformly (over an 
encapsulating box). The gap for K
clusters is defined as
Gap(K) = log Wunif(K) − log W
(K)
Gap statistic
The reference datasets are in our case 
generated by sampling uniformly from the 
original dataset’s bounding box (see 
green box in the upper right plot of the 
figures below). To obtain the estimate log 
Wunif(K), we compute the average of B
copies of log W(K), each of which is 
generated from the uniform sample, and 
their standard deviation s(K).
Gap statistic
Gap statistic
Gap statistic
Then we choose K by:
Kˆ = minK ∈ {1, . . . Kmax} :Gap(K) ≥ Gap(K +1) 
− s(K +1),
Gap statistic
This means:
Choose the cluster 
size k̂ to be the 
smallest k such that
Gap(k)≥Gap(k+1)−s(k+1)
Example: gap statistic
Running example: n = 250, p = 2, K = 1, . . . 10
We would choose K = 3 clusters, which is also 
reasonable
The gap statistic does especially well when the data fall 
into one cluster. (Why? Hint: think about the null 
distribution that it uses)
CH index and gap statistic in R
The CH index can be computed using the kmeans 
function in the base distribution, which returns both the 
within-cluster variation and the between-cluster 
varation
E.g.,
k = 5
km = kmeans(x, k, alg="Lloyd") 
names(km)
# Now use some of these return items to compute ch
The gap statistic is implemented by the function gap in 
the package lga, and by the function gap in the 
package SAGx. (Beware: these functions are poorly 
documented ... it’s unclear what clustering method 
they’re using)
Silhouette Analysis
• A method of interpretation and 
validation of consistency within clusters 
of data
• Provides a succinct graphical 
representation of how well each object 
lies within its cluster
Silhouette Analysis
• Assume the data have been clustered 
via any technique, such as k-means, 
into k clusters.
• For each sample xi
, let ai be the 
average distance between xi and all 
other data within the same cluster.
• Can interpret ai as a measure of how 
well xi is assigned to its cluster (the 
smaller the value, the better the 
assignment). 
Silhouette Analysis
• We then define the average 
dissimilarity of point xi to a cluster 
c as the average of the distance 
from xi i to all points in c.
• Let bi be the lowest average 
distance of xi to all points in any 
other cluster, of which xi is not a 
member. 
Silhouette Analysis
• The cluster with this lowest average 
dissimilarity is said to be the 
"neighboring cluster" of xi
, because it is 
the next best fit cluster for point xi
. We 
now define a silhouette:
• or
si
=
bi −ai
max(ai
,bi
)
si
=
1−ai / bi ai < bi
0 ai
= bi
bi / ai −1 ai > bi
⎧
⎨
⎪
⎪
⎪
⎪
⎩
⎪
⎪
⎪
⎪
Silhouette Analysis
• From
it is obvious that -1≤ si ≤ 1.
si
=
1−ai / bi ai < bi
0 ai
= bi
bi / ai −1 ai > bi
⎧
⎨
⎪
⎪
⎪
⎪
⎩
⎪
⎪
⎪
⎪
Silhouette Analysis
• si close to 1 requires ai
≪bi
. 
• ai is a measure of dissimilarity of xi
to its own cluster
• Thus a small ai means xi is well 
matched. 
• Large bi implies that xi is badly 
matched to its neighboring cluster.
• Thus an si close to one means that 
xi is appropriately clustered. 
Silhouette Analysis
• si close to negative one, by the 
same logic means that xi would be 
more appropriate if it was clustered 
in its neighboring cluster. 
• An si near zero means that xi is on 
the border of two natural clusters.
Silhouette Analysis
• The average si over a cluster 
measures how tightly grouped all 
the data in the cluster are.
• Thus the average si over the entire 
dataset is a measure of how 
appropriately the data have been 
clustered.
Silhouette Analysis
• If there are too many or too few clusters, 
some of the clusters will display 
narrower silhouettes than the rest. 
• So silhouette plots and averages may 
be used to determine the natural 
number of clusters within a dataset.
Silhouette Analysis
• One can also increase the likelihood of 
the silhouette being maximized at the 
correct number of clusters by re-scaling 
the data using feature weights that are 
cluster specific.
Silhouette Plot
Silhouette Plot
Silhouette Plot
Silhouette Plot
Silhouette Plot
Silhouette Plot
• The silhouette plot shows that k= 3, 5 
and 6 are a bad pick for the given data 
due to the presence of clusters with 
below average silhouette scores and 
also due to wide fluctuations in the size 
of the silhouette plots.
• Silhouette analysis is more ambivalent 
in deciding between 2 and 4.
Cross Validation
• The data is partitioned into v folds.
• Each of the folds is then held out at turn 
as a test set, a clustering model 
computed on the other v − 1 training 
sets
• The value of an objective function is 
calculated for the test set. 
• Example of objective function: the sum 
of the squared distances to the 
centroids for k-means
Cross Validation
• These v values are calculated and 
averaged for each alternative number of 
clusters c, and the cluster number 
selected such that further increase in 
number of clusters leads to only a small 
reduction in the objective function (scree 
plots)
More
• https://stackoverflow.com/questions/153
76075/cluster-analysis-in-r-determinethe-optimal-number-ofclusters/15376462#15376462
More
• https://stats.stackexchange.com/questio
ns/3685/where-to-cut-a-dendrogram
Once again, it really is a hard problem
Variable Selection for Clustering
• .
Variable Selection for Clustering
Variable Selection for Clustering
Variable Selection for Clustering
• .
Variable Selection for Clustering
•The goal of feature selection for 
unsupervised learning is to find 
the smallest feature subset that 
best uncovers “interesting 
natural” groupings (clusters) 
from data according to the 
chosen criterion. 
Subset Selection
• Two paradigms:
• feature subset selection criteria 
should be the criteria used for 
clustering 
• The two criteria need not be the 
same 
Subset Selection
Searching for the best subset of features, 
we run into a new problem: 
k depends on the feature subset. 
Therefore, in each step of subset 
selection, the best k has to be found. 
Many ways are conceivable for best 
subset selection for clustering. The 
details are left to the students.
Subset Selection
Subset Selection
Two dimensions: three clusters
One-dimension: only two clusters.
A fixed number of clusters for all feature 
sets does not model the data in the 
respective subspace correctly. 
K-Means
Large number of k-means variations 
were proposed to handle feature 
selection
Most start cluster the data into k clusters. 
Then, assign weight to each feature. 
The feature that minimizes the withincluster distance / maximizes betweencluster distance is preferred, hence, gets 
higher weight. 
Sparse Methods Based on L1 
Norm
Witten and Tibshirani formulate clustering 
using a parameter vector w, and use L1 
penalty and optimization to obtain a 
sparse set of features.
See 
https://www.ncbi.nlm.nih.gov/pmc/articles
/PMC2930825/pdf/nihms201124.pdf
Principal Components Analysis
•PCA produces a low-dimensional 
representation of a dataset. It finds 
a sequence of linear combinations of 
the variables that have maximal 
variance, and are mutually 
uncorrelated.
•Apart from producing derived 
variables for use in supervised 
learning problems, PCA also serves 
as a tool for data visualization.
Principal Components Analysis: 
details
• The first principal component of a set of 
features X1, X2, . . . , Xp is the normalized 
linear combination of the features
Z1 = φ11X1 + φ21X2 + . . . + φp1Xp
that has the largest variance. By normalized, 
we mean that
Principal Components Analysis:
details
•We refer to the elements φ11, . . . , φp1 as 
the loadings of the first principal 
component; together, the loadings make up 
the principal component loading vector,
φ1 = (φ11 φ21 . . . φp1)T .
•We constrain the loadings so that their 
sum of squares is equal to one, since 
otherwise setting these elements to be 
arbitrarily large in absolute value could 
result in an arbitrarily large variance.
PCA: example
The population size (pop) and ad spending (ad) 
for 100 different cities are shown as purple 
circles. The green solid line indicates the first 
principal component direction, and the blue 
dashed line indicates the second principal 
component direction.
Computation of Principal Components
• Suppose we have a n×p data set. Since we are 
only interested in variance, we assume X that 
each of the variables in X has been centered to 
have mean zero (that is, the column means of X 
are zero).
• We then look for the linear combination of 
the sample feature values of the form zi1 = 
φ11xi1 + φ21xi2 + . . . + φp1xip
for i = 1, . . . , n that has largest sample 
variance, subject to the constraint that
Computation of Principal Components
•Since each of the xij has mean zero, 
then so does zi1 (for any values of φj1). 
Hence the sample variance of the zi1
can be written as
Computation: continued
• Therefore, the first principal component 
loading vector solves the optimization 
problem
• This problem can be solved via a singularvalue decomposition of the matrix X, a 
standard technique in linear algebra.
• We refer to Z1 as the first principal 
component, with realized values z11, . . . 
, zn1
Geometry of PCA
•The loading vector φ1 with elements 
φ11, φ21, . . . , φp1 defines a 
direction in feature space along 
which the data vary the most.
• If we project the n data points x1, . . . , 
xn onto this direction, the projected 
values are the principal component 
scores z11, . . . , zn1 themselves.
Further principal components
• The second principal component is the linear
combination of X1, . . . , Xp that has maximal
variance among all linear combinations that are
uncorrelated with Z1.
• The second principal component scores z12, z22, . 
. . , zn2 take the form
zi2 = φ12xi1 + φ22xi2 + . . . + φp2xip,
where φ2 is the second principal component 
loading vector, with elements φ12, φ22, . . . , φp2.
Further principal components:
continued
•It turns out that constraining Z2 to be 
uncorrelated with Z1 is equivalent to 
constraining the direction φ2 to be 
orthogonal (perpendicular) to the 
direction φ1. And so on.
Further principal components:
continued
•The principal component directions φ1, φ2, 
φ3, . . . are the ordered sequence of right 
singular vectors of the matrix X, and the 
variances of the components are 1/n times 
the squares of the singular values. There are 
at most min(n − 1, p) principal components.
Illustration
•USAarrests data: For each of the fifty 
states in the United States, the data set 
contains the number of arrests per 100, 
000 residents for each of three crimes: 
Assault, Murder, and Rape. We also 
record UrbanPop (the percent of the 
population in each state living in urban 
areas).
Illustration
•The principal component score vectors 
have length n = 50, and the principal 
component loading vectors have length p 
= 4.
•PCA was performed after standardizing 
each variable to have mean zero and 
standard deviation one.
USAarrests data: PCA plot
−3 −2 2 3
−3 −2 −1 0 1 2 3
−1 0 1
First Principal Component
Second Principal Component
Arkansas Alabama
California
Colorado
Connecticut
Florida
Hawaii
Idaho
IllinoisArizona
Kentucky
Maine
Michigan
Montana
Nebraska Indiana
Nevada
New HaI
mowp
a
shire
New York
rth Dakota
KansaOs klahoDmelaaware Missouri
South Carolina
North Carolina 
Mississippi
South Dakota
Texas
RhodMeaIslUsaat
nacdhuseNttesw Jersey
Virginia
Washington
VermontWest Virginia
Ohio 
WiscoMnsininnesota Pennsylvania Oregon
Wyoming
−0.5 0.0 0.5
−0.5 0.0 0.5
Alaska 
Georgia
Murder
New Mexico 
Maryland
Assault
TennesseLeouisiana
UrbanPop
Rape
Figure details
The first two principal components 
for the USArrests data.
•The blue state names represent 
the scores for the first two 
principal components.
PCA loadings
The contribution of each variable in each of the 
principal components PC1 and PC2 is shown 
as a vector. For example, Assault is shown as 
[0.58,-0.19]T
Figure details
•The orange arrows indicate the first two 
principal component loading vectors (with 
axes on the top and right). For example, 
the loading for Rape on the first 
component is 0.54, and its loading on the 
second principal component 0.17 [the 
word Rape is centered at the point (0.54, 
0.17)].
Figure details
•This figure is known as a biplot, 
because it displays both the principal 
component scores and the principal 
component loadings.
Figure details
• The first loading vector places 
approximately equal weight on Assault, 
Murder, and Rape, with much less 
weight on UrbanPop.
• Hence this component roughly 
corresponds to a measure of overall 
rates of serious crimes.
Figure details
• The second loading vector places most 
of its weight on UrbanPop and much 
less weight on the other three features. 
• Hence, this component roughly 
corresponds to the level of urbanization 
of the state. 
Figure details
• The crime-related variables (Murder, 
Assault , and Rape) are located close 
to each other
• The UrbanPop variable is far from the 
other three. 
Figure details
• The crime-related variables are 
correlated with each other: states with 
high murder rates tend to have high 
assault and rape rates
• The UrbanPop variable is less 
correlated with the other three.
Figure details
• States with large positive scores on the 
first component, such as California, 
Nevada and Florida, have high crime 
rates
• States like North Dakota, with negative 
scores on the first component, have low 
crime rates. 
Figure details
• California has a high score on the 
second component, hence a high level 
of urbanization
• The opposite is true for states like 
Mississippi. 
• States close to zero on both 
components, such as Virginia, have 
approximately average levels of both 
crime and urbanization.
Another Interpretation of 
Principal Components Second principal component
−1.0 −0.5 0.0 0.5
First principal component
1.0
−1.0 −0.5 0.0 0.5 1.0
•
•
• •
• •
•
•
•
• ••• •
••
••
• • • •
•
•
•
•
•
•
• • • • •
•
•
•
•
• •
• ••
•• •••
• • •
• •
•
•
•
•
•
•
•
•
•
• • • •
••
• • •• • • •
•
• • • • • • • •
The first three principal components of a data 
set span the three-dimensional hyperplane that 
is closest to the n observations,
PCA find the hyperplane closest 
to the observations
•The first principal component loading 
vector has a very special property: it 
defines the line in p-dimensional space 
that is closest to the n observations 
(using average squared Euclidean 
distance as a measure of closeness)
PCA find the hyperplane closest 
to the observations
•The notion of principal components as 
the dimensions that are closest to the 
n observations extends beyond just the 
first principal component.
•For instance, the first two principal
components of a data set span the
plane that is closest to the n
observations, in terms of average
squared Euclidean distance.
Scaling of the variables matters
• Murder, Rape, Assault, and UrbanPop
have variance 18. 97, 87. 73, 6945. 16, 
and 209. 5, respectively.
• In PCA on the unscaled variables, the first 
principal component loading vector will 
have a very large loading for Assault
Scaling of the variables matters
• This is simply a consequence of the 
scales of the variables. 
• For instance, if Assault were measured in 
units of the number of occurrences per 100 
people (rather than number of occurrences 
per 100,000 people), then this would amount 
to dividing all of the elements of that variable 
by 1,000. Then the variance of the variable 
would be tiny
Scaling of the variables matters
• It is undesirable for the principal components 
obtained to depend on an arbitrary choice of 
scaling
• We typically scale each variable to have 
standard deviation one before we perform PCA.
−3 −2 −1 0 1 2 3
−3 −2 −1 0 1 2 3
First Principal Component
Second Principal Component
−0.5 0.0 0.5
****
UrbanPop
* *
* * * R**
ape * * * * *
* *
* * * *
* * * *
*
*
*
*
*
*
*
* * *
* *
* A*
ssault * * * *
M*urder *
**
*
−100 −50 0 50 100 150
−100 −50 0 50 100 150
First Principal Component
Second Principal Component
*
*
*
* * *
* *
** *
*
**
* * *
* * * * **
*
*** * * * *
* **
* *
−0.5 0.0 0.5 1.0
*
M*urd*er * * * * * Assa
UrbanPop
Rape
*
Scaled Unscaled
−0.5 0.0 0.5 −0.5 0.0 0.5 1.0
Scaling of the variables matters
• If the variables are in different units, scaling 
each to have standard deviation equal to 
one is recommended.
• If they are in the same units, you might or 
might not scale the variables.
−3 −2 −1 0 1 2 3
−3 −2 −1 0 1 2 3
First Principal Component
Second Principal Component
−0.5 0.0 0.5
****
UrbanPop
* * * * * R**
ape * * * * *
* *
* * * *
* * * *
*
*
*
*
*
*
*
* * *
* *
* A*
ssault * * * *
M*urder *
**
*
−100 −50 0 50 100 150
−100 −50 0 50 100 150
First Principal Component
Second Principal Component
*
*
*
* * *
* *
** *
*
**
* * *
* * * * **
*
*** * * * *
* **
* *
−0.5 0.0 0.5 1.0
*
M*urd*er * * * * * Assa
UrbanPop
Rape
*
Scaled Unscaled
−0.5 0.0 0.5 −0.5 0.0 0.5 1.0
Proportion Variance Explained
• We can now ask a natural question: 
how much of the information in a given 
data set is lost by projecting the 
observations onto the first few 
principal components? 
• That is, how much of the variance in 
the data is not contained in the first 
few principal components?
Proportion Variance Explained
• To understand the strength of each 
component, we are interested in knowing 
the proportion of variance explained (PVE) 
by each one.
• The total variance present in a data set 
(assuming that the variables have been 
centered to have mean zero) is defined as
Proportion Variance Explained
The variance explained by the 
mth principal component is
• It can be shown that
with M = min(n − 1, p).
Proportion Variance Explained: 
continued
•Therefore, the PVE of the mth
principal component is given by the 
positive quantity between 0 and 1
Principal Component Principal Component
Proportion Variance Explained: 
continued
•The PVEs sum to one. We sometimes 
display the cumulative PVEs.
How many principal components 
should we use?
If we use principal components as a 
summary of our data, how many 
components are sufficient?
• No simple answer to this question, as 
cross-validation is not available for this 
purpose.
• Why not?
How many principal components 
should we use?
• When could we use crossvalidation to select the number of 
components?
• the “scree plot” on the previous 
slide can be used as a guide: we 
look for an “elbow”.
How many principal components 
should we use?
• If we compute principal components in a 
supervised analysis, then we can treat 
the number of principal component 
score vectors to be used as a tuning 
parameter to be selected via crossvalidation
PCA vs Clustering
•PCA looks for a low-dimensional 
representation of the 
observations that explains a good 
fraction of the variance.
•Clustering looks for 
homogeneous subgroups among 
the observations.
Linear Discriminant Analysis
• Whereas PCA seeks directions that 
are efficient for representation, 
discriminant analysis seeks directions 
that are efficient for discrimination.
• It is in some sense a supervised 
version of PCA.
Fisher’s Linear Discriminant Analysis
Linear Discriminant Analysis
Projection of the same set of samples onto two 
different lines in the directions marked as w. The 
figure on the right shows greater separation between 
the red and black projected points.
Fisher’s Linear Discriminant Analysis
Linear Discriminant Analysis
• Given x1, . . . , xn ∈ Rp divided into 
two subsets D 1 and D 2
corresponding to the classes 1 and 
2, respectively, the goal is to find a 
projection onto a line defined as
y = βT x
where the points corresponding to 
D 1 and D 2 are well separated.
Fisher’s Linear Discriminant Analysis
Linear Discriminant Analysis
• This is called the Fisher’s linear discriminant 
with the geometric interpretation that the 
best projection makes the difference 
between the means as large as possible 
relative to the variance.
Fisher’s Linear Discriminant Analysis
Linear Discriminant Analysis
• Once the transformation from the pdimensional original feature space to a 
lower dimensional subspace is done using 
PCA or Fisher’s LDA, classification methods 
can be used to train pertinent classifiers.
Fisher’s Linear Discriminant Analysis
Examples
(c) Projection onto the first LDAaxis. (a) Scatter plot.
Scatter plot and the PCA and LDA axes for a bivariate sample 
with two classes. Histogram of the projection onto the first LDA 
axis shows better separation than the projection onto the first 
PCA axis.
(b) Projection onto the first PCA axis.
Fisher’s Linear Discriminant Analysis
Examples
(c) Projection onto the first LDAaxis. (a) Scatter plot.
Scatter plot and the PCA and LDA axes for a bivariate sample 
with two classes. Histogram of the projection onto the first LDA 
axis shows better separation than the projection onto the first 
PCA axis. 27 /64
(b) Projection onto the first PCA axis.
Fisher’s Linear Discriminant Analysis
Conclusions
•Unsupervised learning is important for 
understanding the variation and grouping 
structure of a set of unlabeled data, and 
can be a useful pre-processor for 
supervised learning
• It is intrinsically more difficult than 
supervised learning because there is 
no gold standard (like an outcome 
variable) and no single objective (like 
test set accuracy)
Conclusions
• It is an active field of research, with 
many recently developed tools such as 
self-organizing maps, independent 
components analysis and spectral 
clustering.
See The Elements of Statistical Learning, 
chapter 14.
Appendix 1
Novelty/Anomaly/Outlier Detection
Novelty Detection is
• An unsupervised learning problem (data unlabeled)
• About the identification of new or unknown data or 
signal that a machine learning system is not aware of 
during training
157
Example 1
“Novel”
“Novel”
“Novel”
“Normal”
So what’s seems to be the problem?
It’s a 2-Class problem. 
“Normal vs. “Novel”
The Problem is
• That “All positive examples are alike but 
each negative example is negative in its 
own way”.
Example 2
Suppose we want to build a classifier that 
recognizes web pages about “jigsaw puzzles”.
How can we collect a training data?
We can surf the web and pretty easily assemble a 
sample to be our collection of positive examples.
Example 2
What about negative examples ?
The negative examples are… the rest of the web. That is 
~(“pickup sticks web page”)
So the negative examples come from an unknown # 
of negative classes. 
Applications
Many exist
Intrusion detection
Fraud detection
Fault detection
Robotics 
Medical diagnosis
E-Commerce
And more…
Possible Approaches
Density Estimation:
Estimate a density based on training data
Threshold the estimated density for test 
points
Quantile Estimation:
Estimate a quantile of the distribution 
underlying the training data: for a fixed 
constant , attempt to find a 
small set such that 
Check whether test points are inside or 
outside 
a Î(0,1]
S
S
Pr(xÎS) =a
One Class Support Vector 
Machine (OCSVM) Algorithm
Maps input data into a high dimensional feature 
space via kernels
Iteratively finds the maximal margin in the hyperplane 
which best separates the training data from the origin
Solves optimization problem to find rule f with 
maximal margin
f(x)=β0+ β1x1+…+ βpxp
If f(x)<0 , label x as anomalous
OCSVM
Efficiency
Complexity of OCSVM
Time: O(pL3 )
Space: O(p(L+T))
p – number of dimensions
L – number of records in training 
dataset
T – number of records in test dataset 
Appendix 2
In this Appendix, we introduce SOMs
Self-Organizing Maps
• Idea: to learn a map of the data, in a 
low-dimensional embedding.
• Self-Organizing Maps can be thought 
of as a constrained version of kmeans clustering, in which the 
prototypes are encouraged to lie in a 
one- or two-dimensional manifold in 
the feature space.
Key idea (Kohonen, 1982)
Self-Organizing Maps
• This manifold is also referred to as 
constrained topological map, since the 
original high-dimensional observations 
can be mapped down onto the twodimensional coordinate system.
• The original SOM algorithm was online, 
but batch versions have also been 
proposed.
Key idea (Kohonen, 1982)
Self-Organizing Maps
• We consider a SOM with a twodimensional rectangular grid of k
prototypes with p components. The 
choice of a grid is arbitrary, other choices 
like hexagonal grids are also possible.
• Each of the k prototypes are 
parameterized with respect to an integer 
coordinate pair lj ∈ Q1 × Q2.
Self-Organizing Maps
• Here Q1 = {1, 2, . . . , q1}, Q2 = {1, 2, . . . , 
q2}, and k = q1q2.
• One can think of the protoypes as 
"buttons” sewn on the principal compnent 
plane in a regular pattern.
• Intuitively, the SOM tries to bend the 
plane so that the buttons approximate the 
data points as well as possible.
• Once the model is fit, the observations 
can be mapped onto the two-dimensional 
grid.
Self-Organizing Maps
• The mj (prototypes) are initialized, for 
example, to lie in the two-dimensional 
principal component plane of the data
• The observations xi are processed one at 
a time.
• We find the closest prototype mj to xi in 
Euclidean distance in Rp , and then for 
all neighbors mk of mj , we move mk 
toward xi via the update mk ← mk + α(xi − 
mk )
Self-Organizing Maps
• The neighbors of mj are defined to be all 
mk such that the distance between lj and 
lk is small. 
• α ∈ R is the learning rate and determines 
the scale of the step. The simplest 
approach uses Euclidean distance, and 
“small” is determined by a threshold s. 
The neighborhood always includes the 
closest prototype itself.
Self-Organizing Maps
• Notice that the distance is defined in 
the space Q1 × Q2 of integer 
topological coordinates of the 
prototypes, rather than in the feature 
space Rd .
• The effect of the update is to move the 
prototypes closer to the data, but also 
to maintain a smooth two-dimensional 
spatial relationship between the 
prototypes.
• The performance is influenced by 
the learning rate α and the 
distance threshold s. Typically α is 
decreased from 1 to 0 in a few 
thousand iterations.
• Similarly s is decreased linearly from 
starting value R to 1 over a few 
thousand iterations. The description 
above refers to the simplest version 
of SOM.
SOMs: Practical Considerations
SOMs: Practical Considerations
• In more advanced approaches, the 
update step is changed with distance:
• mk ← mk + αh(||l
j − l
k ||)(xi − mk ),
where the neighborhood function h
gives more weight to prototypes mk
with indices l
k closer to l
j than those 
further away.
• If s is chosen so small that each 
neighborhood contains only one point, 
then the spatial connection between 
prototypes is lost. In that case, SOM is an 
online version of k-means, and converges 
to a local minimum of the k-means 
objective.
SOMs: Link to k-means
Self-Organizing Maps: Summary
• Self-Organizing Maps are a variant of kmeans for nonlinear dimensionality 
reduction and visualisation.
• As in k-means, points are iteratively 
assigned to cluster means.
• Unlike k-means, updates of cluster means 
are constrained and synchronized based 
on a user-defined lattice between cluster 
means.
Self-Organizing Maps: Video
• https://www.youtube.com/watch?v=H9H6
s-x-0YE
DSCI 552, Machine Learning for Data 
Science
University of Southern California
M. R. Rajati, PhD
1
Lesson 9
Semi-Supervised and Active 
Learning
Introduction to Semi-Supervised 
Learning (SSL)
Classifier based methods 
Co-Training, Yarowsky, and Their Combination
Data based methods (Not discussed here)
Manifold Regularization
Harmonic Mixtures
Information Regularization
Learning Problems Revisited
Supervised learning:
Given data consisting of feature-label pairs (xi
,yi
), find 
the predictive relationship between features and 
labels.
Unsupervised learning:
Given a sample consisting of only objects, look for 
interesting structures in the data, and group similar 
objects.
What is Semi-supervised learning?
Supervised learning + Additional unlabeled data
Unsupervised learning + Additional labeled data
Motivation for SSL
Pragmatic:
Unlabeled data is cheap to collect.
Example: Classifying web pages,
There are some annotated web pages.
A huge amount of un-annotated pages is easily 
available by crawling the web.
Methodological:
The brain can exploit unlabeled data and 
learn from similarities.
How can unlabeled data help ?
Red: + 1, Dark Blue: -1
How can unlabeled data help ?
Let’s include some additional 
unlabeled data (Light Blue 
points)
How can unlabeled data help ?
Assumption: Examples from the same 
class follow a coherent distribution
Unlabeled data can give a better sense 
of the class separation boundary
Intuition
+
+
_
_
Labeled data only
+
+
_
_
+
+
_
_
Semi-Supervised SVM
SVM
Inductive vs.Transductive
• Transductive: Produce label only for the 
available unlabeled data.
– The output of the method is not a classifier.
• Inductive: Not only produce label for 
unlabeled data, but also produce a 
classifier.
• Let’s first focus on inductive semisupervised learning..
Two Algorithmic Approaches
• Classifier based methods (SelfTraining):
–Start from initial classifier(s), and 
iteratively enhance it (them)
• Data based methods:
–Discover an inherent geometry in the 
data, and exploit it in finding a good 
classifier.
Self-Training (Bootstrap)
Self-Training/ The Yarowsky Algorithm
• Train supervised model on labeled data 
L
• Test on unlabeled data U
• Add the most confidently classified 
members of U to L
• Repeat
Classifier Based Methods: The 
Yarowsky Algorithm
Iteration: 0
+
-
A 
Classifier
trained 
by SL
Choose instances 
labeled with high 
confidence
Iteration: 1
+
-
Add them to the
pool of current
labeled training 
data
……
(Yarowsky 1995)
Iteration: 2
+
-
A New
Classifier
trained 
by SL
Classifier-Based Methods: 
Refinement
● Refinement: 
• Reduce weight of unlabeled data 
to increase power of more 
accurate labeled data
Advantages and Disadvantages of 
Self-Training
• Advantages:
• The simplest semi-supervised learning 
method.
• A wrapper method, applies to existing 
(complex) classifiers.
• Often used in real tasks like natural 
language processing. 
• Disadvantages
• Early mistakes could reinforce themselves.
Heuristic solutions, e.g. “un-label” an 
instance if its confidence falls below a 
threshold.
Co-Training
• Instances contain two sufficient sets of features
– i.e. an instance is x=(x1,x2)
– Each set of features is called a View
• Two views are independent given the label:
• Two views are consistent:
x
x1 x2
Co-Training
Iteration: t
+
-
Iteration: t+1
+
-
……
C1: A 
Classifier
trained 
on view 1
C2: A 
Classifier
trained 
on view 2
Allow C1 to label 
Some instances
Allow C2 to label 
Some instances
Add self-labeled 
instances to the pool 
of training data
Co-Training
● Example of learning from multiple 
views (multiple sets of attributes): 
classifying webpages
• First set of attributes describes 
content of web page
• Second set of attributes describes 
links from other pages
●Independence Assumption:
• Reduces the probability of the 
models agreeing on incorrect labels
Yarowsky+ Co-training
• Like Yarowsky Algorithm for semisupervised learning, but view is 
switched in each iteration 
• Uses all the unlabeled data 
(probabilistically labeled) for training
• In each iteration, only one of the 
classifiers labels the data.
Yarowsky+ Co-training
• Has been used successfully with 
neural networks and support vector 
machines.
Example
Classify web pages into category for students and 
category for professors
Two views of web page
Content: I am currently a professor of ...
Hyperlinks: a link to the faculty list of computer 
science department
General Multiview Learning
Train multiple diverse models on L. 
Those instances in U which most 
models agree on are placed in L.
Semi-Supervised SVM
Semi-Supervised SVM (S3VM)
Maximize margin of both L and U. Decision 
surface placed in non-dense spaces
Assumes classes are "well-separated"
Can also try to simultaneously maintain class 
proportion on both sides similar to labeled 
proportion
Cluster-and-Label Approach
• Assumption: Clusters coincide with 
decision boundaries
• Poor results if this assumption is wrong
• Cluster labeled and unlabeled data
• For each cluster, train a classifier based on 
the labeled points within that cluster
• Label all data in each cluster using the 
classifier designed for that cluster.
• Train a model based on the whole data 
(that is now labeled)
1
Some figures from Burr Settles
(Passive) Supervised Learning
(Passive) Supervised Learning
(Passive) Supervised Learning
Semi-supervised Learning
Active Learning
Active Learning
Active Learning
Active Learning
Active Learning
Active Learning
Active Learning vs Random Sampling
Passive Learning curve: Randomly selects examples to 
get labels for Active Learning curve: Active learning 
selects examples to get labels for
Types of Active Learning
Largely falls into one of these two types:
Stream-Based Active Learning 
Consider one unlabeled example 
at a time
Decide whether to query its label 
or ignore it
Pool-Based Active Learning Given: 
a large unlabeled pool of 
examples
Rank examples in order of 
informativeness
Query the labels for the most 
informative example(s)
Query Selection Strategies
Any Active Learning algorithm requires a query 
selection strategy
Some examples: 
• Uncertainty Sampling
• Query By Committee (QBC) 
• Expected Model Change 
• Expected Error Reduction 
• Variance Reduction
• Density Weighted Methods
How Active Learning Operates
• Active Learning proceeds in rounds
• Each round has a current model (learned 
using the labeled data seen so far) 
• The current model is used to assess 
informativeness of unlabeled examples
• ... using one of the query selection strategies
• The most informative example(s) is/ are 
selected
How Active Learning Operates
• The labels are obtained (by the labeling 
oracle)
• The (now) labeled example(s) 
is/are included in the training data 
The model is re-trained using the 
new training data
• The process repeat until we have budget 
left for getting labels
Uncertainty Sampling
Select examples which the current model 
is the most uncertain about 
Various ways to measure uncertainty. For 
example:
Based on the distance from the 
hyperplane
Using the label probability P (y |x) (for 
probabilistic models)
Uncertainty Sampling
A simple illustration of uncertainty sampling based 
on the distance from the hyperplane (i.e., margin 
based)
Query By Committee (QBC)
• QBC uses a committee of models
• All models trained using the currently 
available labeled data L
• How is the committee constructed? 
• Some possible ways: Sampling 
different models from the model 
distribution
• Using ensemble methods 
(bagging/boosting, etc.)
Query By Committee (QBC)
• All models vote their predictions on the 
unlabeled pool
• The example(s) with maximum 
disagreement is/are chosen for labeling
• Each model in the committee is retrained after including the new 
example(s)
Effect of Outlier Examples
Uncertainty Sampling or QBC may 
wrongly think an outlier to be an 
informative example
Such examples won’t really help (and 
can even be misleading)
Effect of Outlier Examples
Other robust query selection methods exist to 
deal with outliers
Idea: Instead of using the confidence of a model 
on an example, see how a labeled example 
affects the model itself (various ways to quantify 
this)
The example(s) that affects the model the 
most is probably the most informative
Other Query Selection Methods
Expected Model Change
Select the example whose inclusion brings about the 
maximum change in the model (e.g., the gradient of 
the loss function w.r.t. the parameters)
Expected Error Reduction
Select example that reduces the expected 
generalization error the most
.. measured w.r.t. the remaining unlabeled examples 
(using the expected labels)
Other Query Selection Methods
Variance Reduction
Select example(s) that reduces the model variance by 
the most
.. by maximizing Fisher information of model 
parameters (e.g., by minimizing the trace or determinant 
of the inverse Fisher information matrix)
Fisher information matrix: computed using the loglikelihood
Density Weighting
Weight the informativeness of an example by its 
average similarity to the entire unlabeled pool of 
examples
An outlier will not get a substantial weight!
Active Learning with Selective Sampling
Looking at one example at a time witha margin-based classifier
Input: Parameter b > 0 (dictates how aggressively we want to query 
labels)
For n = 1 : N
Get xn , compute pn 
Predict yˆn = sign(pn )
Draw Bernoulli random variable Z ∈ {0, 1} with probability
b/b+|p|
If Z ==1, query the true label yn
Else if Z ==0, ignore the example xn and don’t update w
Comments:
|pn | is the absolute margin of xn
Large |pn | ⇒ Small label query 
probability
Interesting Observations and 
Questions
• Active Learning: Label efficient learning strategy
Based on judging the informativeness of examples 
• Several variants possible. E.g.,
• Different examples having
different labeling costs 
• Access to multiple labeling 
oracles (possibly noisy)
• Active Learning on features instead of labels (e.g., 
if features are expensive)
• Being “actively” used in industry (IBM, Microsoft, 
Siemens, Google, etc.)
Interesting Observations and 
Questions
• Can an actively labeled dataset be 
reused to train a new different model? 
• Sampling is biased. The actively labeled 
dataset doesn’t reflect the true
training/test data distribution. What could 
be the consequences? How could this 
be accounted for?
• Can we formulate active learning for 
regression?
DSCI 552, Machine Learning for Data 
Science
University of Southern California
M. R. Rajati, PhD
1
Lesson 10
Neural Networks and Deep 
Learning
2
Introductory Remark
In this lecture, we use matrix and vector 
notations. Matrices and vectors are shown 
by bold letters.
Vectors of functions are shown with bold 
letters (e.g. f), and unless otherwise stated, 
act on vectors element-wise. 
3
Perceptron
Perceptron is an algorithm for binary classification 
that uses a linear prediction function:
f(x) = 1, βTx + β0 ≥ 0
-1, βTx + β0 < 0
This is called a step function, which reads:
• the output is 1 if “βTx + β0 ≥ 0” is true, and 
the output is -1 if instead “βTx + β0 < 0” is 
true
4
Perceptron
Perceptron is an algorithm for binary classification 
that uses a linear prediction function:
f(x) = 1, βTx + β0 ≥ 0
-1, βTx + β0 < 0
By convention, the two classes are +1 or -1.
5
Perceptron
Perceptron is an algorithm for binary classification 
that uses a linear prediction function:
f(x) = 1, βTx + β0 ≥ 0
-1, βTx + β0 < 0
• Often these parameters are called weights.
• β=(β1, β2 ,…, βp)
6
Perceptron
Perceptron is an algorithm for binary classification 
that uses a linear prediction function:
f(x) = 1, βTx + β0 ≥ 0
-1, βTx + β0 < 0
By convention, ties are broken in favor of the 
positive class.
• If “βTx + β0 ” is exactly 0, output +1 instead of 
-1.
7
Perceptron
The β parameters are unknown. This is what we 
have to learn.
f(x) = 1, βTx + β0≥ 0
-1, βTx + β0< 0
In the same way that linear regression learns the 
slope parameters to best fit the data points, 
perceptron learns the parameters to best separate 
the instances.
8
Learning the Weights
The perceptron algorithm learns the weights by:
1.Initialize all weights β to 0 or randomly.
2.Iterate through the training data. For each 
training instance, classify the instance.
a)If the prediction (the output of the classifier) was 
correct, don’t do anything. (It means the classifier is 
working, so leave it alone!)
b)If the prediction was wrong, modify the weights by 
using the update rule.
3.Repeat step 2 some number of times (more 
on this later).
9
Learning the Weights
What does an update rule do?
• If the classifier predicted an instance was 
negative but it should have been positive…
Currently: βTx + β0 < 0 Want: βTx + β0 ≥ 0
• Adjust the weights β so that this function value 
moves toward positive
• If the classifier predicted positive but it should 
have been negative, shift the weights so that 
the value moves toward negative.
10
Structure of A Perceptron: 
Example
11
Learning the Weights
+ and - classes
Random initial 
weight:
βT=[1, -0.8];
For now, assume 
that β0=0 all the 
time.
12
Learning the Weights
The Perceptron 
classifies x(1) as -
incorrectly, so the 
weights have to be 
changed.
13
Learning the Weights
The weight 
vector has to
move towards 
the misclassified 
vector.
β new= β old+ x(1)
14
Learning the Weights
The 
Perceptron 
misclassifies 
x(2) as +. 
15
Learning the Weights
The weight 
vector has to
move away from 
the misclassified 
vector.
βnew = βold- x(2)
16
Learning the Weights
The 
Perceptron 
misclassifies 
x(3) as +. 
17
Learning the Weights
The weight 
vector has to
move away 
from x(3).
βnew
= βold- x(3)
18
Learning the Weights
The perceptron update rule:
β(i+1) = β(i) + 0.5[y(i)-f(βT(i)x(i)+ β0(i))] x(i)
19
Learning the Weights
The perceptron update rule:
β(i+1) = β (i) + 0.5[y(i)-f(βT(i)x(i)+ β0(i))] x(i)
If y(i) = f(βT(i)x(i)+ β0(i)). i.e., if Perceptron correctly 
classifies the instance x(i), the weights will not be 
updated. 
20
Learning the Weights
The perceptron update rule:
β(i+1) = β (i) + 0.5[y(i)-f(βT(i)x(i)+ β0(i)) ] x(i)
If y(i) =1 and f(βT(i)x(i)+ β0(i)) =-1, then.5[y(i)-f
(βT(i)x(i)+ β0(i))]=1, so the weights will move towards 
x(i) to make βTx(i) + β0 more positive and x(i) is more 
likely to be classified correctly.
21
Learning the Weights
The perceptron update rule:
β(i+1) = β (i) + 0.5[y(i)-f(βT(i)x(i)+ β0(i)) ] x(i)
If y(i) =-1 and f(y(i)-f(βT(i)x(i)+ β0(i)) )=+1, then.5[y(i)-
f(βT(i)x(i)+ β0(i))]=-1, so the weights will move away 
from x(i) to make βTx(i) + β0 more negative and x(i) is 
more likely to be classified correctly.
22
Learning the Weights
The perceptron update rule:
β(i+1) = β(i) + 0.5e(i) x(i)
where e(i) = e(i) =y(i)-f(βT(i)x(i)+ β0(i)) is the error of the 
model in classifying x(i).
23
Learning the Weights
For the bias term we have:
β0(i+1)= β0(i)+.5[e(i)]
This means if e(i) =y(i)-f(βT(i)x(i)+ β0(i)) is positive, 
increase the bias term and if it is negative, decrease it, 
i.e.:
Move the decision boundary up or down to correct the 
classifier 
24
Learning the Weights
Move the decision boundary up or down to correct the 
classifier.
Example: β1= 1, β2 = -1, β0=0. x(1)=[0 1]Tand y(1)=+1.
25
Remark
So far, we have used β for showing the parameters of 
the decision hyperplane, which is a common notation 
for regression.
Using w is way more common for showing weights in 
neural networks. Let’s switch to w. 
The parameter β0 is often called a bias, and is more 
commonly shown as b.
26
Perceptron vs. SVC
Note that Perceptron and SVC are both linear 
classifiers. However, SVC’s optimization needs 
all data to yield a classifier, so it is a batch
algorithm.
On the other hand, perceptron adapts its 
weights online, based on the data points that 
are presented to it. Thus it is an online
algorithm. 
27
Linear Separability
RECALL: The training instances are linearly 
separable if there exists a hyperplane that will 
separate the two classes.
Linear Separability
If the training instances are linearly separable, 
the perceptron algorithm will eventually find 
weights W such that the classifier gets 
everything correct.
29
Linear Separability
If the training instances are not linearly 
separable, the classifier will always get 
some predictions wrong.
• You need to implement some type of 
stopping criteria for when the algorithm will 
stop making updates, or it will run forever.
• Usually this is specified by running the 
algorithm for a maximum number of 
iterations or epochs.
30
Multi-class Perceptron: Architecture
Using S hyperplanes, one can partition the 
space Rp to 2S regions. Each region can 
represent a class. For p = 2 and S = 2:
31
Multi-class Perceptron
The weights associated with each hyperplane 
can be represented as the columns of a weight 
matrix W:
W = [w1|w2|…|wS]
The biases can be augmented in a vector b:
b = [b1 b2…bS]
T
32
Multi-class Perceptron
A maximum of K = 2S classes can be encoded 
as vectors with S elements of the form:
y = [y1 y2…yS]
T yk Î {-1,1}
In other words, each of the S perceptrons is in 
charge of one of the elements of y.
33
Multi-class Perceptron: Learning Rule
The weights can be updated for multiclass 
perceptron. Assume that e(i) is the vector of 
errors at epoch i
W(i+1) = W (i) + 0.5x(i)eT(i)
As well as the biases 
bT(i+1) = bT(i) + 0.5eT(i)
34
Multi-class Perceptron: Learning Rule
W(i+1) = W (i) + 0.5x(i)eT(i)
bT(i+1) = bT(i) + 0.5eT(i)
e(i) = y(i) – f(WT(i)x(i)+b(i))
y(i) is the actual label vector for x(i), and 
WT(i)x(i)+b(i) is a vector of S linear 
combinations of features of x(i), whose S 
elements are fed to S step functions, 
symbolically shown as f.
35
Multi-class Perceptron: Learning Rule
W(i+1) = W (i) + 0.5x(i)eT(i)
bT(i+1) = bT(i) + 0.5eT(i)
We wish to show that the above rule is nothing 
but application of the Perceptron Learning Rule 
to each of the S perceptrons:
36
Multi-class Perceptron: Learning Rule
W(i+1) = W (i) + 0.5x(i)eT(i)
bT(i+1) = bT(i) + 0.5eT(i)
37
Multi-class Perceptron: Learning Rule
Example: Assume that biases are kept zero and 
w1(0) = [1 1]T
w2(0) = [1 -1]T
Assume that x(1) = [1 0]T is in a class encodes 
as C1 = [1 -1]T and x(2) = [0 -1]T is in a class 
encoded as C0 = [-1 -1]T. Update the weights 
according to the perceptron rule.
38
Learning Rate
Let’s make a modification to the update rule:
W(i+1) = W (i) + αx(i)eT(i)
bT(i+1) = bT(i) + αeT(i)
where α is called the learning rate or step size.
• When you update wj to be more positive or 
negative, this controls the size of the change you 
make (or, how large a “step” you take).
• If α = 0.5 (a common value), then this is the 
same update rule from the earlier slide.
39
Learning Rate
How to choose the step size?
• If α is too small, the algorithm will be slow 
because the updates won’t make much 
progress.
• If α is too large, the algorithm will be slow 
because the updates will “overshoot” and may 
cause previously correct classifications to 
become incorrect.
• One choice: in first iterations choose a large 
learning rate and then reduce it α(i) = α0/(i+c)
40
Learning Rate
See:
https://www.jeremyjordan.me/nn-learning-rate/
41
A Little Bit of History
•McCulloch and Pitts in 1943 contended that 
neurons with a binary threshold activation 
function can implement logical functions.
•In 1949, Hebb observed a working principle 
in neurons that inspired the invention of 
Perceptron Learning Rule.
42
A Little Bit of History
•Hebb observed that, when two neurons fire 
together, the connection between the 
neurons is strengthened
•He concluded that this is one of the 
fundamental operations necessary for 
learning and memory. 
43
A Little Bit of History
•Rosenblatt, using the McCulloch-Pitts 
neuron and the findings of Hebb, went on to 
develop the first Perceptron Learning Rule.
•Perceptron could learn in the Hebbian
sense through the weighting of inputs
•Perceptron was instrumental in the later 
formation of neural networks.
44
Perceptron as A Neural 
Architecture
.
.
.
x1
x2
xp
+
w1
w2
wp
sign(w1 x1 + w2 x2 + …+ wp xp + b)
=sign(wTx+b1)
1
b
A Single “Neuron”
Activation 
Function
45
Perceptron as A Neural 
Architecture
WT
xp x 1
+
1 bS x 1
f
a = f (n) = f(WTx+b) =
sign(WTx+b)
.
.
.
w1
Tx+b1
w2
Tx+b2
wS
Tx+bS
sign(w1
Tx+b1)
sign(w2
Tx+b2)
sign(wS
Tx+bS)
Dimensions:
Wp x S
WT 
S x p
Multiple “Neurons”
46
n
Adaptation in Neural 
Systems 
•The Perceptron Learning Rule acts like a 
simple neural system: it adapts its weights 
based on the error that is a result of 
mismatch between its output and the desired 
output
•This general process is the basis of 
supervised learning with a large class of 
neural networks.
47
Caveat: Linear Separability
•If the training set is not separable, 
the Perceptron rule never 
converges.
•Naturally, it may have a large 
classification error.
48
Solution?
•The general solution is to represent the 
data in a feature space where they are 
linearly separable.
•One idea is two use layers of 
perceptrons and adjust their weights to 
learn the feature representations
49
Example: XOR Problem
50
Example: XOR Problem
51
Example: XOR Problem
52
Example: XOR Problem
x1
x2
+
0.6
0.6 1
-1
+
1.1
1
-1
1.1
+
-2
1.1
X1 XOR x2
53
Caveat
•Finding the weights is ad-hoc and becomes 
complicated when implementing complex 
functions with multiple inputs.
•A principled way of representing and 
training multiple layers of Perceptrons is 
needed. 
54
Multi-Layer Perceptron (MLP)
•MLP consists of multiple layers of 
Perceptrons
•Each layer may have a different number of 
neurons
•Different types of activation functions can 
be used, not just the sign (threshold) function
55
Multi-Layer Perceptron (MLP)
•MLPs can essentially be represented as 
nested functions:
g(x) = g(M)
(g(M-1)(…(g(3)(g(2)(g(1)(x))))…))
•For example, a three layer perceptron can 
be represented as:
•g(x) = g(3)(g(2)(g(1)(x)))
56
Multi-Layer Perceptron (MLP)
•Each g(i) is a layer of neurons with its 
weight matrix W(i) and bias vector b(i)
and activation function f(i)
57
Multi-Layer Perceptron (MLP)
•This means the output of the i
th layer, a(i) is:
a(i) =f(i)
(W(i) Ta(i-1)+b(i)
)=g(i)
(a(i-1))
58
Multi-Layer Perceptron (MLP)
59
W(1)T
xp x 1
+
b(1) 1
f(1)
+
b 1 (2)
+
b(M)
…
…
W(2)T
f(2)
W(M)T
1
f(M)
a(1) a(2)
a(M)
Multi-Layer Perceptron (MLP)
•MLPs are also called Feedforward Neural 
Networks
•Especially when the number of layers is 
large (e.g. M >10), they are called Deep 
Feedforward Neural Networks
60
Multi-Layer Perceptron (MLP)
•Weights in each layer can be learned so 
that the MLP is used for either classification 
or regression tasks.
61
Types of Activation Functions 
Used
•Traditionally, sigmoid functions were 
commonly used for both classification and 
regression tasks 
62
Types of Activation Functions 
Used
•Traditionally, tanh sigmoid functions were 
commonly used for both classification and 
regression tasks 
63
f (x) =
ex −e
−x
ex +e
−x
=
e
2x −1
e
2x +1
Types of Activation Functions 
Used
•In more modern practices for training deep 
neural networks, the Rectified Linear Unit 
(ReLu) is commonly used
64
MLPs are Powerful Tools for Regression
•It can be proven mathematically that any 
smooth function can be approximated with 
any precision using an MLP with only two 
layers:
•A hidden layer of smooth nonlinear 
functions (e.g. sigmoids)
•An output layer of smooth functions (even 
linear functions are adequate!) 
65
•This means that MLPs are 
universal approximators!
66
MLPs are Powerful Tools for Classification
•MLPs can be used for classification usually 
with at least:
•A hidden layer of smooth nonlinear 
functions (e.g. sigmoids)
•An output layer of linear functions 
followed by threshold functions.
•Classes are usually encoded using 
•Binary encoding
•One hot encoding
•Therefore, desired outputs are vectors 
representing each class 67
Architectural Considerations
68
•Choice of depth (number 
of layers) of network
•Choice of width (number 
of neurons) of each layer
Architectural Considerations
69
•Deeper networks have
•Far fewer units in each layer
•Often generalize well to the test set
•They are often more difficult to train
•Ideal network architecture must be found via 
experimentation guided by validation set error
There is No Free Lunch!
70
There is no universal procedure for 
examining a training set of specific 
examples and choosing a function that will 
generalize to points not in training set.
How to Train MLPs
71
•MLPs are trained using optimization 
algorithms.
•Optimization algorithms need calculation of 
Gradients to be numerically solved
How to Train MLPs
72
•Backpropagation (of errors) is a brilliant yet 
simple method to calculate gradients used to 
adjust the weights in MLPs to learn regression 
and classification tasks
•It was proposed by Rumelhart, Hinton, and 
Williams in 80’s 
•Still used in modern practices
Multi-Layer Perceptron (MLP)
73
W(1)T
xp x 1
+
b(1) 1
f(1)
+
b 1 (2)
+
b(M)
…
…
W(2)T
f(2)
W(M)T
1
f(M)
n(1) a(1) n(2) a(2)
a(M-1) n(M)
a(M)
n(j)
=W(j)Ta(j-1) + b(j) 
j = 1,…, M
a(j)
=f(j) (n(j)
)
a(0) = x
S(1) S(2)
S(M)
S(2) S x1 (1)
x1
S(M)
x1 S(M)
x1
S(2)
x1
S(1)
x1
S(1)
x1
S(2)
x1
S(M)
x1
Training Formulation for MLP
74
•Training Set
•{x(1), y(1)} , {x(2), y(2)} ,… , {x(N), 
y(N)} 
•We would like to minimize some 
objective function J by finding 
“suitable” weight matrices for each 
layer.
Training Formulation for MLP
75
•A suitable objective function J is 
expected sum of square errors
J = E ei
2
i=1
S M
∑
⎧
⎨
⎪
⎪
⎩
⎪
⎪
⎫
⎬
⎪
⎪
⎭
⎪
⎪
= E ( yi −ai
M )
2
i=1
S M
∑
⎧
⎨
⎪
⎪
⎩
⎪
⎪
⎫
⎬
⎪
⎪
⎭
⎪
⎪
= eT =E{ee
Te}
Training formulation for MLP
76
J = E{eT
e}≈
1
N
e(k)
T e(k)
k=1
N
∑
e(k) = y(k)−a = y(k)−a( M )
= y(k) –g( M )
(g( M−1)
(…(g
(3)
(g
(2)
(g
(1)
(x(k)))))…))
Training formulation for MLP
77
• We don't know J at each moment 
that some pair x(k) and y(k) 
becomes available.
J = E{eT
e}≈
1
N
e(k)
T e(k)
k=1
N
∑
Training formulation for MLP
78
• We approximate J based on the 
error of the network with respect to 
the pair x(k) and y(k), i. e. 
J ≈e(k)Te(k)
• Alternatively, one can use a “minibatch” of L pairs. For simplicity, we 
use only one pair.
J = E{eT
e}≈
1
N
e(k)
T e(k)
k=1
N
∑
Approximate Gradient Descent
79
• Wij
(m)
(k+1) = wij
(m)
(k)-α∂J/∂wij
(m)
• bi
(m)
(k+1) = bi
(m)
(k)-α∂J/∂bi
(m)
Gradient Descent Formulation
Gradient Descent Formulation
Forward Propagation
• a(0) = x(k)
• a(m+1)= f(m+1)(W(m+1)Ta(m)
+b(m+1))
• m = 0 ,1, 2 , ... , M – 1
• a(M) = a
W(1)T
xp x 1
+
b(1) 1
f(1)
+
1 b(2)
+
b(M)
…
…
W(2)T
f(2)
W(M)T
1
f(M)
n(1) a(1) n(2) a(2)
a(M-1) n(M)
a(M)
S(1) S(2)
S(M)
S(2) S x1 (1)
x1
S(M) S(M)
x1 x1
S(2)
x1
S(1)
x1
S(1)
x1
S(2)
x1
S(M)
x1
Elementwise 
operation
Back Propagation
83
• Define F'
(m) )(n(m)
) as:
Back Propagation
• s(M) = -2F'
(M)(n(M))(y-a)
• s(m)= F'
(m)(n(m))W(m+1)s(m+1)
• m = M – 1,...,2,1
• a(M) = a
W(1)T
xp x 1
+
b(1) 1
f(1)
+
1 b(2)
+
b(M)
…
…
W(2)T
f(2)
W(M)T
1
f(M)
n(1) a(1) n(2) a(2)
a(M-1) n(M)
a(M)
S(1) S(2)
S(M)
S(2) S x1 (1)
x1
S(M) S(M)
x1 x1
S(2)
x1
S(1)
x1
S(1)
x1
S(2)
x1
S(M)
x1
Back Propagation
85
• The sensitivities are computed by 
starting at the last layer, and then 
propagating backwards through the 
network to the first layer.
Back Propagation
86
• For an objective function other than 
J = eTe, the term -2(y-a) in 
sensitivity calculation
s(M) = -2F'
(M)
(n(M)
)(y-a)
will be replaced with 
Weight Update
87
• W(m)
(k+1) = W(m)
(k)-αa(m-1)s(m)T
• b(m)T(k+1) = b(m)T(k)-αs(m)T
• Compare these equations with the 
Perceptron update rule.
Weight Update
88
• W(m)
(k+1) = W(m)
(k)-αa(m-1)s(m)T
• b(m)T(k+1) = b(m)T(k)-αs(m)T
• We present each (randomly 
selected) training sample (or batch 
of samples) and use the forward 
and backward paths to calculate 
sensitivities and update the weights.
• This is called an iteration.
Weight Update
89
• W(m)
(k+1) = W(m)
(k)-αa(m-1)s(m)T
• b(m)T(k+1) = b(m)T(k)-αs(m)T
• Each time we finish presenting the 
whole training set to the network, 
we complete an epoch.
• An epoch includes multiple 
iterations.
• Training the network requires 
multiple epochs.
Backpropagation Schema
90
Regularization Methods
91
• In general, any method to prevent 
overfitting or help the optimization is 
considered regularization.
• One can add L1 and L2 regularizers
to the objective function in 
backpropagation.
• However, empirical regularization is 
also very popular for Neural 
Networks.
Regularization Methods
92
• One can show that L2 regularization 
is equivalent to weight decay
• W(m)
(k+1) = (1-ηα)W(m)
(k)
-αa(m-1) s(m)T
• b(m)T(k+1) = (1-ηα)b(m)T(k)-αs(m)T
• η is called the decay rate.
Regularization Methods
93
• Empirical regularization is also very 
popular for Neural Networks.
Adding Noise to The Input
94
Adding Noise to The Input
95
Drawback: Too much noise is 
harmful
96
Adding Noise to Weights
97
• Adding noise to the weights can be shown 
to be equivalent to adding a regularization 
term to the objective function\
• Noise has shown to have benefits when 
added to many learning algorithms.
Data Augmentation
98
• Adding noisy or transformed versions of 
training data to the training set
Be Careful!
99
• Be careful about the transformation applied:
• Example: classifying ‘b’ and ‘d’
• Example: classifying ‘6’ and ‘9’
Early Stopping
100
• Idea: don’t train the network to have very 
small training error
• Prevent overfitting: use a validation set and 
validation error to decide when to stop the 
algorithm
• When the validation error starts increasing, 
overfitting is occurring.
Early Stopping
101
Another Version of Early 
Stopping
102
• When training, monitor validation error
• Every time validation error improved, store 
a copy of the weights
• When validation error not improved for 
some time, stop
• Return the copy of the weights stored
Early Stopping: Pros and Cons
103
• Advantage
• Efficient: along with training; only store an 
extra copy of weights
• Simple: no change to the model/algorithm
• Disadvantage:
• Need validation data
Early Stopping: Pros and Cons
104
• How to remedy the disadvantage?
• After early stopping of the first run, 
train a second run and reuse validation 
data
Early Stopping: Pros and Cons
105
• How to reuse validation data? 
• 1.Start fresh, train with both training data 
and validation data up to the previous 
number of epochs 
• 2. Start from the weights in the first run, 
train with both training data and validation 
data until the training loss < the validation 
loss at the early stopping point
Dropout
106
• Randomly select weights to update
• More precisely, in each update step:
• Randomly sample a different binary mask to 
all the input and hidden units
• Multiple the mask bits with the units and do 
the update as usual
• Typical dropout probability: 0.2 for 
input and 0.5 for hidden units
What regularizations are 
frequently used?
107
• L2 regularization
• Early stopping
• Dropout
• Data augmentation if the transformations 
known/easy to implement
See Appendix
108
Unsupervised Learning/ Feature 
Learning Using NNs
&
Pre-Training
See Appendix
109
Adversarial Training
The Challenge of Dealing with Images
MNIST: Handwritten digit recognition
CIFAR-10: 10 distinct classes – airplane, 
automobile, bird, cat, deer, dog, frog, horse, 
ship & truck)
MNIST CIFAR-10
Image Classification and 
MLPs
• A two-layer MLP can achieve an 
accuracy of 98.2%, which can be quite 
easily improved.
• Fully connected MLPs will usually not 
be the model of choice for imagerelated tasks
• It is far more typical to make advantage 
of a convolutional neural network 
(CNN) for images.
Image Classification and 
MLPs: Weight Proliferation
• The number of parameters 
(weights) of MLPs fed with raw 
image data is very large for images
• CIFAR-10: 32×32×3 colored 
images
Image Classification and 
MLPs: Weight Proliferation
• If we treat each channel of each 
pixel as an independent input to an 
MLP, each neuron of the first 
hidden layer adds ∼3000 new 
parameters to the model!
• The number of weights quickly 
becomes unmanageable as image 
sizes grow larger
Image Classification and 
MLPs: Weight Proliferation
• If we treat each channel of each pixel 
as an independent input to an MLP, 
each neuron of the first hidden layer 
adds 3072 new parameters to the 
model!
• The number of weights quickly 
becomes unmanageable as image 
sizes grow larger
• This is sometimes called weight 
proliferation
Image Classification and 
MLPs: Downsampling
• Common solution: down-sampling the 
images to have fewer weights
• Drawback: direct down-sampling = 
loosing valuable information
• Key concept: images carry more 
information than their vectorized
versions
• Adjacent pixels carry some information 
about the image
Exploiting The Structure 
• The information carried by adjacent 
pixels can be summarized.
• Summarization is performed by the socalled convolution operator.
A Simple “Image”
The Convolution Operator: 
Kernel/ Filter
• The Kernel (or Filter) in the convolution 
operator is a small matrix which 
encodes a way of extracting an 
interesting feature of an image.
A Simple Kernel
The Convolution Operator
• The result is called a feature map, 
convolved feature, or activation map.
Different Kernels?
• It is evident that different Kernels will 
produce different Feature Maps for the 
same input image.
• Consider the following image:
Sharpen
Blur
Edge enhance
Edge detect
Emboss
The Convolution Operator: A 
More Practical Example
Which Kernel to Use?
• Modern Machine Learning is about 
using machines to decide which 
features are the best
• A structure called a Convolutional 
Neural Network learns the values 
of these filters
Structure of a CNN
• The size of the Feature Map 
(Convolved Feature) is controlled 
by three parameters:
• Depth
• Stride
• Zero-padding
Structure of a CNN: Depth
• Depth is the number of filters used 
for convolution.
• In the following network, 
three distinct filters are used, thus 
three different feature maps
Structure of a CNN: Depth
• These three feature maps can be 
viewed as three stacked matrices, 
so, the “depth” of the feature map 
would be three.
Structure of a CNN: Stride
• Stride: the number of 
pixels by which we slide our filter 
matrix over the input matrix. 
• Stride = 1: the filters move one 
pixel at a time. 
• Stride = 2: the filters jump 2 pixels 
at a time . 
• Larger stride yield smaller feature 
maps.
Structure of a CNN: Zero-Padding
• Sometimes, we pad the input 
matrix with zeros around the 
border to apply the filter to 
bordering elements of the input 
image matrix. 
Structure of a CNN: ZeroPadding
• Zero padding controls the size of 
the feature maps.
• Adding zero-padding: wide 
convolution
• Not using zero-padding : narrow 
convolution.
Introducing Nonlinearity
• ReLU is applied element-wise (per 
pixel) and replaces all negative 
pixel values in the feature map by 
zero. 
• ReLU introduces nonlinearity in 
CNN to deal with inherent 
nonlinearity of classification 
problems
Introducing Nonlinearity
• ReLU is applied to feature maps 
yielding Rectified Feature Maps
• Tanh and sigmoids have also been 
used
• ReLU has shown better performance.
Pooling = Downsampling
• Spatial Pooling (also called 
subsampling or downsampling) reduces 
the dimensionality of each feature map 
but retains the most 
important information.
• Spatial Pooling can be of different 
types: Max, Average, Sum etc.
Pooling = Downsampling
• Max Pooling: define a spatial 
neighborhood (e.g., a 2×2 window) 
and take the largest element within 
that window. 
• Instead the largest element the 
average (Average Pooling) or sum 
of all elements could be taken.
• In practice, Max Pooling has been 
shown to work better.
Max Pooling Example
Max Pooling Strides
• Slide the 2 x 2 window by 2 cells (also 
called “stride”) and take the maximum 
value in each region to reduces the 
dimensionality of the feature map.
Max Pooling Strides
• Comparing Max Pooling and Sum 
Pooling on the Rectified Feature Map of 
The Buildings Image 
Properties of Pooling
• Reduces the dimensions of the input
• Reduces the number of parameters 
and, therefore, controls overfitting
• Is invariant to small transformations, 
distortions and translations in the input 
image 
• A small distortion in input will not 
change the output of Pooling too 
much – since we take the maximum / 
average value in a local 
neighborhood
Properties of Pooling
• Yields an almost scale invariant 
representation of our image.
• This is very powerful since we can 
detect objects in an image no 
matter where they are located
• In essence, convolutions extract 
features from the image and 
pooling makes the features 
invariant to transformations
Convolution-ReLU-Pooling 
Repeated
• The convolution-ReLU-Pooling 
operation can be repeated many 
times in many convolutional 
“layers” with different depths and 
strides
Fully Connected Layer
• Convolutional Layers play the role 
of feature extractors for a Deep 
Neural Network
• The output layer of the MLP must 
be softmax
Other Classification 
Techniques?
• Instead of MLP, other classifiers 
such as SVM can be used
MLP: Even Better Feature 
Learning!
• Adding a fully-connected layer is 
also a (usually) cheap way of 
learning non-linear combinations of 
these features. 
• Most of the features from 
convolutional and pooling layers 
may be good for the classification 
task, but combinations of those 
features might be even better
Putting It Altogether: Train 
Using Backpropagation
•Input Image = Boat
•Target Vector = [0, 0, 1, 0]
Putting It Altogether: Train 
Using Backpropagation
• Initialize weights and filters randomly 
and repeat the following steps for the 
training set many times
• Pass each image through the network 
(forward pass) and read the output
• Calculate the total error
• Total Error = ∑ (target probability –
output probability) ²
• Target Vector = [0, 0, 1, 0]
• Output Vector =[0.2, 0.4, 0.3, 0.1]
• Error =0.7
Putting It Altogether: Train 
Using Backpropagation
• Use Backpropagation to calculate 
the gradients of the error with 
respect to all weights in the network 
and use gradient descent to update 
all filter values / weights and to 
minimize the output error.
• The weights are adjusted in 
proportion to their contribution to 
the total error.
Putting It Altogether: Train 
Using Backpropagation
• Number of filters, filter sizes, 
architecture of the network etc
are fixed and do not change 
during training process 
• Only the values of the filter matrix 
and connection weights are 
updated. •
Problems with squared error
• The squared error measure has some 
drawbacks:
– If the desired output is 1 and the actual 
output is 0.00000001 there is almost no 
gradient for a sigmoid unit to fix up the 
error.
0.00000001
df//dn ≈ 0
Problems with squared error
• The squared error measure has 
some drawbacks:
–If we are trying to assign 
probabilities to mutually exclusive 
class labels, we know that the 
outputs should sum to 1, but we are 
depriving the network of this 
knowledge.
Problems with squared error
• Is there a different cost function that 
works better?
–Yes: Force the outputs to 
represent a probability 
distribution across discrete 
alternatives.
Softmax
The output units in a softmax 
group use a non-local nonlinearity:
softmax 
ai group
a'i
this is called the “logit”
a
′
i
=
e
ai
e
aj
j∈group
∑
∂a
′
i
∂ai
= a
′
i
(1−a
′
i
)
Relationship with Logistic 
Regression?
softmax 
group ai
a'i
this is called the “logit”
a
′
i
=
e
ai
e
aj
j∈group
∑
∂a
′
i
∂ai
= a
′
i
(1−a
′
i
) In logistic regression, logits are 
linear functions of the features.
In neural networks, they are 
features learned by multiple 
layers of the network!
Relationship with Logistic 
Regression?
softmax 
group a
p
Linear Function 
of Features
In fact, 
a= b0+ b1 x1 +…+ bp xp
= bTx(j)+b0
and 
0
are the logits for two 
classes, where x(j) is the j
th
data point whose label is 
z(j) Î {0,1}.
1- p
0
Relationship with Logistic 
Regression?
softmax 
group a
p
Linear Function 
of Features
Consequently, for the 
positive class:
p=ea/(e0+ea)=ea/(1+ea)
And obviously the output 
for the negative class is:
1- p=1/(1+ea)
The negative log-likelihood 
loss for this single data 
point x(j) is:
-z(j)log p-(1-z(j))log(1- p)
1- p
0
Relationship with Logistic 
Regression?
softmax 
group a2
a'2
Linear Function 
of Features
The log-likelihood loss for 
this single data point x(i) is:
-z(j)log p-(1-z(j))log(1- p)
If we rename the desired 
outputs 1-z(j) and z(j) to y1
and y2, and the probability 
outputs of softmax as 
1-p=a'
1 and p=a'
2, the negative 
log-likelihood becomes the 
cross-entropy function:
a'1
a1
Relationship with Logistic 
Regression?
softmax 
group a2
a'2
Linear Function 
of Features
If we rename the desired 
outputs 1-z(j) and z(j) to y1
and y2, and the probability 
outputs of softmax as 
1-p=a'
1 and p=a'
2, the 
negative log-likelihood 
becomes the cross-entropy 
function:
a'1
a1
Relationship with Logistic 
Regression?
softmax 
group aS (M)
a’S (M)
Linear Function 
of Features
In this case, we only have 
two neurons representing 
two classes. If we have 
more than two classes, we 
have M neurons, and the 
cross-entropy loss for one 
data point is: 
a'1
a1 …
Cross-entropy: the right cost 
function to use with softmax
• This cost function is the 
right cost function. J ′
=− yi log
i
∑ a
′
i
∂J ′
∂ai
=
∂J ′
∂a
′
j
∂a
′
j
∂ai j
∑
= (a
′
i − yi
)
=1
S (M)
Cross-entropy: the right cost 
function to use with softmax
• J'
2 has a very big gradient 
when the target value is 1 
and the output is almost 
zero.
J ′
=− yi log
i
∑ a
′
i
∂J ′
∂ai
=
∂J ′
∂a
′
j
∂a
′
j
∂ai j
∑
= (a
′
i − yi
)
S (M)
Cross-entropy: the right cost 
function to use with softmax
• This basically makes 
FFNNs with a softmax
output layer a nonlinear 
version of logistic 
regression, where logits
are highly nonlinear 
functions of the input that 
are learned from data.
J ′
=− yi log
i
∑ a
′
i
∂J ′
∂ai
=
∂J ′
∂a
′
j
∂a
′
j
∂ai j
∑
= (a
′
i − yi
)
S(M)
Sigmoids in hidden layers
• Using cross entropy and softmax remedies 
the problem of sigmoids in the output layer 
being in their saturation region.
• In hidden layers, this cannot be done and 
sigmoids may cause problems, especially 
if there are many hidden layers.
• To solve this problem, ReLUs are used 
way more often than sigmoids in hidden 
layers in modern Deep Learning. 
Note!
• Some of the best performing 
CNNs have tens of Convolution 
and Pooling layers
• Not necessary to have a Pooling 
layer after every Convolutional 
Layer. 
Note!
• Multiple Convolution + ReLU
operations in succession before 
having a Pooling operation:
Visualizing CNNs
• Learning High-Level Features
Visualizing CNNs: Example
• Learning High-Level Features
Visualizing CNNs:
Example
• Detect edges from raw pixels in 
the first layer
• Uses the edges to detect simple 
shapes in the second layer, 
• Use these shapes to deter 
higher-level features, such as 
facial shapes, in higher layers
Visualizing CNNs:
Example
• This is only an example: real life 
convolution filters may detect objects 
that have no meaning to humans.
See Appendix
• Capsule Networks
Recurrent neural networks
•Dates back to (Rumelhart et al., 1986)
•A family of neural networks for handling 
sequential data, which involves 
variable length inputs or outputs
•Especially, for natural language 
processing (NLP)
Sequential data
•Each data point: A sequence of vectors
, 

•Batch data: many sequences with different 
lengths 
•Label: can be a scalar, a vector, or even a 
sequence
•Example
•Sentiment analysis
•Machine translation
Example: Machine 
Translation
Figure from: devblogs.nvidia.com
More complicated sequential 
data
•Data point: two dimensional 
sequences like images
•Label: different type of 
sequences like text sentences
•Example: image captioning
Image captioning
Figure from the paper “DenseCap: Fully Convolutional 
Localization Networks for Dense Captioning”, by Justin 
Johnson, Andrej Karpathy, Li Fei-Fei
A typical dynamical system: 
computational graph
Figure from Deep Learning,
Goodfellow, Bengio and Courville
A system driven by external 
input
Figure from Deep Learning,
Goodfellow, Bengio and
Courville
Compact view
Figure from Deep Learning,
Goodfellow, Bengio and
Courville
Compact view
Key: the same 
for all timesteps
square: one step time delay
Figure from Deep Learning,
Goodfellow, Bengio and
Courville
Recurrent neural networks
•Use the same computational function 
and parameters across different time 
steps of the sequence
•Each time step: takes the input entry 
and the previous hidden state to 
compute the output entry
•Loss: typically computed every time 
step
Recurrent neural networks
Figure from Deep Learning, by 
Goodfellow, Bengio and
Courville
Label
Loss
Output
State
Input
Recurrent neural networks
Figure from Deep Learning,
Goodfellow, Bengio and
Courville
Math formula:
Advantage
•Hidden state: a lossy summary of 
the past
•Shared functions and parameters: 
greatly reduce the capacity and 
good for generalization in learning
•Explicitly use the prior knowledge 
that the sequential data can be 
processed by in the same way at 
different time step (e.g., NLP)
Advantage
•Hidden state: a lossy summary of the 
past
•Shared functions and parameters: 
greatly reduce the capacity and good 
for generalization in learning
•Explicitly use the prior knowledge that 
the sequential data can be processed 
by in the same way at different time 
step (e.g., NLP)
Advantage
•Yet still powerful (actually
universal): any function computable
by a Turing machine can be
computed by such a recurrent
network of a finite size (see, e.g.,
Siegelmann and Sontag (1995))
Training RNN
•Principle: unfold the computational 
graph, and use backpropagation
•Called back-propagation through time 
(BPTT) algorithm
•Can then apply any general-purpose 
gradient-based techniques
•Conceptually: first compute the gradients 
of the internal nodes, then compute the 
gradients of the parameters
RNN
•Use the same computational function and 
parameters across different time steps of the 
sequence
•Each time step: takes the input entry and the 
previous hidden state to compute the output 
entry
•Loss: typically computed every time step
•Many variants
•Information about the past can be in many 
other forms
•Only output at the end of the sequence
Example: use the 
output 
at the 
previous
step
Figure from Deep Learning,
Goodfellow, Bengio and
Courville
Example: only output at theend
Figure from Deep Learning,
Goodfellow, Bengio and Courville
Bidirectional RNNs
•Many applications: output at time 
may depend on the whole input
sequence
•Example in speech recognition: correct
interpretation of the current sound
may depend on the next few
phonemes, potentially even the next
few words
•Bidirectional RNNs are introduced to 
address this
BiRNNs
Figure from Deep
Learning,
Goodfellow, Bengio 
and Courville
Encoder-decoder RNNs
•RNNs: can map sequence to one 
vector; or to sequence of same length
•What about mapping sequence to 
sequence of different length?
•Example: speech recognition, 
machine translation, question 
answering, etc
Encoder-decoder RNNs
Learning to generate an output 
sequence (y(1); …; y(ny)
) given an input 
sequence
(x(1); x(2); …; x(nx)
) . 
Figure from Deep Learning,
Goodfellow, Bengio and
Courville
Encoder-decoder RNNs
• Composed of an encoder RNN that 
reads the input sequence as well as 
a decoder RNN that generates the 
output sequence (or computes the 
probability of a given output 
sequence).
Encoder-decoder RNNs
• The final hidden state of the 
encoder RNN is used to compute a 
generally fixed-size context variable 
C , which represents a semantic 
summary of the input sequence and 
is given as input to the decoder 
RNN.
The Challenge of Long-Term 
Dependencies
• Gradients propagated over 
many stages tend to either 
vanish (most of the time) or 
explode (rarely, but with much 
damage to the optimization).
The LSTM
• The most effective sequence models 
used in practical applications are 
called gated RNNs. 
• These include the Long Short-Term 
Memory (LSTM) and networks 
based on the Gated Recurrent Unit 
(GRU).
A lot of material of this lesson on LSTM were adopted from:
A Critical Review of Recurrent Neural Networks for Sequence Learning
https://arxiv.org/abs/1506.00019
and
Understanding LSTM Networks
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
The LSTM
Applications: 
• unconstrained handwriting 
recognition speech recognition 
• handwriting generation 
• machine translation 
• image captioning parsing
Traditional RNNs
A chunk of neural network, A, looks at 
some input xt and outputs a value ht
. A 
loop allows information to be passed 
from one step of the network to the 
next.
Traditional RNNs
A recurrent neural network can be 
thought of as multiple copies of the 
same network, each passing a 
message to a successor. Consider 
what happens if we unroll the loop:
The Problem of Long-Term 
Dependencies
In cases where the gap between the 
relevant information and the place that 
it’s needed is small, RNNs can learn to 
use the past information.
The Problem of Long-Term 
Dependencies
Unfortunately, as that gap grows, RNNs 
become unable to learn to connect the 
information.:
“I grew up in Iran and studied there and 
in the United States, so I can 
speak…(Persian and English)”
The LSTM
• Long Short Term Memory networks –
usually just called “LSTMs” – are RNNs 
capable of learning long-term 
dependencies.
• Introduced by Hochreiter & Schmidhuber 
(1997), and were refined and popularized 
by many people in following work.1
• They work tremendously well on a large 
variety of problems, and are now widely 
used. 
The LSTM
• LSTMs are explicitly designed to avoid the 
long-term dependency problem. 
• RNNs: a chain of repeating modules of 
neural network that has a very simple 
structure, such as a single tanh layer.
•
The LSTM
• LSTMs also have this chain like structure, 
but the repeating module has a different 
structure. 
• Instead of a single neural network layer, 
there are four, interacting in a very special 
way. It is called a memory cell.
Some Symbols
• Some symbols for representing NNs briefly
Pink circles: pointwise operations, e.g. vector addition
Yellow boxes: learned neural network layers
Core Idea: The Cell State
• The key to LSTMs is the cell state, the 
horizontal line running through the top of 
the diagram.
• The cell state is kind of like a conveyor belt. 
It runs straight down the entire chain, with 
only some minor linear interactions. It’s very 
easy for information to just flow along it 
unchanged.
Core Idea: The Cell State
• The LSTM does have the ability to remove
or add information to the cell state, carefully 
regulated by structures called gates.
• Gates are a way to optionally let information 
through. They are composed out of a 
sigmoid neural net layer and a pointwise 
multiplication operation.
Core Idea: The Cell State
• The LSTM does have the ability to remove
or add information to the cell state, carefully 
regulated by structures called gates.
• Gates are a way to optionally let information 
through. They are composed out of a 
sigmoid neural net layer and a pointwise 
multiplication operation.
Core Idea: The Cell State
• The sigmoid layer outputs numbers 
between zero and one, describing how 
much of each component should be let 
through. 
• A value of zero means “let nothing through,” 
while a value of one means “let everything 
through!”
• An LSTM has three of these gates, to 
protect and control the cell state.
The Memory Cell: The First Step
• The first step for LSTM: decide what 
information is thrown away from the cell 
state. This decision is made by a sigmoid 
layer called the “forget gate layer.”
• It looks at ht−1 and xt
, and outputs a 
number between 0 and 1 for each number 
in the cell state Ct−1. 
• A 1 represents “completely keep this” while 
a 0 represents “completely get rid of this.”
The Memory Cell: The First Step
• Example of a language model trying to predict 
the next word based on all the previous ones:
• The cell state might include the gender of the 
present subject, so that the correct pronouns 
can be used. When we see a new subject, we 
want to forget the gender of the old subject.
The Memory Cell: The Second Step
• The next step is to decide what new 
information is store in the cell state. This 
has two parts: 
• First, a sigmoid layer called the “input 
gate layer” decides which values are 
updated. 
• Next, a tanh layer creates a vector of 
new candidate values, C̃t
, that could be 
added to the state.
• Then, these two are combined to create an 
update to the state.
The Memory Cell: The Second Step
• In the language model example, the gender 
of the new subject needs to be added to the 
cell state, to replace the old one being 
forgotten.
Updating The Cell State
• It is now time to update the old cell 
state, Ct−1, into the new cell state Ct
. The 
previous steps already decided what to do, 
we just need to actually do it.
• We multiply the old state by ft
, forgetting the 
things we decided to forget earlier.
• Then we add i
t
¤C̃t
. This is the new 
candidate values, scaled by how much we 
decided to update each state value.
• ¤ is the pointwise multiplication 
operation.
Updating The Cell State
In the case of the language model, this is 
where the information about the old subject’s 
gender is removed and the new information, 
as decided in the previous steps, is added.
¤ ¤
Output of The Memory Cell
• The output is filtered version of the cell 
state. 
• A sigmoid layer decides what parts of the 
cell state are to be output. 
• Then, the cell state passes through 
through tanh (to force the values to be 
between −1 and 1) and is multiplied by the 
output of the sigmoid gate, so that it 
decides the parts to be output.
Output of The Memory Cell
• In language model example, since it just saw a 
subject, it might want to output information 
relevant to a verb, in case that’s what is coming 
next. 
• For example, it might output whether the subject 
is singular or plural, so that we know what form 
a verb should be conjugated into if that’s what 
follows next.
¤
Variants on Long Short Term Memory
• One popular LSTM variant, introduced 
by Gers & Schmidhuber (2000), is adding 
“peephole connections.” 
• This means that we let the gate layers look at 
the cell state.
Variants on Long Short Term Memory
• Another variation is to use coupled forget and 
input gates. 
• Instead of separately deciding what to forget 
and to what new information has to be added, 
those decisions are made together. 
• We only forget when we’re going to input 
something in its place. We only input new 
values to the state when we forget something 
older.
¤ ¤
Variants on Long Short Term Memory
¤ ¤
¤ ¤
Variants on Long Short Term Memory
• A slightly more dramatic is the Gated Recurrent 
Unit, or GRU ( Cho, et al. (2014)). 
• It combines the forget and input gates into a single 
“update gate.” 
• It also merges the cell state and hidden state, and 
makes some other changes. The resulting model is 
simpler than standard LSTM models, and has been 
growing increasingly popular.
Long-Term Dependencies: Clockwork 
RNNs
• In Clockwork Recurrent Neural Network, the 
long-term dependency problem is solved by 
having different parts (modules) of the RNN 
hidden layer running at different clock 
speeds, timing their computation with 
different, discrete clock periods, hence the 
name CW-RNN. 
Long-Term Dependencies: Clockwork 
RNNs
• CW-RNNs train and evaluate faster 
since not all modules are executed at 
every time step, and have a smaller 
number of weights compared to RNNs, 
because slower modules are not 
connected to faster ones. 
Architectures Constructed 
Using RNNs
Blue rectangles correspond to inputs, red 
rectangles to outputs and green 
rectangles to the entire hidden state of the 
neural network.
Architectures Constructed 
Using RNNs
Text and video classification are tasks in 
which a sequence is mapped to one fixed 
length vector.
Architectures Constructed 
Using RNNs
Image captioning presents the converse 
case, where the input image is a single
non-sequential data point.
Architectures Constructed 
Using RNNs
This architecture has been used for 
natural language translation, a sequenceto-sequence task in which the two 
sequences may have varying and different 
lengths
Architectures Constructed 
Using RNNs
This architecture has been used to learn
a generative model for text, predicting at 
each step the following character.
See Appendix
• Representations of Natural 
Language Inputs and Outputs
Sequence-to-Sequence 
LSTM Model
Sutskever et al. [2014] consists: 
An encoding model (first LSTM) and a 
decoding model (second LSTM). 
Sequence-to-Sequence 
LSTM Model
• The input blocks (blue and purple) 
correspond to word vectors, which are fully 
connected to the corresponding hidden state. 
• Red nodes are softmax outputs. 
• Weights are tied among all encoding steps 
and among all decoding time steps.
Sequence-to-Sequence 
LSTM Model
For training, the true inputs are fed to the 
encoder, the true translation
is fed to the decoder, and loss is propagated 
back from the outputs of the decoder across the 
entire sequence to sequence model.
See Appendix: Another 
Representation of LSTM
DSCI 552, Machine Learning for Data 
Science
University of Southern California
M. R. Rajati, PhD
1
Lesson 12
Graphical Models
http://www.computervisionblog.com/2015/04
/deep-learning-vs-probabilistic.html
Graphical Models
• Graphical models: children of 
graph theory and probability 
theory
• Connect neural networks and 
models such as HMMs, MRFs, and 
Kalman Filters
Advantages of Graphical Models
• Handling inference and learning in a 
unified manner
• Providing a unified framework for 
supervised and unsupervised learning
• Handling missing data easily
• Modeling conditional independence 
Transparency and Explainability (if 
desired)
Graphs
A graph consists of a collection of 
nodes and edges.
– Nodes, or vertices, are usually 
associated with the variables
distinction between discrete and 
continuous ignored in this initial 
discussion
– Edges connect nodes to one another.
Types of Graphical Models
• Two types: 
• undirected graphical models 
• and directed graphical models. 
• Main focus: directed graphical models.
(Specific forms of) Graphical models are 
also known as:
• Belief networks, 
• Bayesian networks, 
• Markov random Fields (MRFs)
Learning and Inference
• Key concept of graphical models
• What can be inferred should not be learned
• Weights make local assertions about the 
relationships between neighboring nodes
Learning and Inference
• Inference algorithms turn local assertions 
into global assertions about the 
relationships between nodes. 
• Examples:
• correlations between hidden units 
conditioned on a certain input and its 
corresponding output
• the probability of an input vector given an 
output vector
• This is achieved by calculating joint 
probability distribution from the network
Preliminaries
A specific form of graphical model are 
Bayesian networks:
– directed acyclic graphs (DAGs)
– directed: all connections have arrows 
associated with them;
– acyclic: following the arrows around 
it is not possible to complete a loop
Preliminaries
• Consider an arbitrary directed (acyclic) 
graph, where each node in the graph 
corresponds to a random variable (scalar 
or vector):
Preliminaries
• Edges represent statistical dependencies 
between the variables
Preliminaries
• No need to designate units as inputs, 
outputs or hidden
• We associate a probability distribution 
P (A, B, C, D, E, F ) with this graph
• All of other calculations are consistent 
with this distribution
• Short hand notation for
• P (A=a, B=b, C=c, D=d, E=e, F=f )
Preliminaries
• Example:
• We marginalize over a variable by 
wading it out via summing on all of its 
possible values

Marginals: simplified notation
• Example:
Problems in GMs
• The main problems that need to be 
addressed are:
– inference (from observation it’s 
cloudy infer probability of wet grass).
– training the models;
– determining the structure of the 
network (i.e. what is connected to 
what)
Notation
• In general the variables (nodes) 
may be split into two groups:
– observed (shaded) variables are the 
ones we have knowledge about.
– unobserved (unshaded) variables are 
ones we don’t know about and therefore 
have to infer the probability.
Using Graphical Models
• Supervised Learning
• Wade unshaded units out
Input
Model
Output
Using Graphical Models
• Prediction
• Wade unshaded units out
Using Graphical Models
• Control and Optimization
• Wade unshaded units out
Building A Graphical Model
• There are two ways to build a 
graphical model:
• Quantitative
• Qualitative
Building GMs Qualitatively 
• What is used to qualitatively build 
GMs? 
• prior knowledge of causal relationships
• assessment from experts
• learning from data
• Application-specific architectures are 
preferred (e.g., layered graphs)
Conditional Independence
• A fundamental concept in graphical 
models is conditional independence.
• Consider three random variables, A, 
B and C:
P(A,B,C) = P(A)P(B|A)P(C|B,A)
• If C is conditionally independent of A 
given B, then we can write
P(A,B,C) = P(A)P(B|A)P(C|B)
• The value of A does not affect the 
distribution of C if B is known.
Conditional Independence: Graph
• Graphically this can be described as
• Conditional independence is 
important when modelling highly 
complex systems.
Building GMs Qualitatively: Structures 
• C not observed:
P(A,B) =ΣCP(A,B,C) = P(A)ΣCP(C|A)P(B|C)
then A and B are dependent on each other.
• C = c observed: 
P(A,B|C = c) = P(A)P(B|C = c)
A and B are then independent. The path is 
sometimes called blocked.
Building GMs Qualitatively: Structures 
• C not observed: 
P(A,B) = ΣCP(A,B,C) =ΣCP(C)P(A|C)P(B|C)
then A and B are dependent on each other.
• C = c observed: 
P(A,B|C = c) = P(A|C = c)P(B|C = c)
A and B are then independent.
Building GMs Qualitatively 
C not observed:
P(A,B) = ΣCP(A,B,C) 
=P(A)P(B)ΣCP(C|A,B)=P(A)P(B)
A and B are independent of each other.
Building GMs Qualitatively 
C=c observed:
P(A,B|C=c) = P(A,B,C=c)/P(C=c)
=P(C=c|A,B)P(A)P(B)/P(C=c)
A and B are not independent of each 
other if C is observed.
Simple Example
•Consider the following Bayesian network
Simple Example
•Whether the grass is wet, W
•Whether the sprinkler has been 
used, S
•Whether it has rained, R
•Whether the it is cloudy C
–Associated with each node
–conditional probability table 
(CPT)
Simple Example
• The Model yields a set of conditional 
independence assumptions so that:
P(C,S,R,W) =
P(C)P(S|C)P(R|C)P(W|S,R)
Simple Example
• Possible to use CPTs for inference: 
Given that it is cloudy, what is the 
probability that the grass is wet:
• P(W = T|C = T)
Simple Example
• Possible to use CPTs for inference: 
Given that it is cloudy, what is the 
probability that the grass is wet:
• P(W = T|C = T)
Example: Alarm
• Question: What is P(E|B)?
P(B)=0.7
P(E)=0.01
P(A|B,E)=1
P(A|B,~E)=.9
P(A|~B,E)=.7
P(A|~B,~E)=.1
Example: Alarm
• Question: What is P(E|B)?
• P(E|B)=P(E)=0.01, because they are 
independent 
P(B)=0.7
P(E)=0.01
P(A|B,E)=1
P(A|B,~E)=.9
P(A|~B,E)=.7
P(A|~B,~E)=.1
Explaining Away
• If something has multiple causes, 
observing one of the causes reduces 
the probability of the other cause, i.e. 
it explains the other cause away.
P(B)=0.7
P(E)=0.01
P(A|B,E)=1
P(A|B,~E)=.9
P(A|~B,E)=.7
P(A|~B,~E)=.1
Explaining Away
• We have to compare P(E|A,B) with 
P(E|A)
P(B)=0.7
P(E)=0.01
P(A|B,E)=1
P(A|B,~E)=.9
P(A|~B,E)=.7
P(A|~B,~E)=.1
Explaining Away
• What is P(E|A)?
• Bayes' Rule:
• P(E|A)=P(A|E)P(E)/P(A)
P(B)=0.7
P(E)=0.01
P(A|B,E)=1
P(A|B,~E)=.9
P(A|~B,E)=.7
P(A|~B,~E)=.1
Example: Alarm
• First we need to calculate P(A).
P(B)=0.7
P(E)=0.01
P(A|B,E)=1
P(A|B,~E)=.9
P(A|~B,E)=.7
P(A|~B,~E)=.1
Example: Alarm
• First we need to calculate P(A).
• Answer:
P(A)=P(A|B,E)P(B,E)+P(A|B,~E)P(B,~E)+P(A|~B,E)P(~B,E)
+P(A|~B,~E) P(~B,~E)=1(0.7)(0.01)+0.9(0.7)(1-0.01)+0.7(1-
0.7)(0.01)+0.1(1-0.7)(1-0.01)=0.6625
P(B)=0.7
P(E)=0.01
P(A|B,E)=1
P(A|B,~E)=.9
P(A|~B,E)=.7
P(A|~B,~E)=.1
Example: Alarm Given Earthquake 
• Second, we need to calculate P(A|E).
• Answer:
P(A|E)=P(A|B,E)P(B)+P(A|~B,E)P(~B)=1(0.7)+0.7(1-
0.7)=.91
P(B)=0.7
P(E)=0.01
P(A|B,E)=1
P(A|B,~E)=.9
P(A|~B,E)=.7
P(A|~B,~E)=.1
Example: Earthquake Given Alarm
• P(E|A)=P(A|E)P(E)/P(A)
• Answer:
P(E|A)=(0.91)(0.01)/0.6625=0.013735
P(B)=0.7
P(E)=0.01
P(A|B,E)=1
P(A|B,~E)=.9
P(A|~B,E)=.7
P(A|~B,~E)=.1
Example: Earthquake | Alarm , Burglar
• Now let’s calculate P(E|A,B)
• Bayes’ Rule: 
P(E|A,B)=P(E,A,B)/P(A,B)
=P(A|B,E)P(B,E)/P(A|B)P(B)
But P(B,E)=P(E|B)P(B)
So
P(E|A,B)=P(A|B,E)P(E|B)P(B)/
P(A|B)P(B)
=P(A|B,E)P(E|B)/P(A|B)
Example: Earthquake | Alarm , Burglar
P(E|A,B)=P(A|B,E)P(E|B)/P(A|B)
• But P(E|B)=P(E) because they are 
independent
P(E|A,B)=P(A|B,E)P(E)/P(A|B)
• P(A|B)=P(A|B,E)P(E)+P(A|B,~E)P(~
E)
• P(A|B)=1(.01)+0.9(1-0.01)
• =0.901
P(B)=0.7
P(E)=0.01
P(A|B,E)=1
P(A|B,~E)=.9
P(A|~B,E)=.7
P(A|~B,~E)=.1
Example: Earthquake | Alarm , Burglar
P(E|A,B)=P(A|B,E)P(E)/P(A|B)
=1(0.01)/0.901= 
0.01109877913<0.013735= P(E|A)
P(B)=0.7
P(E)=0.01
P(A|B,E)=1
P(A|B,~E)=.9
P(A|~B,E)=.7
P(A|~B,~E)=.1
Beyond Naive Bayes’ Classifier
•Consider classifiers for the 
class given sequence: x1, x2, x3
Beyond Naive Bayes’ Classifier
•Consider the simple generative classifiers above 
(with joint distribution)
–naive-Bayes’ classifier on left (conditional 
independent features given class)
–for the classifier on the right - a bigram model
∗addition of sequence start feature x0 
(note P (x0|ωj) = 1)
∗addition of sequence end feature xd+1 
(variable length sequence)
•Decision now based on a more complex model
–this is the approach used for generating (classspecific) language models
Exercise
Calculate the following probabilities. Give both the 
formula and calculations with values. These 
questions are designed so that they can be 
answered with a minimum of computation.
1. P(a,¬b,c,¬d)
P (a)P (¬b|a)P (c|a)P (¬d|¬b) 
= 0.1 × 0.5 × 0.4 × 0.8 = 0.016 
Exercise
2. P(b) 
P(b) = ∑A={a,¬a} P(A)P(b|A) 
= 0.1 × 0.5 + 0.9 × 0.8 = 0.77
3. P(a|b) 
P (a|b) = P (a,b)/ P (b) = P (a)P (b|a) / P (b) 
= 0.1×0.5 / .77 = 0.064935 
Exercise
4. P(d|a)
P (d|a) = ∑B={b,¬b} P (d|B)p(B|a)
= 0.9 × 0.5 + 0.2 × 0.5 = 0. 55 
5. P(d|a,c) 
From the conditional independence properties of 
the graph, D ⊥ C|{A}. Hence, P(d|a,c) = p(d|a) 
= 0.55
Appendix:
More on Graphical Models
Building GMs Quantitatively 
• Question: how do we specify a 
joint distribution over the nodes in 
the graph? 
• Answer: 
• associate a conditional 
probability with each node
• take the product of the local 
probabilities to yield the global 
probabilities
Building GMs Quantitatively 
• Associate a conditional probability 
with each node
• Take the product of the local
probabilities to yield the global
probabilities
Building GMs Quantitatively 
• Let S={S1,…,SN} represent the set 
of random variables 
corresponding to the N nodes of 
the graph
• For any node Si , let pa(Si
) 
represent the set of parents of 
node Si
• Then 
P(S1,S2 ,…SN ) = P Si | pa(Si ( ))
i
∏
General Inference
• A general approach for inference 
with Bayesian Networks is 
message passing
• We present a very brief overview 
here
General Inference
• Process involves identifying:
• Cliques C: fully connected (every 
node is connected to every other 
node) subset of all the nodes.
• Separators S: the subset of the 
nodes of a clique that are 
connected to nodes outside the 
clique.
General Inference
• Thus given the value of the 
separators for a clique it is 
conditionally independent of all 
other variables.
General Inference
• To understand General Inference, 
we need to understand the 
semantics of undirected Graphical 
Models.
Semantics of undirected graphs
A and B are marginally dependent
A and B are conditionally independent
Quantitative specifcation of 
undirected models
Identify the cliques in the graph:
Quantitative specifcation of 
undirected models
• Define a configuration of a clique as a 
specification of values for each node in the
clique
• Define a potential of a clique as a function that 
associates a real number with each 
configuration of the clique
Quantitative specifcation of 
undirected models
Consider the example of a graph with binary
nodes
A potential is a table with entries for 
each combination of nodes in a clique
Quantitative specifcation of 
undirected models
“Marginalizing” over a potential table simply 
means collapsing (summing) the table along 
one or more dimensions
Quantitative specifcation of 
undirected models
Finally, define the probability of a global
configuration of the nodes as the product of 
the local potentials on the cliques:
where, without loss of generality, we 
assume that the normalization 
constant (if any) has been absorbed 
into one of the potentials
Boltzmann machine
• The Boltzmann machine is a special 
case of an undirected graphical model
• For a Boltzmann machine all of the 
potentials are formed by taking products 
of factors of the form exp(Si
Sj
Jij)
Boltzmann machine
Setting Jij equal to zero for non-neighboring
nodes guarantees that we respect the clique
boundaries
But we don't get the full conditional 
probability semantics with the Boltzmann 
machine parameterization
Boltzmann machine
The family of distributions parameterized by 
a Boltzmann machine on a graph is a 
proper subset of the family characterized 
by the conditional independencies
Several inference algorithms; some operate 
directly on the directed graph
The most popular inference algorithm, known 
as the junction tree algorithm (which we'll 
discuss here), operates on an undirected
graph
Inference algorithms for 
directed graphs
It also has the advantage of clarifying some of 
the relationships between the various 
algorithms
To understand the junction tree algorithm, we
need to understand how to "compile" a
directed graph into an undirected graph
Inference algorithms for 
directed graphs
Moral graphs
Note that for both directed graphs and 
undirected graphs, the joint 
probability is in a product form
So let's convert local conditional 
probabilities into potentials; then the 
products of potentials will give the 
right answer
Moral graphs
Indeed we can think of a conditional 
probability, e.g., P (C|A, B) as a 
function of the three variables A, B, 
and C (we get a real number for each 
configuration):
Moral graphs
Problem: A node and its parents are not 
generally in the same clique
Solution: Marry the parents to obtain 
the “moral graph"
Moral graphs
Define the potential on a clique as 
the product over all conditional 
probabilities contained within the 
clique
Moral graphs
Now the products of potentials gives 
the right answer:
P (A, B, C, D, E, F ) = P (A)P (B)
P(C|A, B)P (D|C)P (E|C)P (F|D, E)
Moral graphs
P (A, B, C, D, E, F ) = P (A)P (B)
P(C|A, B)P (D|C)P (E|C)P (F|D, E)
=φ(A,B,C) φ(C,D,E) φ(D,E,F)
Where:
• φ(A,B,C) = P (A)P (B) P(C|A, B)
• φ(C,D,E) =P (D|C)P (E|C)
• φ(D,E,F)= P (F|D, E)
Evidence and Inference
“Absorbing evidence" means observing the 
values of certain of the nodes
Absorbing evidence divides the units of the 
network into two groups:
Evidence and Inference
“Inference" means calculating the conditional 
distribution
• Prediction and diagnosis are special 
cases
Propagation of probabilities
Now suppose that some evidence has 
been absorbed.
How do we propagate this effect to 
the rest of the graph?
Clique trees
A clique tree is an (undirected) tree of
cliques
Clique trees
Consider cases in which two 
neighboring cliques V and W have an 
overlap S (e.g., (A, C) overlaps with 
(C, D, E)).
Clique trees
The cliques need to “agree" on the probability 
of nodes in the overlap; this is achieved by 
marginalizing and rescaling:
This occurs in parallel, distributed fashion 
throughout the clique tree
A problem
Consider the following graph and a 
corresponding clique tree:
Note that C appears in two non-neighboring
cliques.
Question: What guarantee do we have that 
the probability associated with C in these 
two cliques will be the same?
A problem
Question: What guarantee do we have that 
the probability associated with C in these 
two cliques will be the same?
Answer: Nothing. In fact this is a problem 
with the algorithm as described so far. It is 
not true that in general local consistency 
implies global consistency.
Triangulation (last idea, hang in
there)
A triangulated graph is one in which no 
cycles with four or more nodes exist in 
which there is no chord.
We triangulate a graph by adding chords:
Triangulation
Now we no longer have our problem:
A clique tree for a triangulated graph has the 
running intersection property: if a node 
appears in two cliques, it appears 
everywhere on the path between the cliques
Thus local consistency implies global 
consistency for such clique trees
Triangulation
Thus local consistency implies global 
consistency for such clique trees
Junction trees
A clique tree for a triangulated graph is 
referred to as a junction tree
In junction trees, local consistency 
implies global consistency. Thus the 
local message-passing algorithm is 
(provably) correct.
Junction trees
It's also possible to show that only 
triangulated graphs have the property 
that their clique trees are junction 
trees. Thus, if we want local algorithms, 
we must triangulate.
Summary of the junction tree
algorithm
1.Moralize the graph
2.Triangulate the graph
3.Propagate by local message-passing in the 
junction tree
• Note that the first two steps are “offline"
• Note also that these steps provide a 
bound of the complexity of the 
propagation step
DSCI 552, Machine Learning for Data 
Science
University of Southern California
M. R. Rajati, PhD
1
Lesson 13
Reinforcement Learning
Overview
• Supervised Learning: Immediate 
feedback (labels provided for every input).
• Unsupervised Learning: No feedback (no 
labels provided).
• Reinforcement Learning: Delayed scalar 
feedback (a number called reward).
Overview
• RL deals with agents that must sense & 
act upon their environment. 
•This combines classical agent-based AI 
and machine learning techniques.
It is a very comprehensive problem 
setting.
Overview
• Examples:
• A robot cleaning my room and 
recharging its battery
• Robot-soccer
• How to invest in shares
• Modeling the economy through rational 
agents
• Learning how to fly a helicopter
• Scheduling planes to their destinations
• and so on
The Big Picture
Your action influences the state of the world which 
determines its reward
Complications
• The outcome of your actions may be 
uncertain
• You may not be able to perfectly sense the 
state of the world
• The reward may be stochastic. 
• Reward is delayed (i.e. finding food in a 
maze)
Complications
• You may have no clue (model) of how 
rewards are being paid off.
• The world may change while you try to learn 
it 
• How much time do you need to explore 
uncharted territory before you exploit what 
you have learned?
The Task
• To learn an optimal policy that maps states of 
the world to actions of the agent.
I.e., if this patch of room is dirty, I clean it. If my 
battery is empty, I recharge it.
• What is it that the agent tries to optimize?
Answer: the total future discounted reward:
p :S A ®
The Task
• What is it that the agent tries to optimize?
Answer: the total future discounted reward:
2
1 2
0
( ) ...
0 1
ttt t
i
t i
i
Vs r r r
r
p g g
g g
+ +
¥
+
=
=+ + +
= å £ <
Note: immediate reward is worth more 
than future reward.
What would happen to mouse in a maze 
with gamma = 0 ?
Value Function
• Let’s say we have access to the optimal value 
function that computes the total future discounted 
reward 
• What would be the optimal policy ?
• Answer: we choose the action that maximizes: 
π
*
(s) =
a
argmax r(s,a) +γV*
(δ(s,a)) !
"# $
%&
* V s( )
*
p ( ) s
Value Function
• We assume that we know what the reward will 
be if we perform action “a” in state “s”:
• We also assume we know what the next state of 
the world will be if we perform action “a” in state 
“s”: 
rsa (, )
1 (,) t t s sa +
= d
Example I
• Consider some complicated 
graph, and we would like to 
find the shortest path from a 
node Si to a goal node G.
• Traversing an edge will cost 
you 
“length edge” dollars. Si
G
Example I
• The value function encodes the total remaining 
distance to the goal node from any node s, i.e.
V(s) = “1 / distance” to goal from s.
• If you know V(s), the problem is trivial. You 
simply choose the node that has highest V(s).
Si
G
Example II
•A simple deterministic world.
• Each grid square represents a distinct 
state, each arrow a distinct action. 
•The immediate reward function, r(s, a) 
gives reward 100 for actions entering the 
goal state G, and zero otherwise. Values of 
V*(s) follow from r(s, a), and the discount 
factor γ = 0.9. 
•An optimal policy is also shown.
Example II
Find your way to the goal.
2
1 2
0
( ) ...
0 1
ttt t
i
t i
i
Vs r r r
r
p g g
g g
+ +
¥
+
=
=+ + +
= å £ <
Q-Function
• One approach to RL is then to try to estimate 
V*(s). 
• However, this approach requires you to know 
r(s,a) and δ(s,a).
• This is unrealistic in many real problems. What is 
the reward if a robot is exploring mars and decides 
to take a right turn? 
€
V*
(s)← max
a
r(s,a) +γV* [ (δ(s,a))]
Bellman Equation:
Q-Function
•Fortunately we can circumvent this 
problem by exploring and experiencing 
how the world reacts to our actions. We 
need to learn r & δ.
Q-Function
• We want a function that directly learns good 
state-action pairs, i.e. what action should I 
take in this state. We call this Q(s,a).
Q-Function
Q(s,a) ≡ r(s,a) +γV*
(δ(s,a))
= r(s,a) +γ
a'
max Q(δ(s,a),a
')
• Let us define the evaluation function Q(s, a) so 
that its value is the maximum discounted 
cumulative reward that can be achieved 
starting from state s and applying action a as 
the first action. 
• In other words, the value of Q is the reward 
received immediately upon executing action a 
from state s, plus the value (discounted by γ ) 
of following the optimal policy thereafter.
Q-Function
• Why is this rewrite important? Because it 
shows that if the agent learns the Q function 
instead of the V* function, it will be able to 
select optimal actions even when it has no 
knowledge of the functions r and δ.
Q-Function
• It need only consider each available action a 
in its current state s and choose the action 
that maximizes Q(s, a).
*
*
() (, ) argmax
() (, ) max
a
a
s Qsa
V s Qsa
p =
=
Example
•To illustrate, the figure in the next slide 
shows the Q values for every state and action 
in the simple grid world.
•The Q value for each state-action transition 
equals the r value for this transition plus the 
V* value for the resulting state discounted by 
γ.
•The optimal policy shown in the figure 
corresponds to selecting actions with maximal 
Q values.
Example II
Check that 
*
*
() (, ) argmax
() (, ) max
a
a
s Qsa
V s Qsa
p =
=
Q-Learning
• Learning the Q function 
corresponds to learning the optimal 
policy. How can Q be learned?
• The key problem is finding a 
reliable way to estimate training 
values for Q, given only a sequence 
of immediate rewards r spread out 
over time.
Q-Learning
• This can be accomplished through 
iterative approximation. 
• To see how, notice the close 
relationship between Q and V*,
*
*
() (, ) argmax
() (, ) max
a
a
s Qsa
V s Qsa
p =
=
,
,
Q-Learning
which allows rewriting the Q function 
as:
This still depends on r(s,a) and δ(s,a); 
however,
Q(s,a) ≡ r(s,a) +γV*
(δ(s,a))
= r(s,a) +γ
a
'
max Q(δ(s,a),a
')
Q-Learning
this recursive definition of Q provides 
the basis for algorithms that iteratively
approximate Q.
Q refers to the learner's estimate, or 
hypothesis, of the actual Q function.
^
Q(s,a) ≡ r(s,a) +γV*
(δ(s,a))
= r(s,a) +γ
a'
max Q(δ(s,a),a
')
Q-Learning
Q(s,a) ≡ r(s,a) +γV*
(δ(s,a))
= r(s,a) +γ
a'
max Q(δ(s,a),a
')
•Imagine the robot is exploring its environment, 
trying new actions as it goes.
• At every step it receives some reward “
r
”, and it 
observes the environment change into a new 
state s
’ for action a. 
•How can we use these observations, (s,a,s
’
,r) to 
learn a model?
Q-Learning
•The learner represents its hypothesis Q by 
a large table with a separate entry for each 
state-action pair. 
•The table entry for the pair (s, a) stores the 
value for Q(s,a), learner's current 
hypothesis about the actual but unknown 
value Q(s,a). 
•The table can be initially filled with random 
values (though it is easier to understand the 
algorithm if one assumes initial values of 
zero). 
^
^
Q-Learning
•The agent repeatedly observes its current 
state s, chooses some action a, executes 
this action, then observes the resulting 
reward r = r(s, a) and the new state s' = δ(s, 
a).
Q-Learning
• It then updates the table entry for Q (s,a) 
following each such transition, according to the 
rule:
Q
ˆ
(s,a) ← r +γ max
a'
Q
ˆ
(s '
,a
') s
’
=st+1
^
Q-Learning
• This equation continually estimates Q at state s
consistent with an estimate of Q at state s’, one 
step in the future: temporal difference (TD) 
learning.
• Note that s
’ is closer to goal, and hence more 
“reliable”, but still an estimate itself.
Q
ˆ
(s,a) ← r +γ max
a'
Q
ˆ
(s '
,a
') s
’
=st+1
Q-Learning Summary
Q-Learning
• We do an update after each state-action pair. I.e., 
we are learning online!
• We are learning useful things about explored 
state-action pairs. These are typically most useful 
because they are likely to be encountered again.
• Under suitable conditions, these updates can 
actually be proved to converge to the real answer.
Q
ˆ
(s,a) ← r +γ max
a'
Q
ˆ
(s '
,a
')
s
’
=st+1
Example: Q-Learning
1 2
'
ˆ ˆ
( , ) ( , ') max
0 0.9 max{66,81,100}
90
right
a
Qs a r Qs a ¬ + g
¬ +
¬
Q-learning propagates Q-estimates 1-step backwards
Exploration / Exploitation
•The algorithm does not specify how actions 
are chosen by the agent. 
•One strategy would be in state s to select the 
action a that maximizes Q(s,a), thereby 
exploiting its current approximation Q. 
^
^
Exploration / Exploitation
•Using this strategy the agent runs the risk that 
it will overcommit to actions that are found 
during early training to have high Q values, 
while failing to explore other actions that have 
even higher values. 
•It is common in Q learning to use a 
probabilistic approach to selecting actions. 
Exploration / Exploitation
•Actions with higher Q values are assigned 
higher probabilities, but every action is assigned 
a nonzero probability. One way to assign such 
probabilities is
•where P(ai |s) is the probability of selecting 
action ai
, given that the agent is in state s, and 
where T > 0 is a constant that determines how 
strongly the selection favors actions with high Q
values.
P(ai |s) =
e
Qˆ(s,ai )/T
e
Qˆ(s,aj )/T
j
∑
Exploration / Exploitation
•Hence it is good to try new things so now and then, 
e.g. If T large lots of exploring, if T small, exploit
current policy. One can decrease T over time to first 
explore, and then converge and exploit.
•For example T= c/k+d where k is iteration of the 
algorithm
•Decreasing T over time is sometimes called 
simulated annealing, which is inspired by annealing 
process in metals. T is sometimes called the 
Temperature. 
P(ai |s) =
e
Qˆ(s,ai )/T
e
Qˆ(s,aj )/T
j
∑
Improvements
• One can trade-off memory and computation by 
cashing (s,s
’
,r) for observed transitions. After a while, 
as Q(s
’
,a
’) has changed, you can “replay” the 
update:
• One can actively search for state-action pairs for 
which Q(s,a) is expected to change a lot (prioritized 
sweeping).
Q
ˆ
(s,a) ← r +γ max
a'
Q
ˆ
(s '
,a
')
Temporal Difference Learning
•Q learning algorithm learns by iteratively 
reducing the discrepancy between Q value 
estimates for adjacent state
•Q learning is a special case of temporal 
difference algorithms that learn by reducing 
discrepancies between estimates made by the 
agent at different times. 
Temporal Difference Learning
•The raining rule we studied reduces the 
difference between the estimated Q values of a 
state and its immediate successor
•However, we could design an algorithm that 
reduces discrepancies between this state and 
more distant descendants or ancestors.
Temporal Difference Learning
•Recall that our Q learning training rule 
calculates a training value for Q(st
, at
) in terms 
of the values for Q(st+1, at+1) where st+1 is the 
result of applying action at to the state st
.
• Let Q(1)(st
, at
) denote the training value 
calculated by this one-step lookahead:
^
^
Temporal Difference Learning
•One alternative way to compute a training 
value for Q(st
, at
)is to base it on the observed 
rewards for two steps
• or, in general, for n steps
Temporal Difference Learning
•A general method for blending these 
alternative training estimates, called TD(λ). 
•The idea is to use a constant 0 ≤ λ ≤ 1 to 
combine the estimates obtained from various 
lookahead distances in the following fashion
Temporal Difference Learning
•An equivalent recursive definition for Qλ is
Temporal Difference Learning
•If λ = 0 we have our original training estimate 
Q(1), which considers only one-step 
discrepancies in the Q estimates. 
•As λ is increased, the algorithm places 
increasing emphasis on discrepancies based 
on more distant lookaheads.
Temporal Difference Learning
•At the extreme value λ = 1, only the observed 
rt+i values are considered, with no contribution 
from the current Q estimate.
•The motivation for the TD(λ) method is that in 
some settings training will be more efficient if 
more distant lookaheads are considered.
Extensions • To deal with stochastic environments, we need to 
maximize expected future discounted reward:
• Often the state space is too large to deal with all 
states and adopt a table-lookup approach. In this case 
we need to learn a function:
Q(s,a) = E[r(s,a)] +γ P (s '|s,a) max
a' s '
∑ Q(s '
,a
')
Qsa f sa (, ) (, ) » q
Extensions
• Neural network with back-propagation have been 
quite successful. 
• For instance, TD-Gammon is a back-gammon 
program that plays at expert level.
•state-space very large, trained by playing against 
itself, uses NN to approximate value function, uses 
TD(λ) for learning.
Qsa f sa (, ) (, ) » q
More on Function Approximation
Q(s,a) ≈ f θ
(s,a) = θ k
a Φk
(s)
k=1
K
∑
• For instance: linear function:
The features Φ are fixed measurements of the state 
(e.g. # stones on the board).
We only learn the parameters theta.
• Update rule: (start in state s, take action a, 
observe reward r and end up in state s’)
θ k
a ←θ k
a +α r +γ max
a'
Q
ˆ
(s '
,a
') −Q
ˆ
(s,a) ( )Φk
(s)
change in Q
Conclusion
• Reinforcement learning addresses a very 
broad and relevant question:
How can we learn to survive in our 
environment?
• We have looked at Q-learning, which 
simply learns from experience.
No model of the world is needed.
Conclusion
•We made simplifying assumptions: e.g. 
state of the world only depends on last 
state and action. This is the Markov
assumption. The model is called a Markov 
Decision Process (MDP).
Conclusion
• We assumed deterministic dynamics, 
reward function, but the world really is 
stochastic.
• There are many extensions to speed up 
learning.
• There have been many successful real 
world applications.
